{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 37.84259492079457,
  "eval_steps": 400,
  "global_step": 150500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.025144581342720643,
      "grad_norm": 2.1352484226226807,
      "learning_rate": 2.5e-06,
      "loss": 6.7834,
      "step": 100
    },
    {
      "epoch": 0.050289162685441285,
      "grad_norm": 1.857782244682312,
      "learning_rate": 5e-06,
      "loss": 6.4547,
      "step": 200
    },
    {
      "epoch": 0.07543374402816193,
      "grad_norm": 1.5137596130371094,
      "learning_rate": 7.5e-06,
      "loss": 6.2126,
      "step": 300
    },
    {
      "epoch": 0.10057832537088257,
      "grad_norm": 1.4946668148040771,
      "learning_rate": 1e-05,
      "loss": 6.0312,
      "step": 400
    },
    {
      "epoch": 0.10057832537088257,
      "eval_loss": 5.911785125732422,
      "eval_runtime": 16.8676,
      "eval_samples_per_second": 794.064,
      "eval_steps_per_second": 12.45,
      "step": 400
    },
    {
      "epoch": 0.12572290671360323,
      "grad_norm": 1.472823143005371,
      "learning_rate": 1.25e-05,
      "loss": 5.8486,
      "step": 500
    },
    {
      "epoch": 0.15086748805632386,
      "grad_norm": 1.854067325592041,
      "learning_rate": 1.5e-05,
      "loss": 5.6644,
      "step": 600
    },
    {
      "epoch": 0.1760120693990445,
      "grad_norm": 1.7593398094177246,
      "learning_rate": 1.75e-05,
      "loss": 5.5081,
      "step": 700
    },
    {
      "epoch": 0.20115665074176514,
      "grad_norm": 1.9230692386627197,
      "learning_rate": 2e-05,
      "loss": 5.3591,
      "step": 800
    },
    {
      "epoch": 0.20115665074176514,
      "eval_loss": 5.230790615081787,
      "eval_runtime": 16.7854,
      "eval_samples_per_second": 797.955,
      "eval_steps_per_second": 12.511,
      "step": 800
    },
    {
      "epoch": 0.2263012320844858,
      "grad_norm": 1.7787915468215942,
      "learning_rate": 2.25e-05,
      "loss": 5.2144,
      "step": 900
    },
    {
      "epoch": 0.25144581342720645,
      "grad_norm": 1.8353736400604248,
      "learning_rate": 2.5e-05,
      "loss": 5.08,
      "step": 1000
    },
    {
      "epoch": 0.2765903947699271,
      "grad_norm": 1.7183172702789307,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 4.9589,
      "step": 1100
    },
    {
      "epoch": 0.3017349761126477,
      "grad_norm": 1.9467921257019043,
      "learning_rate": 3e-05,
      "loss": 4.838,
      "step": 1200
    },
    {
      "epoch": 0.3017349761126477,
      "eval_loss": 4.739732265472412,
      "eval_runtime": 16.9117,
      "eval_samples_per_second": 791.998,
      "eval_steps_per_second": 12.417,
      "step": 1200
    },
    {
      "epoch": 0.32687955745536834,
      "grad_norm": 1.7826118469238281,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 4.7506,
      "step": 1300
    },
    {
      "epoch": 0.352024138798089,
      "grad_norm": 2.1294872760772705,
      "learning_rate": 3.5e-05,
      "loss": 4.6699,
      "step": 1400
    },
    {
      "epoch": 0.37716872014080965,
      "grad_norm": 1.8965647220611572,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 4.6007,
      "step": 1500
    },
    {
      "epoch": 0.4023133014835303,
      "grad_norm": 2.033254384994507,
      "learning_rate": 4e-05,
      "loss": 4.529,
      "step": 1600
    },
    {
      "epoch": 0.4023133014835303,
      "eval_loss": 4.432247638702393,
      "eval_runtime": 16.918,
      "eval_samples_per_second": 791.703,
      "eval_steps_per_second": 12.413,
      "step": 1600
    },
    {
      "epoch": 0.42745788282625097,
      "grad_norm": 2.3572545051574707,
      "learning_rate": 4.25e-05,
      "loss": 4.4707,
      "step": 1700
    },
    {
      "epoch": 0.4526024641689716,
      "grad_norm": 2.3400394916534424,
      "learning_rate": 4.5e-05,
      "loss": 4.4139,
      "step": 1800
    },
    {
      "epoch": 0.4777470455116922,
      "grad_norm": 2.1773509979248047,
      "learning_rate": 4.75e-05,
      "loss": 4.3673,
      "step": 1900
    },
    {
      "epoch": 0.5028916268544129,
      "grad_norm": 2.339672803878784,
      "learning_rate": 5e-05,
      "loss": 4.3216,
      "step": 2000
    },
    {
      "epoch": 0.5028916268544129,
      "eval_loss": 4.225683212280273,
      "eval_runtime": 16.908,
      "eval_samples_per_second": 792.168,
      "eval_steps_per_second": 12.42,
      "step": 2000
    },
    {
      "epoch": 0.5280362081971335,
      "grad_norm": 2.304389715194702,
      "learning_rate": 4.9998742138364785e-05,
      "loss": 4.2639,
      "step": 2100
    },
    {
      "epoch": 0.5531807895398542,
      "grad_norm": 2.2512753009796143,
      "learning_rate": 4.999748427672956e-05,
      "loss": 4.2333,
      "step": 2200
    },
    {
      "epoch": 0.5783253708825749,
      "grad_norm": 2.216935634613037,
      "learning_rate": 4.999622641509434e-05,
      "loss": 4.1721,
      "step": 2300
    },
    {
      "epoch": 0.6034699522252954,
      "grad_norm": 2.3196399211883545,
      "learning_rate": 4.999496855345912e-05,
      "loss": 4.1383,
      "step": 2400
    },
    {
      "epoch": 0.6034699522252954,
      "eval_loss": 4.075955390930176,
      "eval_runtime": 16.8641,
      "eval_samples_per_second": 794.233,
      "eval_steps_per_second": 12.453,
      "step": 2400
    },
    {
      "epoch": 0.6286145335680161,
      "grad_norm": 2.3696482181549072,
      "learning_rate": 4.99937106918239e-05,
      "loss": 4.122,
      "step": 2500
    },
    {
      "epoch": 0.6537591149107367,
      "grad_norm": 2.52116322517395,
      "learning_rate": 4.999245283018868e-05,
      "loss": 4.0805,
      "step": 2600
    },
    {
      "epoch": 0.6789036962534574,
      "grad_norm": 2.308964729309082,
      "learning_rate": 4.9991194968553464e-05,
      "loss": 4.0642,
      "step": 2700
    },
    {
      "epoch": 0.704048277596178,
      "grad_norm": 2.561660051345825,
      "learning_rate": 4.998993710691824e-05,
      "loss": 4.0244,
      "step": 2800
    },
    {
      "epoch": 0.704048277596178,
      "eval_loss": 3.944016218185425,
      "eval_runtime": 16.8925,
      "eval_samples_per_second": 792.898,
      "eval_steps_per_second": 12.432,
      "step": 2800
    },
    {
      "epoch": 0.7291928589388986,
      "grad_norm": 2.4458584785461426,
      "learning_rate": 4.998867924528302e-05,
      "loss": 3.9988,
      "step": 2900
    },
    {
      "epoch": 0.7543374402816193,
      "grad_norm": 2.5157015323638916,
      "learning_rate": 4.9987421383647804e-05,
      "loss": 3.9806,
      "step": 3000
    },
    {
      "epoch": 0.77948202162434,
      "grad_norm": 2.5359463691711426,
      "learning_rate": 4.998616352201258e-05,
      "loss": 3.952,
      "step": 3100
    },
    {
      "epoch": 0.8046266029670606,
      "grad_norm": 2.5777297019958496,
      "learning_rate": 4.998490566037736e-05,
      "loss": 3.929,
      "step": 3200
    },
    {
      "epoch": 0.8046266029670606,
      "eval_loss": 3.8381690979003906,
      "eval_runtime": 17.0514,
      "eval_samples_per_second": 785.506,
      "eval_steps_per_second": 12.316,
      "step": 3200
    },
    {
      "epoch": 0.8297711843097813,
      "grad_norm": 2.932445526123047,
      "learning_rate": 4.998364779874214e-05,
      "loss": 3.9055,
      "step": 3300
    },
    {
      "epoch": 0.8549157656525019,
      "grad_norm": 2.705190658569336,
      "learning_rate": 4.998238993710692e-05,
      "loss": 3.884,
      "step": 3400
    },
    {
      "epoch": 0.8800603469952225,
      "grad_norm": 2.9452764987945557,
      "learning_rate": 4.9981132075471695e-05,
      "loss": 3.8477,
      "step": 3500
    },
    {
      "epoch": 0.9052049283379432,
      "grad_norm": 2.805428981781006,
      "learning_rate": 4.997987421383648e-05,
      "loss": 3.8506,
      "step": 3600
    },
    {
      "epoch": 0.9052049283379432,
      "eval_loss": 3.7424380779266357,
      "eval_runtime": 16.893,
      "eval_samples_per_second": 792.872,
      "eval_steps_per_second": 12.431,
      "step": 3600
    },
    {
      "epoch": 0.9303495096806638,
      "grad_norm": 2.6072821617126465,
      "learning_rate": 4.997861635220126e-05,
      "loss": 3.8111,
      "step": 3700
    },
    {
      "epoch": 0.9554940910233845,
      "grad_norm": 2.60937762260437,
      "learning_rate": 4.997735849056604e-05,
      "loss": 3.7941,
      "step": 3800
    },
    {
      "epoch": 0.9806386723661051,
      "grad_norm": 2.663933277130127,
      "learning_rate": 4.9976100628930824e-05,
      "loss": 3.7769,
      "step": 3900
    },
    {
      "epoch": 1.0057832537088258,
      "grad_norm": 2.744719982147217,
      "learning_rate": 4.99748427672956e-05,
      "loss": 3.7636,
      "step": 4000
    },
    {
      "epoch": 1.0057832537088258,
      "eval_loss": 3.6670334339141846,
      "eval_runtime": 16.9401,
      "eval_samples_per_second": 790.67,
      "eval_steps_per_second": 12.397,
      "step": 4000
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 2.751534938812256,
      "learning_rate": 4.997358490566038e-05,
      "loss": 3.7365,
      "step": 4100
    },
    {
      "epoch": 1.056072416394267,
      "grad_norm": 2.7905359268188477,
      "learning_rate": 4.997232704402516e-05,
      "loss": 3.7288,
      "step": 4200
    },
    {
      "epoch": 1.0812169977369877,
      "grad_norm": 2.8208212852478027,
      "learning_rate": 4.997106918238994e-05,
      "loss": 3.705,
      "step": 4300
    },
    {
      "epoch": 1.1063615790797083,
      "grad_norm": 2.6150166988372803,
      "learning_rate": 4.9969811320754715e-05,
      "loss": 3.6923,
      "step": 4400
    },
    {
      "epoch": 1.1063615790797083,
      "eval_loss": 3.5995047092437744,
      "eval_runtime": 16.8925,
      "eval_samples_per_second": 792.896,
      "eval_steps_per_second": 12.432,
      "step": 4400
    },
    {
      "epoch": 1.131506160422429,
      "grad_norm": 2.931492567062378,
      "learning_rate": 4.99685534591195e-05,
      "loss": 3.6828,
      "step": 4500
    },
    {
      "epoch": 1.1566507417651497,
      "grad_norm": 2.914693832397461,
      "learning_rate": 4.996729559748428e-05,
      "loss": 3.6632,
      "step": 4600
    },
    {
      "epoch": 1.1817953231078702,
      "grad_norm": 3.1200144290924072,
      "learning_rate": 4.996603773584906e-05,
      "loss": 3.6389,
      "step": 4700
    },
    {
      "epoch": 1.2069399044505909,
      "grad_norm": 2.691009998321533,
      "learning_rate": 4.9964779874213844e-05,
      "loss": 3.6397,
      "step": 4800
    },
    {
      "epoch": 1.2069399044505909,
      "eval_loss": 3.5415401458740234,
      "eval_runtime": 16.8604,
      "eval_samples_per_second": 794.406,
      "eval_steps_per_second": 12.455,
      "step": 4800
    },
    {
      "epoch": 1.2320844857933115,
      "grad_norm": 3.2544894218444824,
      "learning_rate": 4.996352201257862e-05,
      "loss": 3.615,
      "step": 4900
    },
    {
      "epoch": 1.2572290671360322,
      "grad_norm": 2.717719316482544,
      "learning_rate": 4.99622641509434e-05,
      "loss": 3.6123,
      "step": 5000
    },
    {
      "epoch": 1.282373648478753,
      "grad_norm": 2.9593846797943115,
      "learning_rate": 4.996100628930818e-05,
      "loss": 3.5951,
      "step": 5100
    },
    {
      "epoch": 1.3075182298214734,
      "grad_norm": 3.197242259979248,
      "learning_rate": 4.995974842767296e-05,
      "loss": 3.5832,
      "step": 5200
    },
    {
      "epoch": 1.3075182298214734,
      "eval_loss": 3.490705728530884,
      "eval_runtime": 17.0105,
      "eval_samples_per_second": 787.396,
      "eval_steps_per_second": 12.345,
      "step": 5200
    },
    {
      "epoch": 1.332662811164194,
      "grad_norm": 3.023218870162964,
      "learning_rate": 4.9958490566037735e-05,
      "loss": 3.5708,
      "step": 5300
    },
    {
      "epoch": 1.3578073925069147,
      "grad_norm": 3.2030093669891357,
      "learning_rate": 4.995723270440252e-05,
      "loss": 3.5699,
      "step": 5400
    },
    {
      "epoch": 1.3829519738496354,
      "grad_norm": 3.1561331748962402,
      "learning_rate": 4.99559748427673e-05,
      "loss": 3.5541,
      "step": 5500
    },
    {
      "epoch": 1.408096555192356,
      "grad_norm": 3.194258213043213,
      "learning_rate": 4.9954716981132075e-05,
      "loss": 3.5444,
      "step": 5600
    },
    {
      "epoch": 1.408096555192356,
      "eval_loss": 3.443798065185547,
      "eval_runtime": 17.0337,
      "eval_samples_per_second": 786.324,
      "eval_steps_per_second": 12.329,
      "step": 5600
    },
    {
      "epoch": 1.4332411365350768,
      "grad_norm": 2.8687243461608887,
      "learning_rate": 4.995345911949686e-05,
      "loss": 3.544,
      "step": 5700
    },
    {
      "epoch": 1.4583857178777975,
      "grad_norm": 3.028542995452881,
      "learning_rate": 4.995220125786164e-05,
      "loss": 3.5029,
      "step": 5800
    },
    {
      "epoch": 1.483530299220518,
      "grad_norm": 3.230130672454834,
      "learning_rate": 4.995094339622642e-05,
      "loss": 3.5119,
      "step": 5900
    },
    {
      "epoch": 1.5086748805632386,
      "grad_norm": 3.017268657684326,
      "learning_rate": 4.99496855345912e-05,
      "loss": 3.5116,
      "step": 6000
    },
    {
      "epoch": 1.5086748805632386,
      "eval_loss": 3.4058072566986084,
      "eval_runtime": 16.9535,
      "eval_samples_per_second": 790.043,
      "eval_steps_per_second": 12.387,
      "step": 6000
    },
    {
      "epoch": 1.5338194619059593,
      "grad_norm": 3.121727705001831,
      "learning_rate": 4.994842767295598e-05,
      "loss": 3.5006,
      "step": 6100
    },
    {
      "epoch": 1.5589640432486798,
      "grad_norm": 3.2188308238983154,
      "learning_rate": 4.994716981132076e-05,
      "loss": 3.4854,
      "step": 6200
    },
    {
      "epoch": 1.5841086245914004,
      "grad_norm": 2.9861040115356445,
      "learning_rate": 4.994591194968554e-05,
      "loss": 3.4824,
      "step": 6300
    },
    {
      "epoch": 1.6092532059341211,
      "grad_norm": 3.153669834136963,
      "learning_rate": 4.994465408805032e-05,
      "loss": 3.4643,
      "step": 6400
    },
    {
      "epoch": 1.6092532059341211,
      "eval_loss": 3.36798095703125,
      "eval_runtime": 17.3206,
      "eval_samples_per_second": 773.3,
      "eval_steps_per_second": 12.124,
      "step": 6400
    },
    {
      "epoch": 1.6343977872768418,
      "grad_norm": 2.848717451095581,
      "learning_rate": 4.9943396226415094e-05,
      "loss": 3.4505,
      "step": 6500
    },
    {
      "epoch": 1.6595423686195625,
      "grad_norm": 3.153914213180542,
      "learning_rate": 4.9942138364779877e-05,
      "loss": 3.4472,
      "step": 6600
    },
    {
      "epoch": 1.6846869499622832,
      "grad_norm": 3.292623996734619,
      "learning_rate": 4.994088050314465e-05,
      "loss": 3.4375,
      "step": 6700
    },
    {
      "epoch": 1.7098315313050039,
      "grad_norm": 2.9716291427612305,
      "learning_rate": 4.9939622641509434e-05,
      "loss": 3.4349,
      "step": 6800
    },
    {
      "epoch": 1.7098315313050039,
      "eval_loss": 3.333606481552124,
      "eval_runtime": 17.3193,
      "eval_samples_per_second": 773.355,
      "eval_steps_per_second": 12.125,
      "step": 6800
    },
    {
      "epoch": 1.7349761126477246,
      "grad_norm": 3.1792447566986084,
      "learning_rate": 4.9938364779874216e-05,
      "loss": 3.433,
      "step": 6900
    },
    {
      "epoch": 1.760120693990445,
      "grad_norm": 2.951174259185791,
      "learning_rate": 4.9937106918239e-05,
      "loss": 3.4051,
      "step": 7000
    },
    {
      "epoch": 1.7852652753331657,
      "grad_norm": 3.4196207523345947,
      "learning_rate": 4.993584905660378e-05,
      "loss": 3.4096,
      "step": 7100
    },
    {
      "epoch": 1.8104098566758864,
      "grad_norm": 3.112807273864746,
      "learning_rate": 4.9934591194968556e-05,
      "loss": 3.3973,
      "step": 7200
    },
    {
      "epoch": 1.8104098566758864,
      "eval_loss": 3.308467388153076,
      "eval_runtime": 17.3534,
      "eval_samples_per_second": 771.838,
      "eval_steps_per_second": 12.101,
      "step": 7200
    },
    {
      "epoch": 1.8355544380186068,
      "grad_norm": 3.2002792358398438,
      "learning_rate": 4.993333333333334e-05,
      "loss": 3.3928,
      "step": 7300
    },
    {
      "epoch": 1.8606990193613275,
      "grad_norm": 3.225881338119507,
      "learning_rate": 4.9932075471698114e-05,
      "loss": 3.3986,
      "step": 7400
    },
    {
      "epoch": 1.8858436007040482,
      "grad_norm": 3.101343870162964,
      "learning_rate": 4.9930817610062896e-05,
      "loss": 3.3852,
      "step": 7500
    },
    {
      "epoch": 1.910988182046769,
      "grad_norm": 2.9258923530578613,
      "learning_rate": 4.992955974842767e-05,
      "loss": 3.3674,
      "step": 7600
    },
    {
      "epoch": 1.910988182046769,
      "eval_loss": 3.2790396213531494,
      "eval_runtime": 17.4719,
      "eval_samples_per_second": 766.604,
      "eval_steps_per_second": 12.019,
      "step": 7600
    },
    {
      "epoch": 1.9361327633894896,
      "grad_norm": 3.1402599811553955,
      "learning_rate": 4.9928301886792454e-05,
      "loss": 3.3812,
      "step": 7700
    },
    {
      "epoch": 1.9612773447322103,
      "grad_norm": 3.3153929710388184,
      "learning_rate": 4.992704402515723e-05,
      "loss": 3.375,
      "step": 7800
    },
    {
      "epoch": 1.986421926074931,
      "grad_norm": 3.175363540649414,
      "learning_rate": 4.992578616352201e-05,
      "loss": 3.3544,
      "step": 7900
    },
    {
      "epoch": 2.0115665074176516,
      "grad_norm": 4.144075393676758,
      "learning_rate": 4.9924528301886794e-05,
      "loss": 3.3449,
      "step": 8000
    },
    {
      "epoch": 2.0115665074176516,
      "eval_loss": 3.253164529800415,
      "eval_runtime": 17.5289,
      "eval_samples_per_second": 764.112,
      "eval_steps_per_second": 11.98,
      "step": 8000
    },
    {
      "epoch": 2.0367110887603723,
      "grad_norm": 3.1779348850250244,
      "learning_rate": 4.9923270440251576e-05,
      "loss": 3.3348,
      "step": 8100
    },
    {
      "epoch": 2.0618556701030926,
      "grad_norm": 3.2106218338012695,
      "learning_rate": 4.992201257861636e-05,
      "loss": 3.3356,
      "step": 8200
    },
    {
      "epoch": 2.0870002514458132,
      "grad_norm": 3.121774196624756,
      "learning_rate": 4.9920754716981134e-05,
      "loss": 3.3123,
      "step": 8300
    },
    {
      "epoch": 2.112144832788534,
      "grad_norm": 3.2682933807373047,
      "learning_rate": 4.9919496855345916e-05,
      "loss": 3.3142,
      "step": 8400
    },
    {
      "epoch": 2.112144832788534,
      "eval_loss": 3.230166435241699,
      "eval_runtime": 17.3865,
      "eval_samples_per_second": 770.37,
      "eval_steps_per_second": 12.078,
      "step": 8400
    },
    {
      "epoch": 2.1372894141312546,
      "grad_norm": 3.333792209625244,
      "learning_rate": 4.991823899371069e-05,
      "loss": 3.3123,
      "step": 8500
    },
    {
      "epoch": 2.1624339954739753,
      "grad_norm": 3.128021240234375,
      "learning_rate": 4.9916981132075474e-05,
      "loss": 3.3044,
      "step": 8600
    },
    {
      "epoch": 2.187578576816696,
      "grad_norm": 3.0379483699798584,
      "learning_rate": 4.9915723270440256e-05,
      "loss": 3.2926,
      "step": 8700
    },
    {
      "epoch": 2.2127231581594167,
      "grad_norm": 3.206083059310913,
      "learning_rate": 4.991446540880503e-05,
      "loss": 3.2945,
      "step": 8800
    },
    {
      "epoch": 2.2127231581594167,
      "eval_loss": 3.211550235748291,
      "eval_runtime": 17.2335,
      "eval_samples_per_second": 777.207,
      "eval_steps_per_second": 12.186,
      "step": 8800
    },
    {
      "epoch": 2.2378677395021374,
      "grad_norm": 3.4909653663635254,
      "learning_rate": 4.9913207547169814e-05,
      "loss": 3.2947,
      "step": 8900
    },
    {
      "epoch": 2.263012320844858,
      "grad_norm": 3.373971939086914,
      "learning_rate": 4.9911949685534596e-05,
      "loss": 3.286,
      "step": 9000
    },
    {
      "epoch": 2.2881569021875787,
      "grad_norm": 3.325855016708374,
      "learning_rate": 4.991069182389938e-05,
      "loss": 3.2806,
      "step": 9100
    },
    {
      "epoch": 2.3133014835302994,
      "grad_norm": 3.222019672393799,
      "learning_rate": 4.9909433962264154e-05,
      "loss": 3.2711,
      "step": 9200
    },
    {
      "epoch": 2.3133014835302994,
      "eval_loss": 3.189533233642578,
      "eval_runtime": 17.2087,
      "eval_samples_per_second": 778.327,
      "eval_steps_per_second": 12.203,
      "step": 9200
    },
    {
      "epoch": 2.33844606487302,
      "grad_norm": 3.384843111038208,
      "learning_rate": 4.9908176100628936e-05,
      "loss": 3.2921,
      "step": 9300
    },
    {
      "epoch": 2.3635906462157403,
      "grad_norm": 3.25579833984375,
      "learning_rate": 4.990691823899371e-05,
      "loss": 3.2738,
      "step": 9400
    },
    {
      "epoch": 2.388735227558461,
      "grad_norm": 3.4255125522613525,
      "learning_rate": 4.9905660377358493e-05,
      "loss": 3.2752,
      "step": 9500
    },
    {
      "epoch": 2.4138798089011817,
      "grad_norm": 3.3605263233184814,
      "learning_rate": 4.9904402515723276e-05,
      "loss": 3.2545,
      "step": 9600
    },
    {
      "epoch": 2.4138798089011817,
      "eval_loss": 3.1782820224761963,
      "eval_runtime": 17.0829,
      "eval_samples_per_second": 784.057,
      "eval_steps_per_second": 12.293,
      "step": 9600
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 3.0819413661956787,
      "learning_rate": 4.990314465408805e-05,
      "loss": 3.2734,
      "step": 9700
    },
    {
      "epoch": 2.464168971586623,
      "grad_norm": 3.456484794616699,
      "learning_rate": 4.9901886792452833e-05,
      "loss": 3.2608,
      "step": 9800
    },
    {
      "epoch": 2.4893135529293438,
      "grad_norm": 3.394460439682007,
      "learning_rate": 4.990062893081761e-05,
      "loss": 3.2416,
      "step": 9900
    },
    {
      "epoch": 2.5144581342720644,
      "grad_norm": 3.012700319290161,
      "learning_rate": 4.989937106918239e-05,
      "loss": 3.2603,
      "step": 10000
    },
    {
      "epoch": 2.5144581342720644,
      "eval_loss": 3.1515989303588867,
      "eval_runtime": 17.2493,
      "eval_samples_per_second": 776.495,
      "eval_steps_per_second": 12.174,
      "step": 10000
    },
    {
      "epoch": 2.539602715614785,
      "grad_norm": 3.2819769382476807,
      "learning_rate": 4.989811320754717e-05,
      "loss": 3.2382,
      "step": 10100
    },
    {
      "epoch": 2.564747296957506,
      "grad_norm": 3.409106492996216,
      "learning_rate": 4.9896855345911956e-05,
      "loss": 3.2323,
      "step": 10200
    },
    {
      "epoch": 2.589891878300226,
      "grad_norm": 3.1184515953063965,
      "learning_rate": 4.989559748427673e-05,
      "loss": 3.2384,
      "step": 10300
    },
    {
      "epoch": 2.6150364596429467,
      "grad_norm": 3.3903005123138428,
      "learning_rate": 4.989433962264151e-05,
      "loss": 3.2442,
      "step": 10400
    },
    {
      "epoch": 2.6150364596429467,
      "eval_loss": 3.137003183364868,
      "eval_runtime": 17.0468,
      "eval_samples_per_second": 785.719,
      "eval_steps_per_second": 12.319,
      "step": 10400
    },
    {
      "epoch": 2.6401810409856674,
      "grad_norm": 3.108081102371216,
      "learning_rate": 4.9893081761006295e-05,
      "loss": 3.2263,
      "step": 10500
    },
    {
      "epoch": 2.665325622328388,
      "grad_norm": 2.9966354370117188,
      "learning_rate": 4.989182389937107e-05,
      "loss": 3.2214,
      "step": 10600
    },
    {
      "epoch": 2.690470203671109,
      "grad_norm": 3.3046441078186035,
      "learning_rate": 4.989056603773585e-05,
      "loss": 3.2345,
      "step": 10700
    },
    {
      "epoch": 2.7156147850138295,
      "grad_norm": 3.2404956817626953,
      "learning_rate": 4.988930817610063e-05,
      "loss": 3.2232,
      "step": 10800
    },
    {
      "epoch": 2.7156147850138295,
      "eval_loss": 3.1246895790100098,
      "eval_runtime": 17.0349,
      "eval_samples_per_second": 786.266,
      "eval_steps_per_second": 12.328,
      "step": 10800
    },
    {
      "epoch": 2.74075936635655,
      "grad_norm": 3.2490105628967285,
      "learning_rate": 4.988805031446541e-05,
      "loss": 3.2059,
      "step": 10900
    },
    {
      "epoch": 2.765903947699271,
      "grad_norm": 3.2141122817993164,
      "learning_rate": 4.9886792452830186e-05,
      "loss": 3.2139,
      "step": 11000
    },
    {
      "epoch": 2.7910485290419915,
      "grad_norm": 3.324336051940918,
      "learning_rate": 4.988553459119497e-05,
      "loss": 3.209,
      "step": 11100
    },
    {
      "epoch": 2.816193110384712,
      "grad_norm": 3.3121178150177,
      "learning_rate": 4.988427672955975e-05,
      "loss": 3.2055,
      "step": 11200
    },
    {
      "epoch": 2.816193110384712,
      "eval_loss": 3.110912561416626,
      "eval_runtime": 17.0904,
      "eval_samples_per_second": 783.717,
      "eval_steps_per_second": 12.288,
      "step": 11200
    },
    {
      "epoch": 2.841337691727433,
      "grad_norm": 3.0214664936065674,
      "learning_rate": 4.988301886792453e-05,
      "loss": 3.1874,
      "step": 11300
    },
    {
      "epoch": 2.8664822730701536,
      "grad_norm": 3.0575435161590576,
      "learning_rate": 4.9881761006289315e-05,
      "loss": 3.196,
      "step": 11400
    },
    {
      "epoch": 2.8916268544128743,
      "grad_norm": 3.790926218032837,
      "learning_rate": 4.988050314465409e-05,
      "loss": 3.1961,
      "step": 11500
    },
    {
      "epoch": 2.916771435755595,
      "grad_norm": 3.2110321521759033,
      "learning_rate": 4.987924528301887e-05,
      "loss": 3.1867,
      "step": 11600
    },
    {
      "epoch": 2.916771435755595,
      "eval_loss": 3.095017194747925,
      "eval_runtime": 17.0402,
      "eval_samples_per_second": 786.022,
      "eval_steps_per_second": 12.324,
      "step": 11600
    },
    {
      "epoch": 2.941916017098315,
      "grad_norm": 3.216569662094116,
      "learning_rate": 4.987798742138365e-05,
      "loss": 3.1901,
      "step": 11700
    },
    {
      "epoch": 2.967060598441036,
      "grad_norm": 3.249577045440674,
      "learning_rate": 4.987672955974843e-05,
      "loss": 3.1849,
      "step": 11800
    },
    {
      "epoch": 2.9922051797837566,
      "grad_norm": 3.2408668994903564,
      "learning_rate": 4.9875471698113206e-05,
      "loss": 3.1784,
      "step": 11900
    },
    {
      "epoch": 3.0173497611264772,
      "grad_norm": 3.286583423614502,
      "learning_rate": 4.987421383647799e-05,
      "loss": 3.1597,
      "step": 12000
    },
    {
      "epoch": 3.0173497611264772,
      "eval_loss": 3.0863518714904785,
      "eval_runtime": 16.9256,
      "eval_samples_per_second": 791.345,
      "eval_steps_per_second": 12.407,
      "step": 12000
    },
    {
      "epoch": 3.042494342469198,
      "grad_norm": 3.239940881729126,
      "learning_rate": 4.987295597484277e-05,
      "loss": 3.1528,
      "step": 12100
    },
    {
      "epoch": 3.0676389238119186,
      "grad_norm": 3.328418493270874,
      "learning_rate": 4.9871698113207546e-05,
      "loss": 3.1613,
      "step": 12200
    },
    {
      "epoch": 3.0927835051546393,
      "grad_norm": 3.506582498550415,
      "learning_rate": 4.987044025157233e-05,
      "loss": 3.1467,
      "step": 12300
    },
    {
      "epoch": 3.11792808649736,
      "grad_norm": 3.0635907649993896,
      "learning_rate": 4.986918238993711e-05,
      "loss": 3.1351,
      "step": 12400
    },
    {
      "epoch": 3.11792808649736,
      "eval_loss": 3.074643135070801,
      "eval_runtime": 16.9685,
      "eval_samples_per_second": 789.344,
      "eval_steps_per_second": 12.376,
      "step": 12400
    },
    {
      "epoch": 3.1430726678400807,
      "grad_norm": 3.1829919815063477,
      "learning_rate": 4.986792452830189e-05,
      "loss": 3.1548,
      "step": 12500
    },
    {
      "epoch": 3.168217249182801,
      "grad_norm": 2.930508613586426,
      "learning_rate": 4.986666666666667e-05,
      "loss": 3.1357,
      "step": 12600
    },
    {
      "epoch": 3.1933618305255216,
      "grad_norm": 3.326347827911377,
      "learning_rate": 4.986540880503145e-05,
      "loss": 3.1557,
      "step": 12700
    },
    {
      "epoch": 3.2185064118682423,
      "grad_norm": 3.546126365661621,
      "learning_rate": 4.9864150943396226e-05,
      "loss": 3.1396,
      "step": 12800
    },
    {
      "epoch": 3.2185064118682423,
      "eval_loss": 3.0604262351989746,
      "eval_runtime": 16.9324,
      "eval_samples_per_second": 791.029,
      "eval_steps_per_second": 12.402,
      "step": 12800
    },
    {
      "epoch": 3.243650993210963,
      "grad_norm": 3.4438893795013428,
      "learning_rate": 4.986289308176101e-05,
      "loss": 3.138,
      "step": 12900
    },
    {
      "epoch": 3.2687955745536836,
      "grad_norm": 3.3809847831726074,
      "learning_rate": 4.986163522012579e-05,
      "loss": 3.1421,
      "step": 13000
    },
    {
      "epoch": 3.2939401558964043,
      "grad_norm": 3.6093921661376953,
      "learning_rate": 4.9860377358490566e-05,
      "loss": 3.1411,
      "step": 13100
    },
    {
      "epoch": 3.319084737239125,
      "grad_norm": 3.268341302871704,
      "learning_rate": 4.985911949685535e-05,
      "loss": 3.1294,
      "step": 13200
    },
    {
      "epoch": 3.319084737239125,
      "eval_loss": 3.0487284660339355,
      "eval_runtime": 16.9256,
      "eval_samples_per_second": 791.346,
      "eval_steps_per_second": 12.407,
      "step": 13200
    },
    {
      "epoch": 3.3442293185818457,
      "grad_norm": 3.0633366107940674,
      "learning_rate": 4.985786163522012e-05,
      "loss": 3.1312,
      "step": 13300
    },
    {
      "epoch": 3.3693738999245664,
      "grad_norm": 3.3489363193511963,
      "learning_rate": 4.9856603773584906e-05,
      "loss": 3.1311,
      "step": 13400
    },
    {
      "epoch": 3.394518481267287,
      "grad_norm": 3.569866895675659,
      "learning_rate": 4.985534591194969e-05,
      "loss": 3.1254,
      "step": 13500
    },
    {
      "epoch": 3.4196630626100077,
      "grad_norm": 3.1111812591552734,
      "learning_rate": 4.985408805031447e-05,
      "loss": 3.1276,
      "step": 13600
    },
    {
      "epoch": 3.4196630626100077,
      "eval_loss": 3.0381853580474854,
      "eval_runtime": 17.3148,
      "eval_samples_per_second": 773.556,
      "eval_steps_per_second": 12.128,
      "step": 13600
    },
    {
      "epoch": 3.4448076439527284,
      "grad_norm": 3.1951229572296143,
      "learning_rate": 4.985283018867925e-05,
      "loss": 3.1191,
      "step": 13700
    },
    {
      "epoch": 3.4699522252954487,
      "grad_norm": 3.221646308898926,
      "learning_rate": 4.985157232704403e-05,
      "loss": 3.1221,
      "step": 13800
    },
    {
      "epoch": 3.4950968066381694,
      "grad_norm": 3.0413296222686768,
      "learning_rate": 4.985031446540881e-05,
      "loss": 3.1111,
      "step": 13900
    },
    {
      "epoch": 3.52024138798089,
      "grad_norm": 3.3255045413970947,
      "learning_rate": 4.9849056603773585e-05,
      "loss": 3.1101,
      "step": 14000
    },
    {
      "epoch": 3.52024138798089,
      "eval_loss": 3.0265307426452637,
      "eval_runtime": 16.9445,
      "eval_samples_per_second": 790.465,
      "eval_steps_per_second": 12.393,
      "step": 14000
    },
    {
      "epoch": 3.5453859693236107,
      "grad_norm": 3.2668678760528564,
      "learning_rate": 4.984779874213837e-05,
      "loss": 3.1068,
      "step": 14100
    },
    {
      "epoch": 3.5705305506663314,
      "grad_norm": 3.407388925552368,
      "learning_rate": 4.984654088050314e-05,
      "loss": 3.1018,
      "step": 14200
    },
    {
      "epoch": 3.595675132009052,
      "grad_norm": 3.11242938041687,
      "learning_rate": 4.9845283018867925e-05,
      "loss": 3.1109,
      "step": 14300
    },
    {
      "epoch": 3.6208197133517728,
      "grad_norm": 3.459291696548462,
      "learning_rate": 4.984402515723271e-05,
      "loss": 3.1096,
      "step": 14400
    },
    {
      "epoch": 3.6208197133517728,
      "eval_loss": 3.019886016845703,
      "eval_runtime": 16.9673,
      "eval_samples_per_second": 789.402,
      "eval_steps_per_second": 12.377,
      "step": 14400
    },
    {
      "epoch": 3.6459642946944935,
      "grad_norm": 3.211521863937378,
      "learning_rate": 4.984276729559749e-05,
      "loss": 3.1117,
      "step": 14500
    },
    {
      "epoch": 3.671108876037214,
      "grad_norm": 3.141036033630371,
      "learning_rate": 4.984150943396227e-05,
      "loss": 3.1129,
      "step": 14600
    },
    {
      "epoch": 3.6962534573799344,
      "grad_norm": 3.181434392929077,
      "learning_rate": 4.984025157232705e-05,
      "loss": 3.0883,
      "step": 14700
    },
    {
      "epoch": 3.721398038722655,
      "grad_norm": 3.3015658855438232,
      "learning_rate": 4.983899371069183e-05,
      "loss": 3.0951,
      "step": 14800
    },
    {
      "epoch": 3.721398038722655,
      "eval_loss": 3.0119576454162598,
      "eval_runtime": 17.0155,
      "eval_samples_per_second": 787.165,
      "eval_steps_per_second": 12.342,
      "step": 14800
    },
    {
      "epoch": 3.7465426200653758,
      "grad_norm": 3.2170772552490234,
      "learning_rate": 4.9837735849056605e-05,
      "loss": 3.0996,
      "step": 14900
    },
    {
      "epoch": 3.7716872014080964,
      "grad_norm": 3.18080472946167,
      "learning_rate": 4.983647798742139e-05,
      "loss": 3.0974,
      "step": 15000
    },
    {
      "epoch": 3.796831782750817,
      "grad_norm": 3.071488618850708,
      "learning_rate": 4.983522012578616e-05,
      "loss": 3.0991,
      "step": 15100
    },
    {
      "epoch": 3.821976364093538,
      "grad_norm": 3.3020617961883545,
      "learning_rate": 4.9833962264150945e-05,
      "loss": 3.0831,
      "step": 15200
    },
    {
      "epoch": 3.821976364093538,
      "eval_loss": 3.0022740364074707,
      "eval_runtime": 17.0406,
      "eval_samples_per_second": 786.004,
      "eval_steps_per_second": 12.323,
      "step": 15200
    },
    {
      "epoch": 3.8471209454362585,
      "grad_norm": 3.401118278503418,
      "learning_rate": 4.983270440251572e-05,
      "loss": 3.1013,
      "step": 15300
    },
    {
      "epoch": 3.872265526778979,
      "grad_norm": 3.1370928287506104,
      "learning_rate": 4.98314465408805e-05,
      "loss": 3.0915,
      "step": 15400
    },
    {
      "epoch": 3.8974101081217,
      "grad_norm": 3.4118762016296387,
      "learning_rate": 4.9830188679245285e-05,
      "loss": 3.0878,
      "step": 15500
    },
    {
      "epoch": 3.9225546894644205,
      "grad_norm": 3.0219292640686035,
      "learning_rate": 4.982893081761007e-05,
      "loss": 3.0802,
      "step": 15600
    },
    {
      "epoch": 3.9225546894644205,
      "eval_loss": 2.9958338737487793,
      "eval_runtime": 16.9213,
      "eval_samples_per_second": 791.545,
      "eval_steps_per_second": 12.41,
      "step": 15600
    },
    {
      "epoch": 3.9476992708071412,
      "grad_norm": 3.2867064476013184,
      "learning_rate": 4.982767295597485e-05,
      "loss": 3.0707,
      "step": 15700
    },
    {
      "epoch": 3.972843852149862,
      "grad_norm": 3.313915252685547,
      "learning_rate": 4.9826415094339625e-05,
      "loss": 3.071,
      "step": 15800
    },
    {
      "epoch": 3.9979884334925826,
      "grad_norm": 3.0831639766693115,
      "learning_rate": 4.982515723270441e-05,
      "loss": 3.0801,
      "step": 15900
    },
    {
      "epoch": 4.023133014835303,
      "grad_norm": 3.530923366546631,
      "learning_rate": 4.982389937106918e-05,
      "loss": 3.0496,
      "step": 16000
    },
    {
      "epoch": 4.023133014835303,
      "eval_loss": 2.986414909362793,
      "eval_runtime": 16.9742,
      "eval_samples_per_second": 789.079,
      "eval_steps_per_second": 12.372,
      "step": 16000
    },
    {
      "epoch": 4.048277596178024,
      "grad_norm": 3.2932159900665283,
      "learning_rate": 4.9822641509433965e-05,
      "loss": 3.0703,
      "step": 16100
    },
    {
      "epoch": 4.073422177520745,
      "grad_norm": 3.1299328804016113,
      "learning_rate": 4.982138364779875e-05,
      "loss": 3.0575,
      "step": 16200
    },
    {
      "epoch": 4.098566758863465,
      "grad_norm": 3.3408172130584717,
      "learning_rate": 4.982012578616352e-05,
      "loss": 3.0549,
      "step": 16300
    },
    {
      "epoch": 4.123711340206185,
      "grad_norm": 3.2257726192474365,
      "learning_rate": 4.9818867924528305e-05,
      "loss": 3.0484,
      "step": 16400
    },
    {
      "epoch": 4.123711340206185,
      "eval_loss": 2.9788782596588135,
      "eval_runtime": 17.1545,
      "eval_samples_per_second": 780.784,
      "eval_steps_per_second": 12.242,
      "step": 16400
    },
    {
      "epoch": 4.148855921548906,
      "grad_norm": 3.3352255821228027,
      "learning_rate": 4.981761006289308e-05,
      "loss": 3.035,
      "step": 16500
    },
    {
      "epoch": 4.1740005028916265,
      "grad_norm": 3.060375452041626,
      "learning_rate": 4.981635220125786e-05,
      "loss": 3.0531,
      "step": 16600
    },
    {
      "epoch": 4.199145084234347,
      "grad_norm": 3.235373020172119,
      "learning_rate": 4.9815094339622645e-05,
      "loss": 3.0454,
      "step": 16700
    },
    {
      "epoch": 4.224289665577068,
      "grad_norm": 3.4568979740142822,
      "learning_rate": 4.981383647798743e-05,
      "loss": 3.0368,
      "step": 16800
    },
    {
      "epoch": 4.224289665577068,
      "eval_loss": 2.9714553356170654,
      "eval_runtime": 17.3011,
      "eval_samples_per_second": 774.168,
      "eval_steps_per_second": 12.138,
      "step": 16800
    },
    {
      "epoch": 4.2494342469197885,
      "grad_norm": 3.0740644931793213,
      "learning_rate": 4.98125786163522e-05,
      "loss": 3.0412,
      "step": 16900
    },
    {
      "epoch": 4.274578828262509,
      "grad_norm": 2.9110569953918457,
      "learning_rate": 4.9811320754716985e-05,
      "loss": 3.0416,
      "step": 17000
    },
    {
      "epoch": 4.29972340960523,
      "grad_norm": 3.348515272140503,
      "learning_rate": 4.981006289308177e-05,
      "loss": 3.0466,
      "step": 17100
    },
    {
      "epoch": 4.324867990947951,
      "grad_norm": 3.2793750762939453,
      "learning_rate": 4.980880503144654e-05,
      "loss": 3.0514,
      "step": 17200
    },
    {
      "epoch": 4.324867990947951,
      "eval_loss": 2.97037935256958,
      "eval_runtime": 17.1114,
      "eval_samples_per_second": 782.754,
      "eval_steps_per_second": 12.273,
      "step": 17200
    },
    {
      "epoch": 4.350012572290671,
      "grad_norm": 3.4056458473205566,
      "learning_rate": 4.9807547169811324e-05,
      "loss": 3.0384,
      "step": 17300
    },
    {
      "epoch": 4.375157153633392,
      "grad_norm": 3.328766107559204,
      "learning_rate": 4.98062893081761e-05,
      "loss": 3.0503,
      "step": 17400
    },
    {
      "epoch": 4.400301734976113,
      "grad_norm": 3.147191286087036,
      "learning_rate": 4.980503144654088e-05,
      "loss": 3.0238,
      "step": 17500
    },
    {
      "epoch": 4.425446316318833,
      "grad_norm": 3.1616384983062744,
      "learning_rate": 4.980377358490566e-05,
      "loss": 3.0445,
      "step": 17600
    },
    {
      "epoch": 4.425446316318833,
      "eval_loss": 2.960465431213379,
      "eval_runtime": 17.4167,
      "eval_samples_per_second": 769.033,
      "eval_steps_per_second": 12.057,
      "step": 17600
    },
    {
      "epoch": 4.450590897661554,
      "grad_norm": 3.0788564682006836,
      "learning_rate": 4.980251572327044e-05,
      "loss": 3.0315,
      "step": 17700
    },
    {
      "epoch": 4.475735479004275,
      "grad_norm": 3.2862465381622314,
      "learning_rate": 4.980125786163522e-05,
      "loss": 3.0307,
      "step": 17800
    },
    {
      "epoch": 4.500880060346995,
      "grad_norm": 3.022813320159912,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 3.0253,
      "step": 17900
    },
    {
      "epoch": 4.526024641689716,
      "grad_norm": 3.1379988193511963,
      "learning_rate": 4.9798742138364787e-05,
      "loss": 3.0335,
      "step": 18000
    },
    {
      "epoch": 4.526024641689716,
      "eval_loss": 2.953307628631592,
      "eval_runtime": 17.3435,
      "eval_samples_per_second": 772.277,
      "eval_steps_per_second": 12.108,
      "step": 18000
    },
    {
      "epoch": 4.551169223032437,
      "grad_norm": 3.2055416107177734,
      "learning_rate": 4.979748427672956e-05,
      "loss": 3.0333,
      "step": 18100
    },
    {
      "epoch": 4.5763138043751574,
      "grad_norm": 3.237699031829834,
      "learning_rate": 4.9796226415094344e-05,
      "loss": 3.0309,
      "step": 18200
    },
    {
      "epoch": 4.601458385717878,
      "grad_norm": 3.2134957313537598,
      "learning_rate": 4.979496855345912e-05,
      "loss": 3.032,
      "step": 18300
    },
    {
      "epoch": 4.626602967060599,
      "grad_norm": 3.16802978515625,
      "learning_rate": 4.97937106918239e-05,
      "loss": 3.0267,
      "step": 18400
    },
    {
      "epoch": 4.626602967060599,
      "eval_loss": 2.9446210861206055,
      "eval_runtime": 17.2905,
      "eval_samples_per_second": 774.647,
      "eval_steps_per_second": 12.145,
      "step": 18400
    },
    {
      "epoch": 4.6517475484033195,
      "grad_norm": 3.366682767868042,
      "learning_rate": 4.979245283018868e-05,
      "loss": 3.0308,
      "step": 18500
    },
    {
      "epoch": 4.67689212974604,
      "grad_norm": 3.0189626216888428,
      "learning_rate": 4.979119496855346e-05,
      "loss": 3.0208,
      "step": 18600
    },
    {
      "epoch": 4.702036711088761,
      "grad_norm": 3.143533945083618,
      "learning_rate": 4.978993710691824e-05,
      "loss": 3.0121,
      "step": 18700
    },
    {
      "epoch": 4.727181292431481,
      "grad_norm": 3.201019048690796,
      "learning_rate": 4.9788679245283024e-05,
      "loss": 3.0137,
      "step": 18800
    },
    {
      "epoch": 4.727181292431481,
      "eval_loss": 2.9370319843292236,
      "eval_runtime": 17.4817,
      "eval_samples_per_second": 766.173,
      "eval_steps_per_second": 12.013,
      "step": 18800
    },
    {
      "epoch": 4.752325873774201,
      "grad_norm": 3.4422566890716553,
      "learning_rate": 4.9787421383647806e-05,
      "loss": 3.0054,
      "step": 18900
    },
    {
      "epoch": 4.777470455116922,
      "grad_norm": 3.3498260974884033,
      "learning_rate": 4.978616352201258e-05,
      "loss": 3.0138,
      "step": 19000
    },
    {
      "epoch": 4.802615036459643,
      "grad_norm": 3.2466423511505127,
      "learning_rate": 4.9784905660377364e-05,
      "loss": 3.0199,
      "step": 19100
    },
    {
      "epoch": 4.827759617802363,
      "grad_norm": 3.2961678504943848,
      "learning_rate": 4.978364779874214e-05,
      "loss": 3.0186,
      "step": 19200
    },
    {
      "epoch": 4.827759617802363,
      "eval_loss": 2.931539535522461,
      "eval_runtime": 17.2826,
      "eval_samples_per_second": 775.0,
      "eval_steps_per_second": 12.151,
      "step": 19200
    },
    {
      "epoch": 4.852904199145084,
      "grad_norm": 3.4730100631713867,
      "learning_rate": 4.978238993710692e-05,
      "loss": 3.0104,
      "step": 19300
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 3.2286760807037354,
      "learning_rate": 4.97811320754717e-05,
      "loss": 3.0152,
      "step": 19400
    },
    {
      "epoch": 4.9031933618305255,
      "grad_norm": 3.0102126598358154,
      "learning_rate": 4.977987421383648e-05,
      "loss": 3.0066,
      "step": 19500
    },
    {
      "epoch": 4.928337943173246,
      "grad_norm": 3.1196751594543457,
      "learning_rate": 4.977861635220126e-05,
      "loss": 2.9999,
      "step": 19600
    },
    {
      "epoch": 4.928337943173246,
      "eval_loss": 2.925804615020752,
      "eval_runtime": 17.2718,
      "eval_samples_per_second": 775.483,
      "eval_steps_per_second": 12.159,
      "step": 19600
    },
    {
      "epoch": 4.953482524515967,
      "grad_norm": 3.0210041999816895,
      "learning_rate": 4.977735849056604e-05,
      "loss": 3.006,
      "step": 19700
    },
    {
      "epoch": 4.9786271058586875,
      "grad_norm": 3.1058528423309326,
      "learning_rate": 4.977610062893082e-05,
      "loss": 2.9947,
      "step": 19800
    },
    {
      "epoch": 5.003771687201408,
      "grad_norm": 3.039834499359131,
      "learning_rate": 4.97748427672956e-05,
      "loss": 3.0052,
      "step": 19900
    },
    {
      "epoch": 5.028916268544129,
      "grad_norm": 3.1175308227539062,
      "learning_rate": 4.9773584905660384e-05,
      "loss": 2.9737,
      "step": 20000
    },
    {
      "epoch": 5.028916268544129,
      "eval_loss": 2.9254331588745117,
      "eval_runtime": 17.3171,
      "eval_samples_per_second": 773.453,
      "eval_steps_per_second": 12.127,
      "step": 20000
    },
    {
      "epoch": 5.05406084988685,
      "grad_norm": 3.035027503967285,
      "learning_rate": 4.977232704402516e-05,
      "loss": 2.9769,
      "step": 20100
    },
    {
      "epoch": 5.07920543122957,
      "grad_norm": 3.05094838142395,
      "learning_rate": 4.977106918238994e-05,
      "loss": 2.9851,
      "step": 20200
    },
    {
      "epoch": 5.104350012572291,
      "grad_norm": 3.09420108795166,
      "learning_rate": 4.976981132075472e-05,
      "loss": 2.9846,
      "step": 20300
    },
    {
      "epoch": 5.129494593915012,
      "grad_norm": 3.1761434078216553,
      "learning_rate": 4.97685534591195e-05,
      "loss": 2.9877,
      "step": 20400
    },
    {
      "epoch": 5.129494593915012,
      "eval_loss": 2.9173569679260254,
      "eval_runtime": 17.2506,
      "eval_samples_per_second": 776.437,
      "eval_steps_per_second": 12.173,
      "step": 20400
    },
    {
      "epoch": 5.154639175257732,
      "grad_norm": 3.092113971710205,
      "learning_rate": 4.976729559748428e-05,
      "loss": 2.9812,
      "step": 20500
    },
    {
      "epoch": 5.179783756600453,
      "grad_norm": 3.214935064315796,
      "learning_rate": 4.976603773584906e-05,
      "loss": 2.9602,
      "step": 20600
    },
    {
      "epoch": 5.204928337943174,
      "grad_norm": 3.2591516971588135,
      "learning_rate": 4.976477987421384e-05,
      "loss": 2.9955,
      "step": 20700
    },
    {
      "epoch": 5.2300729192858935,
      "grad_norm": 3.33280086517334,
      "learning_rate": 4.9763522012578614e-05,
      "loss": 2.983,
      "step": 20800
    },
    {
      "epoch": 5.2300729192858935,
      "eval_loss": 2.9118759632110596,
      "eval_runtime": 17.3249,
      "eval_samples_per_second": 773.106,
      "eval_steps_per_second": 12.121,
      "step": 20800
    },
    {
      "epoch": 5.255217500628614,
      "grad_norm": 3.372831344604492,
      "learning_rate": 4.97622641509434e-05,
      "loss": 2.9676,
      "step": 20900
    },
    {
      "epoch": 5.280362081971335,
      "grad_norm": 2.8661346435546875,
      "learning_rate": 4.976100628930818e-05,
      "loss": 2.9774,
      "step": 21000
    },
    {
      "epoch": 5.3055066633140555,
      "grad_norm": 3.130908250808716,
      "learning_rate": 4.975974842767296e-05,
      "loss": 2.9715,
      "step": 21100
    },
    {
      "epoch": 5.330651244656776,
      "grad_norm": 3.368299722671509,
      "learning_rate": 4.975849056603774e-05,
      "loss": 2.9728,
      "step": 21200
    },
    {
      "epoch": 5.330651244656776,
      "eval_loss": 2.9055838584899902,
      "eval_runtime": 17.2794,
      "eval_samples_per_second": 775.143,
      "eval_steps_per_second": 12.153,
      "step": 21200
    },
    {
      "epoch": 5.355795825999497,
      "grad_norm": 3.0598654747009277,
      "learning_rate": 4.975723270440252e-05,
      "loss": 2.9726,
      "step": 21300
    },
    {
      "epoch": 5.380940407342218,
      "grad_norm": 2.9822943210601807,
      "learning_rate": 4.97559748427673e-05,
      "loss": 2.9743,
      "step": 21400
    },
    {
      "epoch": 5.406084988684938,
      "grad_norm": 3.036534309387207,
      "learning_rate": 4.9754716981132077e-05,
      "loss": 2.9682,
      "step": 21500
    },
    {
      "epoch": 5.431229570027659,
      "grad_norm": 3.229757785797119,
      "learning_rate": 4.975345911949686e-05,
      "loss": 2.9796,
      "step": 21600
    },
    {
      "epoch": 5.431229570027659,
      "eval_loss": 2.9024343490600586,
      "eval_runtime": 17.1566,
      "eval_samples_per_second": 780.69,
      "eval_steps_per_second": 12.24,
      "step": 21600
    },
    {
      "epoch": 5.45637415137038,
      "grad_norm": 3.1789987087249756,
      "learning_rate": 4.9752201257861634e-05,
      "loss": 2.9847,
      "step": 21700
    },
    {
      "epoch": 5.4815187327131,
      "grad_norm": 3.1888225078582764,
      "learning_rate": 4.9750943396226416e-05,
      "loss": 2.9816,
      "step": 21800
    },
    {
      "epoch": 5.506663314055821,
      "grad_norm": 3.082504987716675,
      "learning_rate": 4.974968553459119e-05,
      "loss": 2.9754,
      "step": 21900
    },
    {
      "epoch": 5.531807895398542,
      "grad_norm": 3.257563591003418,
      "learning_rate": 4.9748427672955974e-05,
      "loss": 2.9723,
      "step": 22000
    },
    {
      "epoch": 5.531807895398542,
      "eval_loss": 2.8964130878448486,
      "eval_runtime": 17.119,
      "eval_samples_per_second": 782.407,
      "eval_steps_per_second": 12.267,
      "step": 22000
    },
    {
      "epoch": 5.556952476741262,
      "grad_norm": 3.301772356033325,
      "learning_rate": 4.9747169811320756e-05,
      "loss": 2.9703,
      "step": 22100
    },
    {
      "epoch": 5.582097058083983,
      "grad_norm": 3.286318778991699,
      "learning_rate": 4.974591194968554e-05,
      "loss": 2.9618,
      "step": 22200
    },
    {
      "epoch": 5.607241639426704,
      "grad_norm": 3.2719337940216064,
      "learning_rate": 4.974465408805032e-05,
      "loss": 2.965,
      "step": 22300
    },
    {
      "epoch": 5.632386220769424,
      "grad_norm": 2.9728336334228516,
      "learning_rate": 4.9743396226415096e-05,
      "loss": 2.9662,
      "step": 22400
    },
    {
      "epoch": 5.632386220769424,
      "eval_loss": 2.8961992263793945,
      "eval_runtime": 17.4108,
      "eval_samples_per_second": 769.293,
      "eval_steps_per_second": 12.061,
      "step": 22400
    },
    {
      "epoch": 5.657530802112145,
      "grad_norm": 3.4089202880859375,
      "learning_rate": 4.974213836477988e-05,
      "loss": 2.9606,
      "step": 22500
    },
    {
      "epoch": 5.682675383454866,
      "grad_norm": 2.9369120597839355,
      "learning_rate": 4.9740880503144654e-05,
      "loss": 2.9655,
      "step": 22600
    },
    {
      "epoch": 5.7078199647975865,
      "grad_norm": 3.5443665981292725,
      "learning_rate": 4.9739622641509436e-05,
      "loss": 2.9693,
      "step": 22700
    },
    {
      "epoch": 5.732964546140307,
      "grad_norm": 3.1329479217529297,
      "learning_rate": 4.973836477987421e-05,
      "loss": 2.9435,
      "step": 22800
    },
    {
      "epoch": 5.732964546140307,
      "eval_loss": 2.8901901245117188,
      "eval_runtime": 16.9568,
      "eval_samples_per_second": 789.89,
      "eval_steps_per_second": 12.384,
      "step": 22800
    },
    {
      "epoch": 5.758109127483028,
      "grad_norm": 3.2598369121551514,
      "learning_rate": 4.9737106918238994e-05,
      "loss": 2.9623,
      "step": 22900
    },
    {
      "epoch": 5.7832537088257485,
      "grad_norm": 3.2294814586639404,
      "learning_rate": 4.9735849056603776e-05,
      "loss": 2.9522,
      "step": 23000
    },
    {
      "epoch": 5.808398290168468,
      "grad_norm": 3.170654773712158,
      "learning_rate": 4.973459119496856e-05,
      "loss": 2.9511,
      "step": 23100
    },
    {
      "epoch": 5.833542871511189,
      "grad_norm": 3.2661447525024414,
      "learning_rate": 4.973333333333334e-05,
      "loss": 2.9548,
      "step": 23200
    },
    {
      "epoch": 5.833542871511189,
      "eval_loss": 2.8836798667907715,
      "eval_runtime": 17.0168,
      "eval_samples_per_second": 787.103,
      "eval_steps_per_second": 12.341,
      "step": 23200
    },
    {
      "epoch": 5.85868745285391,
      "grad_norm": 3.0046560764312744,
      "learning_rate": 4.9732075471698116e-05,
      "loss": 2.9628,
      "step": 23300
    },
    {
      "epoch": 5.88383203419663,
      "grad_norm": 2.8649797439575195,
      "learning_rate": 4.97308176100629e-05,
      "loss": 2.9653,
      "step": 23400
    },
    {
      "epoch": 5.908976615539351,
      "grad_norm": 3.144092321395874,
      "learning_rate": 4.9729559748427674e-05,
      "loss": 2.9684,
      "step": 23500
    },
    {
      "epoch": 5.934121196882072,
      "grad_norm": 2.989419460296631,
      "learning_rate": 4.9728301886792456e-05,
      "loss": 2.9565,
      "step": 23600
    },
    {
      "epoch": 5.934121196882072,
      "eval_loss": 2.8789405822753906,
      "eval_runtime": 17.1301,
      "eval_samples_per_second": 781.899,
      "eval_steps_per_second": 12.259,
      "step": 23600
    },
    {
      "epoch": 5.959265778224792,
      "grad_norm": 3.0526955127716064,
      "learning_rate": 4.972704402515724e-05,
      "loss": 2.9377,
      "step": 23700
    },
    {
      "epoch": 5.984410359567513,
      "grad_norm": 3.210538864135742,
      "learning_rate": 4.9725786163522014e-05,
      "loss": 2.952,
      "step": 23800
    },
    {
      "epoch": 6.009554940910234,
      "grad_norm": 3.377612829208374,
      "learning_rate": 4.9724528301886796e-05,
      "loss": 2.9489,
      "step": 23900
    },
    {
      "epoch": 6.0346995222529545,
      "grad_norm": 3.02586030960083,
      "learning_rate": 4.972327044025157e-05,
      "loss": 2.9232,
      "step": 24000
    },
    {
      "epoch": 6.0346995222529545,
      "eval_loss": 2.874569892883301,
      "eval_runtime": 17.086,
      "eval_samples_per_second": 783.919,
      "eval_steps_per_second": 12.291,
      "step": 24000
    },
    {
      "epoch": 6.059844103595675,
      "grad_norm": 3.4383156299591064,
      "learning_rate": 4.9722012578616354e-05,
      "loss": 2.929,
      "step": 24100
    },
    {
      "epoch": 6.084988684938396,
      "grad_norm": 3.1953835487365723,
      "learning_rate": 4.9720754716981136e-05,
      "loss": 2.9326,
      "step": 24200
    },
    {
      "epoch": 6.1101332662811165,
      "grad_norm": 3.0457828044891357,
      "learning_rate": 4.971949685534592e-05,
      "loss": 2.9255,
      "step": 24300
    },
    {
      "epoch": 6.135277847623837,
      "grad_norm": 2.9847915172576904,
      "learning_rate": 4.9718238993710693e-05,
      "loss": 2.9285,
      "step": 24400
    },
    {
      "epoch": 6.135277847623837,
      "eval_loss": 2.873541831970215,
      "eval_runtime": 17.11,
      "eval_samples_per_second": 782.815,
      "eval_steps_per_second": 12.273,
      "step": 24400
    },
    {
      "epoch": 6.160422428966558,
      "grad_norm": 3.0332891941070557,
      "learning_rate": 4.9716981132075476e-05,
      "loss": 2.9304,
      "step": 24500
    },
    {
      "epoch": 6.185567010309279,
      "grad_norm": 3.1579761505126953,
      "learning_rate": 4.971572327044026e-05,
      "loss": 2.9286,
      "step": 24600
    },
    {
      "epoch": 6.210711591651999,
      "grad_norm": 3.265076160430908,
      "learning_rate": 4.971446540880503e-05,
      "loss": 2.9103,
      "step": 24700
    },
    {
      "epoch": 6.23585617299472,
      "grad_norm": 3.115551471710205,
      "learning_rate": 4.9713207547169816e-05,
      "loss": 2.9243,
      "step": 24800
    },
    {
      "epoch": 6.23585617299472,
      "eval_loss": 2.8682100772857666,
      "eval_runtime": 17.1757,
      "eval_samples_per_second": 779.822,
      "eval_steps_per_second": 12.227,
      "step": 24800
    },
    {
      "epoch": 6.261000754337441,
      "grad_norm": 3.056861400604248,
      "learning_rate": 4.971194968553459e-05,
      "loss": 2.9252,
      "step": 24900
    },
    {
      "epoch": 6.286145335680161,
      "grad_norm": 2.9022204875946045,
      "learning_rate": 4.971069182389937e-05,
      "loss": 2.9239,
      "step": 25000
    },
    {
      "epoch": 6.311289917022881,
      "grad_norm": 3.200631618499756,
      "learning_rate": 4.970943396226415e-05,
      "loss": 2.9206,
      "step": 25100
    },
    {
      "epoch": 6.336434498365602,
      "grad_norm": 3.2805416584014893,
      "learning_rate": 4.970817610062893e-05,
      "loss": 2.9375,
      "step": 25200
    },
    {
      "epoch": 6.336434498365602,
      "eval_loss": 2.864673376083374,
      "eval_runtime": 17.0942,
      "eval_samples_per_second": 783.538,
      "eval_steps_per_second": 12.285,
      "step": 25200
    },
    {
      "epoch": 6.3615790797083225,
      "grad_norm": 3.151275396347046,
      "learning_rate": 4.970691823899371e-05,
      "loss": 2.9428,
      "step": 25300
    },
    {
      "epoch": 6.386723661051043,
      "grad_norm": 2.8842952251434326,
      "learning_rate": 4.9705660377358495e-05,
      "loss": 2.9206,
      "step": 25400
    },
    {
      "epoch": 6.411868242393764,
      "grad_norm": 3.210556745529175,
      "learning_rate": 4.970440251572328e-05,
      "loss": 2.9272,
      "step": 25500
    },
    {
      "epoch": 6.4370128237364845,
      "grad_norm": 3.007498025894165,
      "learning_rate": 4.970314465408805e-05,
      "loss": 2.9135,
      "step": 25600
    },
    {
      "epoch": 6.4370128237364845,
      "eval_loss": 2.862644672393799,
      "eval_runtime": 17.1614,
      "eval_samples_per_second": 780.474,
      "eval_steps_per_second": 12.237,
      "step": 25600
    },
    {
      "epoch": 6.462157405079205,
      "grad_norm": 2.9586400985717773,
      "learning_rate": 4.9701886792452835e-05,
      "loss": 2.9222,
      "step": 25700
    },
    {
      "epoch": 6.487301986421926,
      "grad_norm": 3.314910650253296,
      "learning_rate": 4.970062893081761e-05,
      "loss": 2.9271,
      "step": 25800
    },
    {
      "epoch": 6.512446567764647,
      "grad_norm": 3.0636842250823975,
      "learning_rate": 4.969937106918239e-05,
      "loss": 2.9328,
      "step": 25900
    },
    {
      "epoch": 6.537591149107367,
      "grad_norm": 3.233302354812622,
      "learning_rate": 4.969811320754717e-05,
      "loss": 2.9324,
      "step": 26000
    },
    {
      "epoch": 6.537591149107367,
      "eval_loss": 2.8549017906188965,
      "eval_runtime": 17.2568,
      "eval_samples_per_second": 776.157,
      "eval_steps_per_second": 12.169,
      "step": 26000
    },
    {
      "epoch": 6.562735730450088,
      "grad_norm": 3.094146966934204,
      "learning_rate": 4.969685534591195e-05,
      "loss": 2.9243,
      "step": 26100
    },
    {
      "epoch": 6.587880311792809,
      "grad_norm": 3.0283596515655518,
      "learning_rate": 4.969559748427673e-05,
      "loss": 2.9058,
      "step": 26200
    },
    {
      "epoch": 6.613024893135529,
      "grad_norm": 2.9787561893463135,
      "learning_rate": 4.969433962264151e-05,
      "loss": 2.907,
      "step": 26300
    },
    {
      "epoch": 6.63816947447825,
      "grad_norm": 3.22367000579834,
      "learning_rate": 4.969308176100629e-05,
      "loss": 2.9178,
      "step": 26400
    },
    {
      "epoch": 6.63816947447825,
      "eval_loss": 2.8557443618774414,
      "eval_runtime": 17.1886,
      "eval_samples_per_second": 779.237,
      "eval_steps_per_second": 12.217,
      "step": 26400
    },
    {
      "epoch": 6.663314055820971,
      "grad_norm": 3.345698356628418,
      "learning_rate": 4.969182389937107e-05,
      "loss": 2.9265,
      "step": 26500
    },
    {
      "epoch": 6.688458637163691,
      "grad_norm": 2.8766777515411377,
      "learning_rate": 4.9690566037735855e-05,
      "loss": 2.9083,
      "step": 26600
    },
    {
      "epoch": 6.713603218506412,
      "grad_norm": 3.0615363121032715,
      "learning_rate": 4.968930817610063e-05,
      "loss": 2.9192,
      "step": 26700
    },
    {
      "epoch": 6.738747799849133,
      "grad_norm": 2.9568700790405273,
      "learning_rate": 4.968805031446541e-05,
      "loss": 2.9164,
      "step": 26800
    },
    {
      "epoch": 6.738747799849133,
      "eval_loss": 2.848145008087158,
      "eval_runtime": 17.1382,
      "eval_samples_per_second": 781.527,
      "eval_steps_per_second": 12.253,
      "step": 26800
    },
    {
      "epoch": 6.763892381191853,
      "grad_norm": 2.9083452224731445,
      "learning_rate": 4.968679245283019e-05,
      "loss": 2.9193,
      "step": 26900
    },
    {
      "epoch": 6.789036962534574,
      "grad_norm": 3.068045139312744,
      "learning_rate": 4.968553459119497e-05,
      "loss": 2.9067,
      "step": 27000
    },
    {
      "epoch": 6.814181543877295,
      "grad_norm": 3.1563849449157715,
      "learning_rate": 4.968427672955975e-05,
      "loss": 2.9152,
      "step": 27100
    },
    {
      "epoch": 6.8393261252200155,
      "grad_norm": 3.1741394996643066,
      "learning_rate": 4.968301886792453e-05,
      "loss": 2.9103,
      "step": 27200
    },
    {
      "epoch": 6.8393261252200155,
      "eval_loss": 2.8479220867156982,
      "eval_runtime": 17.2285,
      "eval_samples_per_second": 777.432,
      "eval_steps_per_second": 12.189,
      "step": 27200
    },
    {
      "epoch": 6.864470706562736,
      "grad_norm": 3.070336103439331,
      "learning_rate": 4.968176100628931e-05,
      "loss": 2.907,
      "step": 27300
    },
    {
      "epoch": 6.889615287905457,
      "grad_norm": 2.937831163406372,
      "learning_rate": 4.9680503144654086e-05,
      "loss": 2.9233,
      "step": 27400
    },
    {
      "epoch": 6.914759869248177,
      "grad_norm": 2.9475855827331543,
      "learning_rate": 4.9679245283018875e-05,
      "loss": 2.9179,
      "step": 27500
    },
    {
      "epoch": 6.939904450590897,
      "grad_norm": 3.143771171569824,
      "learning_rate": 4.967798742138365e-05,
      "loss": 2.9036,
      "step": 27600
    },
    {
      "epoch": 6.939904450590897,
      "eval_loss": 2.8437516689300537,
      "eval_runtime": 17.0017,
      "eval_samples_per_second": 787.803,
      "eval_steps_per_second": 12.352,
      "step": 27600
    },
    {
      "epoch": 6.965049031933618,
      "grad_norm": 2.9489290714263916,
      "learning_rate": 4.967672955974843e-05,
      "loss": 2.9152,
      "step": 27700
    },
    {
      "epoch": 6.990193613276339,
      "grad_norm": 3.053105592727661,
      "learning_rate": 4.967547169811321e-05,
      "loss": 2.9022,
      "step": 27800
    },
    {
      "epoch": 7.015338194619059,
      "grad_norm": 2.9473278522491455,
      "learning_rate": 4.967421383647799e-05,
      "loss": 2.896,
      "step": 27900
    },
    {
      "epoch": 7.04048277596178,
      "grad_norm": 3.115997076034546,
      "learning_rate": 4.967295597484277e-05,
      "loss": 2.8912,
      "step": 28000
    },
    {
      "epoch": 7.04048277596178,
      "eval_loss": 2.840319871902466,
      "eval_runtime": 17.278,
      "eval_samples_per_second": 775.207,
      "eval_steps_per_second": 12.154,
      "step": 28000
    },
    {
      "epoch": 7.065627357304501,
      "grad_norm": 3.028563976287842,
      "learning_rate": 4.967169811320755e-05,
      "loss": 2.8845,
      "step": 28100
    },
    {
      "epoch": 7.090771938647221,
      "grad_norm": 2.8841118812561035,
      "learning_rate": 4.967044025157233e-05,
      "loss": 2.8946,
      "step": 28200
    },
    {
      "epoch": 7.115916519989942,
      "grad_norm": 3.1687231063842773,
      "learning_rate": 4.9669182389937106e-05,
      "loss": 2.8883,
      "step": 28300
    },
    {
      "epoch": 7.141061101332663,
      "grad_norm": 3.303973436355591,
      "learning_rate": 4.966792452830189e-05,
      "loss": 2.8973,
      "step": 28400
    },
    {
      "epoch": 7.141061101332663,
      "eval_loss": 2.835576295852661,
      "eval_runtime": 17.1952,
      "eval_samples_per_second": 778.937,
      "eval_steps_per_second": 12.213,
      "step": 28400
    },
    {
      "epoch": 7.1662056826753835,
      "grad_norm": 3.080165147781372,
      "learning_rate": 4.966666666666667e-05,
      "loss": 2.8839,
      "step": 28500
    },
    {
      "epoch": 7.191350264018104,
      "grad_norm": 3.0599000453948975,
      "learning_rate": 4.966540880503145e-05,
      "loss": 2.8815,
      "step": 28600
    },
    {
      "epoch": 7.216494845360825,
      "grad_norm": 3.176584482192993,
      "learning_rate": 4.9664150943396234e-05,
      "loss": 2.8988,
      "step": 28700
    },
    {
      "epoch": 7.2416394267035455,
      "grad_norm": 3.0540246963500977,
      "learning_rate": 4.966289308176101e-05,
      "loss": 2.886,
      "step": 28800
    },
    {
      "epoch": 7.2416394267035455,
      "eval_loss": 2.833523988723755,
      "eval_runtime": 17.1442,
      "eval_samples_per_second": 781.255,
      "eval_steps_per_second": 12.249,
      "step": 28800
    },
    {
      "epoch": 7.266784008046266,
      "grad_norm": 2.959556818008423,
      "learning_rate": 4.966163522012579e-05,
      "loss": 2.8717,
      "step": 28900
    },
    {
      "epoch": 7.291928589388987,
      "grad_norm": 2.953927993774414,
      "learning_rate": 4.966037735849057e-05,
      "loss": 2.8834,
      "step": 29000
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 3.047366142272949,
      "learning_rate": 4.965911949685535e-05,
      "loss": 2.8785,
      "step": 29100
    },
    {
      "epoch": 7.342217752074428,
      "grad_norm": 2.9410929679870605,
      "learning_rate": 4.9657861635220125e-05,
      "loss": 2.8715,
      "step": 29200
    },
    {
      "epoch": 7.342217752074428,
      "eval_loss": 2.829909324645996,
      "eval_runtime": 17.1092,
      "eval_samples_per_second": 782.854,
      "eval_steps_per_second": 12.274,
      "step": 29200
    },
    {
      "epoch": 7.367362333417149,
      "grad_norm": 3.2384989261627197,
      "learning_rate": 4.965660377358491e-05,
      "loss": 2.8897,
      "step": 29300
    },
    {
      "epoch": 7.39250691475987,
      "grad_norm": 2.999455451965332,
      "learning_rate": 4.965534591194968e-05,
      "loss": 2.8781,
      "step": 29400
    },
    {
      "epoch": 7.4176514961025894,
      "grad_norm": 2.8725357055664062,
      "learning_rate": 4.9654088050314465e-05,
      "loss": 2.8937,
      "step": 29500
    },
    {
      "epoch": 7.44279607744531,
      "grad_norm": 3.09820556640625,
      "learning_rate": 4.965283018867925e-05,
      "loss": 2.8713,
      "step": 29600
    },
    {
      "epoch": 7.44279607744531,
      "eval_loss": 2.8278369903564453,
      "eval_runtime": 17.1874,
      "eval_samples_per_second": 779.294,
      "eval_steps_per_second": 12.218,
      "step": 29600
    },
    {
      "epoch": 7.467940658788031,
      "grad_norm": 3.192246437072754,
      "learning_rate": 4.965157232704403e-05,
      "loss": 2.8763,
      "step": 29700
    },
    {
      "epoch": 7.4930852401307515,
      "grad_norm": 3.1542928218841553,
      "learning_rate": 4.965031446540881e-05,
      "loss": 2.8788,
      "step": 29800
    },
    {
      "epoch": 7.518229821473472,
      "grad_norm": 3.022817611694336,
      "learning_rate": 4.964905660377359e-05,
      "loss": 2.8788,
      "step": 29900
    },
    {
      "epoch": 7.543374402816193,
      "grad_norm": 3.1112165451049805,
      "learning_rate": 4.964779874213837e-05,
      "loss": 2.8821,
      "step": 30000
    },
    {
      "epoch": 7.543374402816193,
      "eval_loss": 2.82590651512146,
      "eval_runtime": 17.1749,
      "eval_samples_per_second": 779.858,
      "eval_steps_per_second": 12.227,
      "step": 30000
    },
    {
      "epoch": 7.5685189841589136,
      "grad_norm": 3.0074880123138428,
      "learning_rate": 4.9646540880503145e-05,
      "loss": 2.8975,
      "step": 30100
    },
    {
      "epoch": 7.593663565501634,
      "grad_norm": 3.058912754058838,
      "learning_rate": 4.964528301886793e-05,
      "loss": 2.8855,
      "step": 30200
    },
    {
      "epoch": 7.618808146844355,
      "grad_norm": 3.0028185844421387,
      "learning_rate": 4.96440251572327e-05,
      "loss": 2.8761,
      "step": 30300
    },
    {
      "epoch": 7.643952728187076,
      "grad_norm": 2.9678475856781006,
      "learning_rate": 4.9642767295597485e-05,
      "loss": 2.8789,
      "step": 30400
    },
    {
      "epoch": 7.643952728187076,
      "eval_loss": 2.820873498916626,
      "eval_runtime": 17.0395,
      "eval_samples_per_second": 786.058,
      "eval_steps_per_second": 12.324,
      "step": 30400
    },
    {
      "epoch": 7.669097309529796,
      "grad_norm": 3.072939395904541,
      "learning_rate": 4.964150943396227e-05,
      "loss": 2.8831,
      "step": 30500
    },
    {
      "epoch": 7.694241890872517,
      "grad_norm": 2.99800443649292,
      "learning_rate": 4.964025157232704e-05,
      "loss": 2.8643,
      "step": 30600
    },
    {
      "epoch": 7.719386472215238,
      "grad_norm": 3.0248708724975586,
      "learning_rate": 4.9638993710691825e-05,
      "loss": 2.8753,
      "step": 30700
    },
    {
      "epoch": 7.744531053557958,
      "grad_norm": 2.9917218685150146,
      "learning_rate": 4.963773584905661e-05,
      "loss": 2.8832,
      "step": 30800
    },
    {
      "epoch": 7.744531053557958,
      "eval_loss": 2.819430112838745,
      "eval_runtime": 17.4766,
      "eval_samples_per_second": 766.397,
      "eval_steps_per_second": 12.016,
      "step": 30800
    },
    {
      "epoch": 7.769675634900679,
      "grad_norm": 3.0977261066436768,
      "learning_rate": 4.963647798742139e-05,
      "loss": 2.889,
      "step": 30900
    },
    {
      "epoch": 7.7948202162434,
      "grad_norm": 2.842686176300049,
      "learning_rate": 4.9635220125786165e-05,
      "loss": 2.8761,
      "step": 31000
    },
    {
      "epoch": 7.81996479758612,
      "grad_norm": 2.7892866134643555,
      "learning_rate": 4.963396226415095e-05,
      "loss": 2.8725,
      "step": 31100
    },
    {
      "epoch": 7.845109378928841,
      "grad_norm": 2.842395782470703,
      "learning_rate": 4.963270440251573e-05,
      "loss": 2.8805,
      "step": 31200
    },
    {
      "epoch": 7.845109378928841,
      "eval_loss": 2.817953586578369,
      "eval_runtime": 17.2264,
      "eval_samples_per_second": 777.526,
      "eval_steps_per_second": 12.191,
      "step": 31200
    },
    {
      "epoch": 7.870253960271562,
      "grad_norm": 2.9843485355377197,
      "learning_rate": 4.9631446540880505e-05,
      "loss": 2.8964,
      "step": 31300
    },
    {
      "epoch": 7.8953985416142825,
      "grad_norm": 2.9335358142852783,
      "learning_rate": 4.963018867924529e-05,
      "loss": 2.8806,
      "step": 31400
    },
    {
      "epoch": 7.920543122957003,
      "grad_norm": 3.137129545211792,
      "learning_rate": 4.962893081761006e-05,
      "loss": 2.8868,
      "step": 31500
    },
    {
      "epoch": 7.945687704299724,
      "grad_norm": 2.9728493690490723,
      "learning_rate": 4.9627672955974845e-05,
      "loss": 2.8566,
      "step": 31600
    },
    {
      "epoch": 7.945687704299724,
      "eval_loss": 2.815349578857422,
      "eval_runtime": 16.9976,
      "eval_samples_per_second": 787.996,
      "eval_steps_per_second": 12.355,
      "step": 31600
    },
    {
      "epoch": 7.9708322856424445,
      "grad_norm": 3.0205190181732178,
      "learning_rate": 4.962641509433962e-05,
      "loss": 2.8703,
      "step": 31700
    },
    {
      "epoch": 7.995976866985165,
      "grad_norm": 3.308504581451416,
      "learning_rate": 4.96251572327044e-05,
      "loss": 2.874,
      "step": 31800
    },
    {
      "epoch": 8.021121448327886,
      "grad_norm": 2.8498480319976807,
      "learning_rate": 4.9623899371069185e-05,
      "loss": 2.8524,
      "step": 31900
    },
    {
      "epoch": 8.046266029670607,
      "grad_norm": 3.0761075019836426,
      "learning_rate": 4.962264150943397e-05,
      "loss": 2.8576,
      "step": 32000
    },
    {
      "epoch": 8.046266029670607,
      "eval_loss": 2.810624837875366,
      "eval_runtime": 17.1343,
      "eval_samples_per_second": 781.708,
      "eval_steps_per_second": 12.256,
      "step": 32000
    },
    {
      "epoch": 8.071410611013327,
      "grad_norm": 3.0517661571502686,
      "learning_rate": 4.962138364779875e-05,
      "loss": 2.8473,
      "step": 32100
    },
    {
      "epoch": 8.096555192356048,
      "grad_norm": 2.894226551055908,
      "learning_rate": 4.9620125786163524e-05,
      "loss": 2.8448,
      "step": 32200
    },
    {
      "epoch": 8.121699773698769,
      "grad_norm": 3.04126238822937,
      "learning_rate": 4.961886792452831e-05,
      "loss": 2.8594,
      "step": 32300
    },
    {
      "epoch": 8.14684435504149,
      "grad_norm": 2.9169881343841553,
      "learning_rate": 4.961761006289308e-05,
      "loss": 2.8537,
      "step": 32400
    },
    {
      "epoch": 8.14684435504149,
      "eval_loss": 2.8087034225463867,
      "eval_runtime": 17.0186,
      "eval_samples_per_second": 787.019,
      "eval_steps_per_second": 12.339,
      "step": 32400
    },
    {
      "epoch": 8.17198893638421,
      "grad_norm": 3.1608850955963135,
      "learning_rate": 4.9616352201257864e-05,
      "loss": 2.8524,
      "step": 32500
    },
    {
      "epoch": 8.19713351772693,
      "grad_norm": 3.092944622039795,
      "learning_rate": 4.961509433962264e-05,
      "loss": 2.8518,
      "step": 32600
    },
    {
      "epoch": 8.222278099069651,
      "grad_norm": 2.9548041820526123,
      "learning_rate": 4.961383647798742e-05,
      "loss": 2.8576,
      "step": 32700
    },
    {
      "epoch": 8.24742268041237,
      "grad_norm": 3.2552549839019775,
      "learning_rate": 4.9612578616352204e-05,
      "loss": 2.847,
      "step": 32800
    },
    {
      "epoch": 8.24742268041237,
      "eval_loss": 2.8065853118896484,
      "eval_runtime": 17.2023,
      "eval_samples_per_second": 778.615,
      "eval_steps_per_second": 12.208,
      "step": 32800
    },
    {
      "epoch": 8.272567261755091,
      "grad_norm": 2.8612442016601562,
      "learning_rate": 4.9611320754716986e-05,
      "loss": 2.8466,
      "step": 32900
    },
    {
      "epoch": 8.297711843097812,
      "grad_norm": 2.963982582092285,
      "learning_rate": 4.961006289308177e-05,
      "loss": 2.862,
      "step": 33000
    },
    {
      "epoch": 8.322856424440532,
      "grad_norm": 2.9246978759765625,
      "learning_rate": 4.9608805031446544e-05,
      "loss": 2.8462,
      "step": 33100
    },
    {
      "epoch": 8.348001005783253,
      "grad_norm": 3.1916210651397705,
      "learning_rate": 4.9607547169811326e-05,
      "loss": 2.8504,
      "step": 33200
    },
    {
      "epoch": 8.348001005783253,
      "eval_loss": 2.8027865886688232,
      "eval_runtime": 17.0448,
      "eval_samples_per_second": 785.81,
      "eval_steps_per_second": 12.32,
      "step": 33200
    },
    {
      "epoch": 8.373145587125974,
      "grad_norm": 3.1703543663024902,
      "learning_rate": 4.96062893081761e-05,
      "loss": 2.8559,
      "step": 33300
    },
    {
      "epoch": 8.398290168468694,
      "grad_norm": 2.978623390197754,
      "learning_rate": 4.9605031446540884e-05,
      "loss": 2.844,
      "step": 33400
    },
    {
      "epoch": 8.423434749811415,
      "grad_norm": 3.0499696731567383,
      "learning_rate": 4.960377358490566e-05,
      "loss": 2.8545,
      "step": 33500
    },
    {
      "epoch": 8.448579331154136,
      "grad_norm": 3.0443637371063232,
      "learning_rate": 4.960251572327044e-05,
      "loss": 2.8439,
      "step": 33600
    },
    {
      "epoch": 8.448579331154136,
      "eval_loss": 2.7995760440826416,
      "eval_runtime": 17.372,
      "eval_samples_per_second": 771.01,
      "eval_steps_per_second": 12.088,
      "step": 33600
    },
    {
      "epoch": 8.473723912496856,
      "grad_norm": 2.9804091453552246,
      "learning_rate": 4.9601257861635224e-05,
      "loss": 2.855,
      "step": 33700
    },
    {
      "epoch": 8.498868493839577,
      "grad_norm": 3.1608359813690186,
      "learning_rate": 4.96e-05,
      "loss": 2.8602,
      "step": 33800
    },
    {
      "epoch": 8.524013075182298,
      "grad_norm": 3.0731091499328613,
      "learning_rate": 4.959874213836478e-05,
      "loss": 2.8422,
      "step": 33900
    },
    {
      "epoch": 8.549157656525018,
      "grad_norm": 2.846478223800659,
      "learning_rate": 4.9597484276729564e-05,
      "loss": 2.8479,
      "step": 34000
    },
    {
      "epoch": 8.549157656525018,
      "eval_loss": 2.797999382019043,
      "eval_runtime": 17.0577,
      "eval_samples_per_second": 785.216,
      "eval_steps_per_second": 12.311,
      "step": 34000
    },
    {
      "epoch": 8.57430223786774,
      "grad_norm": 3.0144379138946533,
      "learning_rate": 4.9596226415094346e-05,
      "loss": 2.8509,
      "step": 34100
    },
    {
      "epoch": 8.59944681921046,
      "grad_norm": 3.1947710514068604,
      "learning_rate": 4.959496855345912e-05,
      "loss": 2.8444,
      "step": 34200
    },
    {
      "epoch": 8.62459140055318,
      "grad_norm": 3.066152572631836,
      "learning_rate": 4.9593710691823904e-05,
      "loss": 2.8338,
      "step": 34300
    },
    {
      "epoch": 8.649735981895901,
      "grad_norm": 2.9473867416381836,
      "learning_rate": 4.959245283018868e-05,
      "loss": 2.8585,
      "step": 34400
    },
    {
      "epoch": 8.649735981895901,
      "eval_loss": 2.795994997024536,
      "eval_runtime": 17.2022,
      "eval_samples_per_second": 778.623,
      "eval_steps_per_second": 12.208,
      "step": 34400
    },
    {
      "epoch": 8.674880563238622,
      "grad_norm": 3.3551015853881836,
      "learning_rate": 4.959119496855346e-05,
      "loss": 2.8473,
      "step": 34500
    },
    {
      "epoch": 8.700025144581343,
      "grad_norm": 2.811887264251709,
      "learning_rate": 4.9589937106918244e-05,
      "loss": 2.8429,
      "step": 34600
    },
    {
      "epoch": 8.725169725924063,
      "grad_norm": 3.001351833343506,
      "learning_rate": 4.958867924528302e-05,
      "loss": 2.8567,
      "step": 34700
    },
    {
      "epoch": 8.750314307266784,
      "grad_norm": 3.054389238357544,
      "learning_rate": 4.95874213836478e-05,
      "loss": 2.856,
      "step": 34800
    },
    {
      "epoch": 8.750314307266784,
      "eval_loss": 2.7924821376800537,
      "eval_runtime": 17.0312,
      "eval_samples_per_second": 786.44,
      "eval_steps_per_second": 12.33,
      "step": 34800
    },
    {
      "epoch": 8.775458888609505,
      "grad_norm": 2.946521043777466,
      "learning_rate": 4.958616352201258e-05,
      "loss": 2.8516,
      "step": 34900
    },
    {
      "epoch": 8.800603469952225,
      "grad_norm": 3.1947922706604004,
      "learning_rate": 4.958490566037736e-05,
      "loss": 2.8347,
      "step": 35000
    },
    {
      "epoch": 8.825748051294946,
      "grad_norm": 3.0896878242492676,
      "learning_rate": 4.958364779874214e-05,
      "loss": 2.8487,
      "step": 35100
    },
    {
      "epoch": 8.850892632637667,
      "grad_norm": 3.12526798248291,
      "learning_rate": 4.9582389937106924e-05,
      "loss": 2.8386,
      "step": 35200
    },
    {
      "epoch": 8.850892632637667,
      "eval_loss": 2.791961669921875,
      "eval_runtime": 17.371,
      "eval_samples_per_second": 771.056,
      "eval_steps_per_second": 12.089,
      "step": 35200
    },
    {
      "epoch": 8.876037213980387,
      "grad_norm": 3.0814011096954346,
      "learning_rate": 4.95811320754717e-05,
      "loss": 2.8586,
      "step": 35300
    },
    {
      "epoch": 8.901181795323108,
      "grad_norm": 2.7319672107696533,
      "learning_rate": 4.957987421383648e-05,
      "loss": 2.8447,
      "step": 35400
    },
    {
      "epoch": 8.926326376665829,
      "grad_norm": 2.901862382888794,
      "learning_rate": 4.9578616352201263e-05,
      "loss": 2.8394,
      "step": 35500
    },
    {
      "epoch": 8.95147095800855,
      "grad_norm": 2.768339157104492,
      "learning_rate": 4.957735849056604e-05,
      "loss": 2.8459,
      "step": 35600
    },
    {
      "epoch": 8.95147095800855,
      "eval_loss": 2.786900758743286,
      "eval_runtime": 17.0044,
      "eval_samples_per_second": 787.68,
      "eval_steps_per_second": 12.35,
      "step": 35600
    },
    {
      "epoch": 8.97661553935127,
      "grad_norm": 2.8236868381500244,
      "learning_rate": 4.957610062893082e-05,
      "loss": 2.8512,
      "step": 35700
    },
    {
      "epoch": 9.00176012069399,
      "grad_norm": 2.8416008949279785,
      "learning_rate": 4.95748427672956e-05,
      "loss": 2.832,
      "step": 35800
    },
    {
      "epoch": 9.026904702036711,
      "grad_norm": 3.028381586074829,
      "learning_rate": 4.957358490566038e-05,
      "loss": 2.8215,
      "step": 35900
    },
    {
      "epoch": 9.052049283379432,
      "grad_norm": 2.7869863510131836,
      "learning_rate": 4.9572327044025154e-05,
      "loss": 2.8298,
      "step": 36000
    },
    {
      "epoch": 9.052049283379432,
      "eval_loss": 2.7853987216949463,
      "eval_runtime": 17.1821,
      "eval_samples_per_second": 779.531,
      "eval_steps_per_second": 12.222,
      "step": 36000
    },
    {
      "epoch": 9.077193864722153,
      "grad_norm": 3.01544189453125,
      "learning_rate": 4.9571069182389937e-05,
      "loss": 2.8291,
      "step": 36100
    },
    {
      "epoch": 9.102338446064874,
      "grad_norm": 2.865246295928955,
      "learning_rate": 4.956981132075472e-05,
      "loss": 2.822,
      "step": 36200
    },
    {
      "epoch": 9.127483027407594,
      "grad_norm": 2.890326499938965,
      "learning_rate": 4.95685534591195e-05,
      "loss": 2.8207,
      "step": 36300
    },
    {
      "epoch": 9.152627608750315,
      "grad_norm": 2.7060775756835938,
      "learning_rate": 4.956729559748428e-05,
      "loss": 2.8187,
      "step": 36400
    },
    {
      "epoch": 9.152627608750315,
      "eval_loss": 2.7868173122406006,
      "eval_runtime": 16.9408,
      "eval_samples_per_second": 790.637,
      "eval_steps_per_second": 12.396,
      "step": 36400
    },
    {
      "epoch": 9.177772190093036,
      "grad_norm": 2.9194414615631104,
      "learning_rate": 4.956603773584906e-05,
      "loss": 2.8147,
      "step": 36500
    },
    {
      "epoch": 9.202916771435756,
      "grad_norm": 2.912013292312622,
      "learning_rate": 4.956477987421384e-05,
      "loss": 2.8169,
      "step": 36600
    },
    {
      "epoch": 9.228061352778477,
      "grad_norm": 2.9252138137817383,
      "learning_rate": 4.9563522012578616e-05,
      "loss": 2.8199,
      "step": 36700
    },
    {
      "epoch": 9.253205934121198,
      "grad_norm": 2.9396538734436035,
      "learning_rate": 4.95622641509434e-05,
      "loss": 2.8272,
      "step": 36800
    },
    {
      "epoch": 9.253205934121198,
      "eval_loss": 2.782938003540039,
      "eval_runtime": 17.2583,
      "eval_samples_per_second": 776.092,
      "eval_steps_per_second": 12.168,
      "step": 36800
    },
    {
      "epoch": 9.278350515463918,
      "grad_norm": 2.721040725708008,
      "learning_rate": 4.9561006289308174e-05,
      "loss": 2.8205,
      "step": 36900
    },
    {
      "epoch": 9.303495096806639,
      "grad_norm": 3.2089147567749023,
      "learning_rate": 4.9559748427672956e-05,
      "loss": 2.8276,
      "step": 37000
    },
    {
      "epoch": 9.328639678149358,
      "grad_norm": 2.9807019233703613,
      "learning_rate": 4.955849056603774e-05,
      "loss": 2.8229,
      "step": 37100
    },
    {
      "epoch": 9.35378425949208,
      "grad_norm": 2.9529874324798584,
      "learning_rate": 4.955723270440252e-05,
      "loss": 2.823,
      "step": 37200
    },
    {
      "epoch": 9.35378425949208,
      "eval_loss": 2.781411647796631,
      "eval_runtime": 17.5451,
      "eval_samples_per_second": 763.403,
      "eval_steps_per_second": 11.969,
      "step": 37200
    },
    {
      "epoch": 9.3789288408348,
      "grad_norm": 3.042948007583618,
      "learning_rate": 4.95559748427673e-05,
      "loss": 2.8217,
      "step": 37300
    },
    {
      "epoch": 9.40407342217752,
      "grad_norm": 3.0346834659576416,
      "learning_rate": 4.955471698113208e-05,
      "loss": 2.8258,
      "step": 37400
    },
    {
      "epoch": 9.42921800352024,
      "grad_norm": 3.039545774459839,
      "learning_rate": 4.955345911949686e-05,
      "loss": 2.8154,
      "step": 37500
    },
    {
      "epoch": 9.454362584862961,
      "grad_norm": 2.8537302017211914,
      "learning_rate": 4.9552201257861636e-05,
      "loss": 2.8085,
      "step": 37600
    },
    {
      "epoch": 9.454362584862961,
      "eval_loss": 2.7789196968078613,
      "eval_runtime": 17.301,
      "eval_samples_per_second": 774.175,
      "eval_steps_per_second": 12.138,
      "step": 37600
    },
    {
      "epoch": 9.479507166205682,
      "grad_norm": 2.827547788619995,
      "learning_rate": 4.955094339622642e-05,
      "loss": 2.8075,
      "step": 37700
    },
    {
      "epoch": 9.504651747548403,
      "grad_norm": 2.7986695766448975,
      "learning_rate": 4.9549685534591194e-05,
      "loss": 2.8126,
      "step": 37800
    },
    {
      "epoch": 9.529796328891123,
      "grad_norm": 2.8820831775665283,
      "learning_rate": 4.9548427672955976e-05,
      "loss": 2.8204,
      "step": 37900
    },
    {
      "epoch": 9.554940910233844,
      "grad_norm": 2.8811087608337402,
      "learning_rate": 4.954716981132076e-05,
      "loss": 2.8309,
      "step": 38000
    },
    {
      "epoch": 9.554940910233844,
      "eval_loss": 2.773318290710449,
      "eval_runtime": 17.0545,
      "eval_samples_per_second": 785.365,
      "eval_steps_per_second": 12.313,
      "step": 38000
    },
    {
      "epoch": 9.580085491576565,
      "grad_norm": 3.097111940383911,
      "learning_rate": 4.9545911949685534e-05,
      "loss": 2.821,
      "step": 38100
    },
    {
      "epoch": 9.605230072919285,
      "grad_norm": 2.951068878173828,
      "learning_rate": 4.9544654088050316e-05,
      "loss": 2.8338,
      "step": 38200
    },
    {
      "epoch": 9.630374654262006,
      "grad_norm": 2.956064224243164,
      "learning_rate": 4.95433962264151e-05,
      "loss": 2.8294,
      "step": 38300
    },
    {
      "epoch": 9.655519235604727,
      "grad_norm": 2.9667887687683105,
      "learning_rate": 4.954213836477988e-05,
      "loss": 2.8091,
      "step": 38400
    },
    {
      "epoch": 9.655519235604727,
      "eval_loss": 2.7739932537078857,
      "eval_runtime": 17.0355,
      "eval_samples_per_second": 786.241,
      "eval_steps_per_second": 12.327,
      "step": 38400
    },
    {
      "epoch": 9.680663816947447,
      "grad_norm": 2.802713394165039,
      "learning_rate": 4.9540880503144656e-05,
      "loss": 2.8201,
      "step": 38500
    },
    {
      "epoch": 9.705808398290168,
      "grad_norm": 2.839024066925049,
      "learning_rate": 4.953962264150944e-05,
      "loss": 2.8318,
      "step": 38600
    },
    {
      "epoch": 9.730952979632889,
      "grad_norm": 2.9779751300811768,
      "learning_rate": 4.953836477987422e-05,
      "loss": 2.8146,
      "step": 38700
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 3.0560450553894043,
      "learning_rate": 4.9537106918238996e-05,
      "loss": 2.8201,
      "step": 38800
    },
    {
      "epoch": 9.75609756097561,
      "eval_loss": 2.77164363861084,
      "eval_runtime": 17.1903,
      "eval_samples_per_second": 779.158,
      "eval_steps_per_second": 12.216,
      "step": 38800
    },
    {
      "epoch": 9.78124214231833,
      "grad_norm": 3.047854423522949,
      "learning_rate": 4.953584905660378e-05,
      "loss": 2.8123,
      "step": 38900
    },
    {
      "epoch": 9.806386723661051,
      "grad_norm": 3.000631093978882,
      "learning_rate": 4.9534591194968553e-05,
      "loss": 2.8175,
      "step": 39000
    },
    {
      "epoch": 9.831531305003772,
      "grad_norm": 3.2110097408294678,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 2.8218,
      "step": 39100
    },
    {
      "epoch": 9.856675886346492,
      "grad_norm": 2.8847711086273193,
      "learning_rate": 4.953207547169811e-05,
      "loss": 2.8291,
      "step": 39200
    },
    {
      "epoch": 9.856675886346492,
      "eval_loss": 2.773228168487549,
      "eval_runtime": 16.9651,
      "eval_samples_per_second": 789.503,
      "eval_steps_per_second": 12.378,
      "step": 39200
    },
    {
      "epoch": 9.881820467689213,
      "grad_norm": 2.8650705814361572,
      "learning_rate": 4.953081761006289e-05,
      "loss": 2.833,
      "step": 39300
    },
    {
      "epoch": 9.906965049031934,
      "grad_norm": 2.8581597805023193,
      "learning_rate": 4.9529559748427676e-05,
      "loss": 2.8266,
      "step": 39400
    },
    {
      "epoch": 9.932109630374654,
      "grad_norm": 2.828064441680908,
      "learning_rate": 4.952830188679246e-05,
      "loss": 2.8241,
      "step": 39500
    },
    {
      "epoch": 9.957254211717375,
      "grad_norm": 2.89209246635437,
      "learning_rate": 4.952704402515724e-05,
      "loss": 2.8073,
      "step": 39600
    },
    {
      "epoch": 9.957254211717375,
      "eval_loss": 2.7682547569274902,
      "eval_runtime": 17.2876,
      "eval_samples_per_second": 774.776,
      "eval_steps_per_second": 12.147,
      "step": 39600
    },
    {
      "epoch": 9.982398793060096,
      "grad_norm": 2.806072950363159,
      "learning_rate": 4.9525786163522016e-05,
      "loss": 2.8209,
      "step": 39700
    },
    {
      "epoch": 10.007543374402816,
      "grad_norm": 2.898984909057617,
      "learning_rate": 4.95245283018868e-05,
      "loss": 2.8059,
      "step": 39800
    },
    {
      "epoch": 10.032687955745537,
      "grad_norm": 3.074563980102539,
      "learning_rate": 4.952327044025157e-05,
      "loss": 2.7883,
      "step": 39900
    },
    {
      "epoch": 10.057832537088258,
      "grad_norm": 3.0169641971588135,
      "learning_rate": 4.9522012578616355e-05,
      "loss": 2.792,
      "step": 40000
    },
    {
      "epoch": 10.057832537088258,
      "eval_loss": 2.766726016998291,
      "eval_runtime": 17.0942,
      "eval_samples_per_second": 783.543,
      "eval_steps_per_second": 12.285,
      "step": 40000
    },
    {
      "epoch": 10.082977118430978,
      "grad_norm": 2.9914329051971436,
      "learning_rate": 4.952075471698113e-05,
      "loss": 2.801,
      "step": 40100
    },
    {
      "epoch": 10.1081216997737,
      "grad_norm": 3.087526321411133,
      "learning_rate": 4.951949685534591e-05,
      "loss": 2.8004,
      "step": 40200
    },
    {
      "epoch": 10.13326628111642,
      "grad_norm": 2.9517767429351807,
      "learning_rate": 4.951823899371069e-05,
      "loss": 2.802,
      "step": 40300
    },
    {
      "epoch": 10.15841086245914,
      "grad_norm": 2.83296799659729,
      "learning_rate": 4.951698113207547e-05,
      "loss": 2.7954,
      "step": 40400
    },
    {
      "epoch": 10.15841086245914,
      "eval_loss": 2.764228105545044,
      "eval_runtime": 17.5816,
      "eval_samples_per_second": 761.818,
      "eval_steps_per_second": 11.944,
      "step": 40400
    },
    {
      "epoch": 10.183555443801861,
      "grad_norm": 2.9615554809570312,
      "learning_rate": 4.951572327044025e-05,
      "loss": 2.7835,
      "step": 40500
    },
    {
      "epoch": 10.208700025144582,
      "grad_norm": 2.9081509113311768,
      "learning_rate": 4.9514465408805035e-05,
      "loss": 2.7848,
      "step": 40600
    },
    {
      "epoch": 10.233844606487303,
      "grad_norm": 2.8470990657806396,
      "learning_rate": 4.951320754716982e-05,
      "loss": 2.8002,
      "step": 40700
    },
    {
      "epoch": 10.258989187830023,
      "grad_norm": 3.0208287239074707,
      "learning_rate": 4.951194968553459e-05,
      "loss": 2.7946,
      "step": 40800
    },
    {
      "epoch": 10.258989187830023,
      "eval_loss": 2.759641408920288,
      "eval_runtime": 17.0098,
      "eval_samples_per_second": 787.429,
      "eval_steps_per_second": 12.346,
      "step": 40800
    },
    {
      "epoch": 10.284133769172744,
      "grad_norm": 3.1019489765167236,
      "learning_rate": 4.9510691823899375e-05,
      "loss": 2.7921,
      "step": 40900
    },
    {
      "epoch": 10.309278350515465,
      "grad_norm": 2.951878786087036,
      "learning_rate": 4.950943396226415e-05,
      "loss": 2.8026,
      "step": 41000
    },
    {
      "epoch": 10.334422931858185,
      "grad_norm": 2.955610990524292,
      "learning_rate": 4.950817610062893e-05,
      "loss": 2.8097,
      "step": 41100
    },
    {
      "epoch": 10.359567513200906,
      "grad_norm": 2.8643946647644043,
      "learning_rate": 4.9506918238993715e-05,
      "loss": 2.8003,
      "step": 41200
    },
    {
      "epoch": 10.359567513200906,
      "eval_loss": 2.7620794773101807,
      "eval_runtime": 17.5387,
      "eval_samples_per_second": 763.68,
      "eval_steps_per_second": 11.973,
      "step": 41200
    },
    {
      "epoch": 10.384712094543627,
      "grad_norm": 3.0461666584014893,
      "learning_rate": 4.950566037735849e-05,
      "loss": 2.7886,
      "step": 41300
    },
    {
      "epoch": 10.409856675886347,
      "grad_norm": 2.977621555328369,
      "learning_rate": 4.950440251572327e-05,
      "loss": 2.792,
      "step": 41400
    },
    {
      "epoch": 10.435001257229068,
      "grad_norm": 3.1722164154052734,
      "learning_rate": 4.9503144654088055e-05,
      "loss": 2.8062,
      "step": 41500
    },
    {
      "epoch": 10.460145838571787,
      "grad_norm": 2.8934199810028076,
      "learning_rate": 4.950188679245284e-05,
      "loss": 2.7909,
      "step": 41600
    },
    {
      "epoch": 10.460145838571787,
      "eval_loss": 2.7580008506774902,
      "eval_runtime": 17.4843,
      "eval_samples_per_second": 766.059,
      "eval_steps_per_second": 12.011,
      "step": 41600
    },
    {
      "epoch": 10.485290419914508,
      "grad_norm": 2.710293769836426,
      "learning_rate": 4.950062893081761e-05,
      "loss": 2.7983,
      "step": 41700
    },
    {
      "epoch": 10.510435001257228,
      "grad_norm": 3.001633882522583,
      "learning_rate": 4.9499371069182395e-05,
      "loss": 2.7892,
      "step": 41800
    },
    {
      "epoch": 10.535579582599949,
      "grad_norm": 3.162814140319824,
      "learning_rate": 4.949811320754717e-05,
      "loss": 2.8102,
      "step": 41900
    },
    {
      "epoch": 10.56072416394267,
      "grad_norm": 2.8374133110046387,
      "learning_rate": 4.949685534591195e-05,
      "loss": 2.7963,
      "step": 42000
    },
    {
      "epoch": 10.56072416394267,
      "eval_loss": 2.7571768760681152,
      "eval_runtime": 17.1806,
      "eval_samples_per_second": 779.6,
      "eval_steps_per_second": 12.223,
      "step": 42000
    },
    {
      "epoch": 10.58586874528539,
      "grad_norm": 3.3931093215942383,
      "learning_rate": 4.9495597484276735e-05,
      "loss": 2.8056,
      "step": 42100
    },
    {
      "epoch": 10.611013326628111,
      "grad_norm": 2.824655771255493,
      "learning_rate": 4.949433962264151e-05,
      "loss": 2.7981,
      "step": 42200
    },
    {
      "epoch": 10.636157907970832,
      "grad_norm": 2.8755605220794678,
      "learning_rate": 4.949308176100629e-05,
      "loss": 2.8022,
      "step": 42300
    },
    {
      "epoch": 10.661302489313552,
      "grad_norm": 2.7887346744537354,
      "learning_rate": 4.949182389937107e-05,
      "loss": 2.7926,
      "step": 42400
    },
    {
      "epoch": 10.661302489313552,
      "eval_loss": 2.7570831775665283,
      "eval_runtime": 17.3509,
      "eval_samples_per_second": 771.949,
      "eval_steps_per_second": 12.103,
      "step": 42400
    },
    {
      "epoch": 10.686447070656273,
      "grad_norm": 2.868258237838745,
      "learning_rate": 4.949056603773585e-05,
      "loss": 2.7939,
      "step": 42500
    },
    {
      "epoch": 10.711591651998994,
      "grad_norm": 2.820812940597534,
      "learning_rate": 4.948930817610063e-05,
      "loss": 2.7992,
      "step": 42600
    },
    {
      "epoch": 10.736736233341714,
      "grad_norm": 2.8593831062316895,
      "learning_rate": 4.9488050314465415e-05,
      "loss": 2.8027,
      "step": 42700
    },
    {
      "epoch": 10.761880814684435,
      "grad_norm": 2.8948824405670166,
      "learning_rate": 4.948679245283019e-05,
      "loss": 2.8005,
      "step": 42800
    },
    {
      "epoch": 10.761880814684435,
      "eval_loss": 2.751159429550171,
      "eval_runtime": 17.2371,
      "eval_samples_per_second": 777.043,
      "eval_steps_per_second": 12.183,
      "step": 42800
    },
    {
      "epoch": 10.787025396027156,
      "grad_norm": 2.856459617614746,
      "learning_rate": 4.948553459119497e-05,
      "loss": 2.7936,
      "step": 42900
    },
    {
      "epoch": 10.812169977369877,
      "grad_norm": 3.010122060775757,
      "learning_rate": 4.9484276729559755e-05,
      "loss": 2.7921,
      "step": 43000
    },
    {
      "epoch": 10.837314558712597,
      "grad_norm": 2.857264757156372,
      "learning_rate": 4.948301886792453e-05,
      "loss": 2.7852,
      "step": 43100
    },
    {
      "epoch": 10.862459140055318,
      "grad_norm": 2.720790147781372,
      "learning_rate": 4.948176100628931e-05,
      "loss": 2.7996,
      "step": 43200
    },
    {
      "epoch": 10.862459140055318,
      "eval_loss": 2.751176118850708,
      "eval_runtime": 17.1974,
      "eval_samples_per_second": 778.839,
      "eval_steps_per_second": 12.211,
      "step": 43200
    },
    {
      "epoch": 10.887603721398039,
      "grad_norm": 2.8816587924957275,
      "learning_rate": 4.948050314465409e-05,
      "loss": 2.8033,
      "step": 43300
    },
    {
      "epoch": 10.91274830274076,
      "grad_norm": 2.887911081314087,
      "learning_rate": 4.947924528301887e-05,
      "loss": 2.8038,
      "step": 43400
    },
    {
      "epoch": 10.93789288408348,
      "grad_norm": 2.843343734741211,
      "learning_rate": 4.9477987421383645e-05,
      "loss": 2.8004,
      "step": 43500
    },
    {
      "epoch": 10.9630374654262,
      "grad_norm": 2.8166861534118652,
      "learning_rate": 4.947672955974843e-05,
      "loss": 2.7886,
      "step": 43600
    },
    {
      "epoch": 10.9630374654262,
      "eval_loss": 2.751467704772949,
      "eval_runtime": 17.1738,
      "eval_samples_per_second": 779.907,
      "eval_steps_per_second": 12.228,
      "step": 43600
    },
    {
      "epoch": 10.988182046768921,
      "grad_norm": 2.9054858684539795,
      "learning_rate": 4.947547169811321e-05,
      "loss": 2.797,
      "step": 43700
    },
    {
      "epoch": 11.013326628111642,
      "grad_norm": 2.7728981971740723,
      "learning_rate": 4.947421383647799e-05,
      "loss": 2.7888,
      "step": 43800
    },
    {
      "epoch": 11.038471209454363,
      "grad_norm": 3.0315983295440674,
      "learning_rate": 4.9472955974842774e-05,
      "loss": 2.7705,
      "step": 43900
    },
    {
      "epoch": 11.063615790797083,
      "grad_norm": 2.847517490386963,
      "learning_rate": 4.947169811320755e-05,
      "loss": 2.7816,
      "step": 44000
    },
    {
      "epoch": 11.063615790797083,
      "eval_loss": 2.748887300491333,
      "eval_runtime": 17.0492,
      "eval_samples_per_second": 785.61,
      "eval_steps_per_second": 12.317,
      "step": 44000
    },
    {
      "epoch": 11.088760372139804,
      "grad_norm": 2.9055557250976562,
      "learning_rate": 4.947044025157233e-05,
      "loss": 2.7677,
      "step": 44100
    },
    {
      "epoch": 11.113904953482525,
      "grad_norm": 2.8997650146484375,
      "learning_rate": 4.946918238993711e-05,
      "loss": 2.769,
      "step": 44200
    },
    {
      "epoch": 11.139049534825245,
      "grad_norm": 2.8719425201416016,
      "learning_rate": 4.946792452830189e-05,
      "loss": 2.7778,
      "step": 44300
    },
    {
      "epoch": 11.164194116167966,
      "grad_norm": 3.043776273727417,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 2.7728,
      "step": 44400
    },
    {
      "epoch": 11.164194116167966,
      "eval_loss": 2.7480058670043945,
      "eval_runtime": 17.2545,
      "eval_samples_per_second": 776.26,
      "eval_steps_per_second": 12.171,
      "step": 44400
    },
    {
      "epoch": 11.189338697510687,
      "grad_norm": 2.9432408809661865,
      "learning_rate": 4.946540880503145e-05,
      "loss": 2.7817,
      "step": 44500
    },
    {
      "epoch": 11.214483278853407,
      "grad_norm": 3.0081658363342285,
      "learning_rate": 4.946415094339623e-05,
      "loss": 2.7674,
      "step": 44600
    },
    {
      "epoch": 11.239627860196128,
      "grad_norm": 3.260760545730591,
      "learning_rate": 4.9462893081761005e-05,
      "loss": 2.7709,
      "step": 44700
    },
    {
      "epoch": 11.264772441538849,
      "grad_norm": 2.9482510089874268,
      "learning_rate": 4.946163522012579e-05,
      "loss": 2.7675,
      "step": 44800
    },
    {
      "epoch": 11.264772441538849,
      "eval_loss": 2.7471628189086914,
      "eval_runtime": 17.2821,
      "eval_samples_per_second": 775.023,
      "eval_steps_per_second": 12.151,
      "step": 44800
    },
    {
      "epoch": 11.28991702288157,
      "grad_norm": 2.7287964820861816,
      "learning_rate": 4.946037735849057e-05,
      "loss": 2.7598,
      "step": 44900
    },
    {
      "epoch": 11.31506160422429,
      "grad_norm": 3.168217897415161,
      "learning_rate": 4.945911949685535e-05,
      "loss": 2.7838,
      "step": 45000
    },
    {
      "epoch": 11.34020618556701,
      "grad_norm": 2.882962465286255,
      "learning_rate": 4.945786163522013e-05,
      "loss": 2.7666,
      "step": 45100
    },
    {
      "epoch": 11.365350766909732,
      "grad_norm": 2.8319005966186523,
      "learning_rate": 4.945660377358491e-05,
      "loss": 2.7716,
      "step": 45200
    },
    {
      "epoch": 11.365350766909732,
      "eval_loss": 2.743784189224243,
      "eval_runtime": 17.3833,
      "eval_samples_per_second": 770.51,
      "eval_steps_per_second": 12.081,
      "step": 45200
    },
    {
      "epoch": 11.390495348252452,
      "grad_norm": 2.868969202041626,
      "learning_rate": 4.9455345911949685e-05,
      "loss": 2.7756,
      "step": 45300
    },
    {
      "epoch": 11.415639929595173,
      "grad_norm": 2.8156492710113525,
      "learning_rate": 4.945408805031447e-05,
      "loss": 2.7777,
      "step": 45400
    },
    {
      "epoch": 11.440784510937894,
      "grad_norm": 2.810117244720459,
      "learning_rate": 4.945283018867925e-05,
      "loss": 2.7906,
      "step": 45500
    },
    {
      "epoch": 11.465929092280614,
      "grad_norm": 2.8641767501831055,
      "learning_rate": 4.9451572327044025e-05,
      "loss": 2.7834,
      "step": 45600
    },
    {
      "epoch": 11.465929092280614,
      "eval_loss": 2.742170572280884,
      "eval_runtime": 17.3597,
      "eval_samples_per_second": 771.558,
      "eval_steps_per_second": 12.097,
      "step": 45600
    },
    {
      "epoch": 11.491073673623335,
      "grad_norm": 2.8514885902404785,
      "learning_rate": 4.945031446540881e-05,
      "loss": 2.7806,
      "step": 45700
    },
    {
      "epoch": 11.516218254966056,
      "grad_norm": 2.7780611515045166,
      "learning_rate": 4.944905660377358e-05,
      "loss": 2.7777,
      "step": 45800
    },
    {
      "epoch": 11.541362836308775,
      "grad_norm": 2.805931568145752,
      "learning_rate": 4.944779874213837e-05,
      "loss": 2.7837,
      "step": 45900
    },
    {
      "epoch": 11.566507417651497,
      "grad_norm": 2.9050819873809814,
      "learning_rate": 4.944654088050315e-05,
      "loss": 2.7797,
      "step": 46000
    },
    {
      "epoch": 11.566507417651497,
      "eval_loss": 2.740935802459717,
      "eval_runtime": 17.393,
      "eval_samples_per_second": 770.078,
      "eval_steps_per_second": 12.074,
      "step": 46000
    },
    {
      "epoch": 11.591651998994216,
      "grad_norm": 2.827784776687622,
      "learning_rate": 4.944528301886793e-05,
      "loss": 2.7713,
      "step": 46100
    },
    {
      "epoch": 11.616796580336937,
      "grad_norm": 2.6949777603149414,
      "learning_rate": 4.9444025157232705e-05,
      "loss": 2.7675,
      "step": 46200
    },
    {
      "epoch": 11.641941161679657,
      "grad_norm": 3.166933059692383,
      "learning_rate": 4.944276729559749e-05,
      "loss": 2.7771,
      "step": 46300
    },
    {
      "epoch": 11.667085743022378,
      "grad_norm": 2.7832415103912354,
      "learning_rate": 4.944150943396227e-05,
      "loss": 2.7639,
      "step": 46400
    },
    {
      "epoch": 11.667085743022378,
      "eval_loss": 2.7415754795074463,
      "eval_runtime": 17.4749,
      "eval_samples_per_second": 766.472,
      "eval_steps_per_second": 12.017,
      "step": 46400
    },
    {
      "epoch": 11.692230324365099,
      "grad_norm": 2.7607593536376953,
      "learning_rate": 4.9440251572327045e-05,
      "loss": 2.7766,
      "step": 46500
    },
    {
      "epoch": 11.71737490570782,
      "grad_norm": 2.952683210372925,
      "learning_rate": 4.943899371069183e-05,
      "loss": 2.7814,
      "step": 46600
    },
    {
      "epoch": 11.74251948705054,
      "grad_norm": 3.2382521629333496,
      "learning_rate": 4.94377358490566e-05,
      "loss": 2.7666,
      "step": 46700
    },
    {
      "epoch": 11.76766406839326,
      "grad_norm": 2.8143601417541504,
      "learning_rate": 4.9436477987421384e-05,
      "loss": 2.774,
      "step": 46800
    },
    {
      "epoch": 11.76766406839326,
      "eval_loss": 2.737732410430908,
      "eval_runtime": 17.3997,
      "eval_samples_per_second": 769.782,
      "eval_steps_per_second": 12.069,
      "step": 46800
    },
    {
      "epoch": 11.792808649735981,
      "grad_norm": 2.9001598358154297,
      "learning_rate": 4.943522012578617e-05,
      "loss": 2.7759,
      "step": 46900
    },
    {
      "epoch": 11.817953231078702,
      "grad_norm": 2.7825865745544434,
      "learning_rate": 4.943396226415095e-05,
      "loss": 2.7827,
      "step": 47000
    },
    {
      "epoch": 11.843097812421423,
      "grad_norm": 2.7635583877563477,
      "learning_rate": 4.943270440251573e-05,
      "loss": 2.7741,
      "step": 47100
    },
    {
      "epoch": 11.868242393764143,
      "grad_norm": 2.8352227210998535,
      "learning_rate": 4.9431446540880507e-05,
      "loss": 2.7679,
      "step": 47200
    },
    {
      "epoch": 11.868242393764143,
      "eval_loss": 2.7358014583587646,
      "eval_runtime": 17.4369,
      "eval_samples_per_second": 768.142,
      "eval_steps_per_second": 12.043,
      "step": 47200
    },
    {
      "epoch": 11.893386975106864,
      "grad_norm": 2.7629101276397705,
      "learning_rate": 4.943018867924529e-05,
      "loss": 2.7781,
      "step": 47300
    },
    {
      "epoch": 11.918531556449585,
      "grad_norm": 2.758486032485962,
      "learning_rate": 4.9428930817610064e-05,
      "loss": 2.7809,
      "step": 47400
    },
    {
      "epoch": 11.943676137792306,
      "grad_norm": 2.854532480239868,
      "learning_rate": 4.9427672955974847e-05,
      "loss": 2.7724,
      "step": 47500
    },
    {
      "epoch": 11.968820719135026,
      "grad_norm": 2.9609181880950928,
      "learning_rate": 4.942641509433962e-05,
      "loss": 2.7651,
      "step": 47600
    },
    {
      "epoch": 11.968820719135026,
      "eval_loss": 2.736401319503784,
      "eval_runtime": 17.4639,
      "eval_samples_per_second": 766.952,
      "eval_steps_per_second": 12.025,
      "step": 47600
    },
    {
      "epoch": 11.993965300477747,
      "grad_norm": 2.6735575199127197,
      "learning_rate": 4.9425157232704404e-05,
      "loss": 2.783,
      "step": 47700
    },
    {
      "epoch": 12.019109881820468,
      "grad_norm": 3.006073236465454,
      "learning_rate": 4.942389937106918e-05,
      "loss": 2.7593,
      "step": 47800
    },
    {
      "epoch": 12.044254463163188,
      "grad_norm": 2.6766037940979004,
      "learning_rate": 4.942264150943396e-05,
      "loss": 2.7401,
      "step": 47900
    },
    {
      "epoch": 12.069399044505909,
      "grad_norm": 2.904358386993408,
      "learning_rate": 4.9421383647798744e-05,
      "loss": 2.7506,
      "step": 48000
    },
    {
      "epoch": 12.069399044505909,
      "eval_loss": 2.7371480464935303,
      "eval_runtime": 17.4581,
      "eval_samples_per_second": 767.208,
      "eval_steps_per_second": 12.029,
      "step": 48000
    },
    {
      "epoch": 12.09454362584863,
      "grad_norm": 3.1200451850891113,
      "learning_rate": 4.9420125786163526e-05,
      "loss": 2.7497,
      "step": 48100
    },
    {
      "epoch": 12.11968820719135,
      "grad_norm": 2.99312162399292,
      "learning_rate": 4.941886792452831e-05,
      "loss": 2.7569,
      "step": 48200
    },
    {
      "epoch": 12.144832788534071,
      "grad_norm": 2.8963990211486816,
      "learning_rate": 4.9417610062893084e-05,
      "loss": 2.7618,
      "step": 48300
    },
    {
      "epoch": 12.169977369876792,
      "grad_norm": 2.743134021759033,
      "learning_rate": 4.9416352201257866e-05,
      "loss": 2.7442,
      "step": 48400
    },
    {
      "epoch": 12.169977369876792,
      "eval_loss": 2.7342779636383057,
      "eval_runtime": 17.3035,
      "eval_samples_per_second": 774.065,
      "eval_steps_per_second": 12.136,
      "step": 48400
    },
    {
      "epoch": 12.195121951219512,
      "grad_norm": 3.0559170246124268,
      "learning_rate": 4.941509433962264e-05,
      "loss": 2.7487,
      "step": 48500
    },
    {
      "epoch": 12.220266532562233,
      "grad_norm": 3.0634188652038574,
      "learning_rate": 4.9413836477987424e-05,
      "loss": 2.7539,
      "step": 48600
    },
    {
      "epoch": 12.245411113904954,
      "grad_norm": 2.9832916259765625,
      "learning_rate": 4.94125786163522e-05,
      "loss": 2.7639,
      "step": 48700
    },
    {
      "epoch": 12.270555695247674,
      "grad_norm": 2.7839548587799072,
      "learning_rate": 4.941132075471698e-05,
      "loss": 2.7418,
      "step": 48800
    },
    {
      "epoch": 12.270555695247674,
      "eval_loss": 2.7310616970062256,
      "eval_runtime": 17.4466,
      "eval_samples_per_second": 767.714,
      "eval_steps_per_second": 12.037,
      "step": 48800
    },
    {
      "epoch": 12.295700276590395,
      "grad_norm": 3.145413637161255,
      "learning_rate": 4.9410062893081764e-05,
      "loss": 2.7544,
      "step": 48900
    },
    {
      "epoch": 12.320844857933116,
      "grad_norm": 2.983288049697876,
      "learning_rate": 4.940880503144654e-05,
      "loss": 2.7651,
      "step": 49000
    },
    {
      "epoch": 12.345989439275836,
      "grad_norm": 2.7218589782714844,
      "learning_rate": 4.940754716981132e-05,
      "loss": 2.7656,
      "step": 49100
    },
    {
      "epoch": 12.371134020618557,
      "grad_norm": 3.054539680480957,
      "learning_rate": 4.9406289308176104e-05,
      "loss": 2.7528,
      "step": 49200
    },
    {
      "epoch": 12.371134020618557,
      "eval_loss": 2.731501579284668,
      "eval_runtime": 17.4126,
      "eval_samples_per_second": 769.213,
      "eval_steps_per_second": 12.06,
      "step": 49200
    },
    {
      "epoch": 12.396278601961278,
      "grad_norm": 2.999204158782959,
      "learning_rate": 4.9405031446540886e-05,
      "loss": 2.7551,
      "step": 49300
    },
    {
      "epoch": 12.421423183303999,
      "grad_norm": 2.9528958797454834,
      "learning_rate": 4.940377358490566e-05,
      "loss": 2.7669,
      "step": 49400
    },
    {
      "epoch": 12.44656776464672,
      "grad_norm": 3.101876974105835,
      "learning_rate": 4.9402515723270444e-05,
      "loss": 2.7638,
      "step": 49500
    },
    {
      "epoch": 12.47171234598944,
      "grad_norm": 2.974886178970337,
      "learning_rate": 4.9401257861635226e-05,
      "loss": 2.7558,
      "step": 49600
    },
    {
      "epoch": 12.47171234598944,
      "eval_loss": 2.7292823791503906,
      "eval_runtime": 17.3322,
      "eval_samples_per_second": 772.782,
      "eval_steps_per_second": 12.116,
      "step": 49600
    },
    {
      "epoch": 12.49685692733216,
      "grad_norm": 2.771876811981201,
      "learning_rate": 4.94e-05,
      "loss": 2.7476,
      "step": 49700
    },
    {
      "epoch": 12.522001508674881,
      "grad_norm": 2.892273187637329,
      "learning_rate": 4.9398742138364784e-05,
      "loss": 2.7546,
      "step": 49800
    },
    {
      "epoch": 12.547146090017602,
      "grad_norm": 2.8775713443756104,
      "learning_rate": 4.939748427672956e-05,
      "loss": 2.7533,
      "step": 49900
    },
    {
      "epoch": 12.572290671360323,
      "grad_norm": 2.8176236152648926,
      "learning_rate": 4.939622641509434e-05,
      "loss": 2.7561,
      "step": 50000
    },
    {
      "epoch": 12.572290671360323,
      "eval_loss": 2.728229284286499,
      "eval_runtime": 17.165,
      "eval_samples_per_second": 780.308,
      "eval_steps_per_second": 12.234,
      "step": 50000
    },
    {
      "epoch": 12.597435252703043,
      "grad_norm": 2.787327527999878,
      "learning_rate": 4.939496855345912e-05,
      "loss": 2.7546,
      "step": 50100
    },
    {
      "epoch": 12.622579834045762,
      "grad_norm": 2.7617785930633545,
      "learning_rate": 4.93937106918239e-05,
      "loss": 2.7598,
      "step": 50200
    },
    {
      "epoch": 12.647724415388485,
      "grad_norm": 2.773205518722534,
      "learning_rate": 4.939245283018868e-05,
      "loss": 2.7486,
      "step": 50300
    },
    {
      "epoch": 12.672868996731204,
      "grad_norm": 3.0101675987243652,
      "learning_rate": 4.9391194968553463e-05,
      "loss": 2.7391,
      "step": 50400
    },
    {
      "epoch": 12.672868996731204,
      "eval_loss": 2.726348638534546,
      "eval_runtime": 17.2579,
      "eval_samples_per_second": 776.106,
      "eval_steps_per_second": 12.168,
      "step": 50400
    },
    {
      "epoch": 12.698013578073924,
      "grad_norm": 2.8816075325012207,
      "learning_rate": 4.9389937106918246e-05,
      "loss": 2.7523,
      "step": 50500
    },
    {
      "epoch": 12.723158159416645,
      "grad_norm": 2.970957040786743,
      "learning_rate": 4.938867924528302e-05,
      "loss": 2.7598,
      "step": 50600
    },
    {
      "epoch": 12.748302740759366,
      "grad_norm": 3.0629587173461914,
      "learning_rate": 4.93874213836478e-05,
      "loss": 2.7537,
      "step": 50700
    },
    {
      "epoch": 12.773447322102086,
      "grad_norm": 2.956465482711792,
      "learning_rate": 4.938616352201258e-05,
      "loss": 2.7551,
      "step": 50800
    },
    {
      "epoch": 12.773447322102086,
      "eval_loss": 2.7230992317199707,
      "eval_runtime": 17.2803,
      "eval_samples_per_second": 775.104,
      "eval_steps_per_second": 12.153,
      "step": 50800
    },
    {
      "epoch": 12.798591903444807,
      "grad_norm": 2.8440234661102295,
      "learning_rate": 4.938490566037736e-05,
      "loss": 2.7462,
      "step": 50900
    },
    {
      "epoch": 12.823736484787528,
      "grad_norm": 2.778142213821411,
      "learning_rate": 4.9383647798742136e-05,
      "loss": 2.753,
      "step": 51000
    },
    {
      "epoch": 12.848881066130248,
      "grad_norm": 2.780057668685913,
      "learning_rate": 4.938238993710692e-05,
      "loss": 2.7604,
      "step": 51100
    },
    {
      "epoch": 12.874025647472969,
      "grad_norm": 2.7645275592803955,
      "learning_rate": 4.93811320754717e-05,
      "loss": 2.7594,
      "step": 51200
    },
    {
      "epoch": 12.874025647472969,
      "eval_loss": 2.7215452194213867,
      "eval_runtime": 17.4707,
      "eval_samples_per_second": 766.655,
      "eval_steps_per_second": 12.02,
      "step": 51200
    },
    {
      "epoch": 12.89917022881569,
      "grad_norm": 3.0149660110473633,
      "learning_rate": 4.937987421383648e-05,
      "loss": 2.7591,
      "step": 51300
    },
    {
      "epoch": 12.92431481015841,
      "grad_norm": 2.8265609741210938,
      "learning_rate": 4.9378616352201265e-05,
      "loss": 2.7579,
      "step": 51400
    },
    {
      "epoch": 12.949459391501131,
      "grad_norm": 3.105048179626465,
      "learning_rate": 4.937735849056604e-05,
      "loss": 2.7705,
      "step": 51500
    },
    {
      "epoch": 12.974603972843852,
      "grad_norm": 2.691718101501465,
      "learning_rate": 4.937610062893082e-05,
      "loss": 2.7551,
      "step": 51600
    },
    {
      "epoch": 12.974603972843852,
      "eval_loss": 2.7228052616119385,
      "eval_runtime": 17.3044,
      "eval_samples_per_second": 774.022,
      "eval_steps_per_second": 12.136,
      "step": 51600
    },
    {
      "epoch": 12.999748554186572,
      "grad_norm": 2.8032963275909424,
      "learning_rate": 4.93748427672956e-05,
      "loss": 2.7597,
      "step": 51700
    },
    {
      "epoch": 13.024893135529293,
      "grad_norm": 2.988888740539551,
      "learning_rate": 4.937358490566038e-05,
      "loss": 2.7128,
      "step": 51800
    },
    {
      "epoch": 13.050037716872014,
      "grad_norm": 2.7689242362976074,
      "learning_rate": 4.9372327044025156e-05,
      "loss": 2.7323,
      "step": 51900
    },
    {
      "epoch": 13.075182298214735,
      "grad_norm": 2.740121364593506,
      "learning_rate": 4.937106918238994e-05,
      "loss": 2.7366,
      "step": 52000
    },
    {
      "epoch": 13.075182298214735,
      "eval_loss": 2.720271110534668,
      "eval_runtime": 17.3157,
      "eval_samples_per_second": 773.516,
      "eval_steps_per_second": 12.128,
      "step": 52000
    },
    {
      "epoch": 13.100326879557455,
      "grad_norm": 2.9075567722320557,
      "learning_rate": 4.936981132075472e-05,
      "loss": 2.7166,
      "step": 52100
    },
    {
      "epoch": 13.125471460900176,
      "grad_norm": 2.882800817489624,
      "learning_rate": 4.9368553459119496e-05,
      "loss": 2.7249,
      "step": 52200
    },
    {
      "epoch": 13.150616042242897,
      "grad_norm": 2.85158371925354,
      "learning_rate": 4.936729559748428e-05,
      "loss": 2.74,
      "step": 52300
    },
    {
      "epoch": 13.175760623585617,
      "grad_norm": 2.955763339996338,
      "learning_rate": 4.936603773584906e-05,
      "loss": 2.729,
      "step": 52400
    },
    {
      "epoch": 13.175760623585617,
      "eval_loss": 2.7217764854431152,
      "eval_runtime": 17.3465,
      "eval_samples_per_second": 772.145,
      "eval_steps_per_second": 12.106,
      "step": 52400
    },
    {
      "epoch": 13.200905204928338,
      "grad_norm": 2.72990083694458,
      "learning_rate": 4.936477987421384e-05,
      "loss": 2.7362,
      "step": 52500
    },
    {
      "epoch": 13.226049786271059,
      "grad_norm": 2.8989789485931396,
      "learning_rate": 4.936352201257862e-05,
      "loss": 2.7344,
      "step": 52600
    },
    {
      "epoch": 13.25119436761378,
      "grad_norm": 2.705209970474243,
      "learning_rate": 4.93622641509434e-05,
      "loss": 2.7381,
      "step": 52700
    },
    {
      "epoch": 13.2763389489565,
      "grad_norm": 2.7289912700653076,
      "learning_rate": 4.9361006289308176e-05,
      "loss": 2.7221,
      "step": 52800
    },
    {
      "epoch": 13.2763389489565,
      "eval_loss": 2.7194972038269043,
      "eval_runtime": 17.5554,
      "eval_samples_per_second": 762.957,
      "eval_steps_per_second": 11.962,
      "step": 52800
    },
    {
      "epoch": 13.30148353029922,
      "grad_norm": 2.698352813720703,
      "learning_rate": 4.935974842767296e-05,
      "loss": 2.7327,
      "step": 52900
    },
    {
      "epoch": 13.326628111641941,
      "grad_norm": 2.8614423274993896,
      "learning_rate": 4.935849056603774e-05,
      "loss": 2.7347,
      "step": 53000
    },
    {
      "epoch": 13.351772692984662,
      "grad_norm": 2.9313066005706787,
      "learning_rate": 4.9357232704402516e-05,
      "loss": 2.7372,
      "step": 53100
    },
    {
      "epoch": 13.376917274327383,
      "grad_norm": 3.1551287174224854,
      "learning_rate": 4.93559748427673e-05,
      "loss": 2.7354,
      "step": 53200
    },
    {
      "epoch": 13.376917274327383,
      "eval_loss": 2.7181599140167236,
      "eval_runtime": 17.3752,
      "eval_samples_per_second": 770.868,
      "eval_steps_per_second": 12.086,
      "step": 53200
    },
    {
      "epoch": 13.402061855670103,
      "grad_norm": 2.8272814750671387,
      "learning_rate": 4.9354716981132074e-05,
      "loss": 2.7267,
      "step": 53300
    },
    {
      "epoch": 13.427206437012824,
      "grad_norm": 2.7897725105285645,
      "learning_rate": 4.9353459119496856e-05,
      "loss": 2.731,
      "step": 53400
    },
    {
      "epoch": 13.452351018355545,
      "grad_norm": 2.579282522201538,
      "learning_rate": 4.935220125786164e-05,
      "loss": 2.7485,
      "step": 53500
    },
    {
      "epoch": 13.477495599698265,
      "grad_norm": 2.689899444580078,
      "learning_rate": 4.935094339622642e-05,
      "loss": 2.7347,
      "step": 53600
    },
    {
      "epoch": 13.477495599698265,
      "eval_loss": 2.7171404361724854,
      "eval_runtime": 17.3526,
      "eval_samples_per_second": 771.874,
      "eval_steps_per_second": 12.102,
      "step": 53600
    },
    {
      "epoch": 13.502640181040986,
      "grad_norm": 3.0163681507110596,
      "learning_rate": 4.9349685534591196e-05,
      "loss": 2.7409,
      "step": 53700
    },
    {
      "epoch": 13.527784762383707,
      "grad_norm": 2.6347551345825195,
      "learning_rate": 4.934842767295598e-05,
      "loss": 2.746,
      "step": 53800
    },
    {
      "epoch": 13.552929343726428,
      "grad_norm": 2.741657018661499,
      "learning_rate": 4.934716981132076e-05,
      "loss": 2.7335,
      "step": 53900
    },
    {
      "epoch": 13.578073925069148,
      "grad_norm": 2.644843816757202,
      "learning_rate": 4.9345911949685536e-05,
      "loss": 2.7498,
      "step": 54000
    },
    {
      "epoch": 13.578073925069148,
      "eval_loss": 2.7137818336486816,
      "eval_runtime": 17.4353,
      "eval_samples_per_second": 768.213,
      "eval_steps_per_second": 12.045,
      "step": 54000
    },
    {
      "epoch": 13.603218506411869,
      "grad_norm": 2.7251839637756348,
      "learning_rate": 4.934465408805032e-05,
      "loss": 2.7501,
      "step": 54100
    },
    {
      "epoch": 13.62836308775459,
      "grad_norm": 2.840683937072754,
      "learning_rate": 4.934339622641509e-05,
      "loss": 2.7493,
      "step": 54200
    },
    {
      "epoch": 13.65350766909731,
      "grad_norm": 2.960531711578369,
      "learning_rate": 4.9342138364779876e-05,
      "loss": 2.7354,
      "step": 54300
    },
    {
      "epoch": 13.678652250440031,
      "grad_norm": 2.9085795879364014,
      "learning_rate": 4.934088050314465e-05,
      "loss": 2.7371,
      "step": 54400
    },
    {
      "epoch": 13.678652250440031,
      "eval_loss": 2.71317720413208,
      "eval_runtime": 17.3295,
      "eval_samples_per_second": 772.9,
      "eval_steps_per_second": 12.118,
      "step": 54400
    },
    {
      "epoch": 13.70379683178275,
      "grad_norm": 2.973602294921875,
      "learning_rate": 4.933962264150943e-05,
      "loss": 2.7422,
      "step": 54500
    },
    {
      "epoch": 13.728941413125472,
      "grad_norm": 2.9103031158447266,
      "learning_rate": 4.9338364779874215e-05,
      "loss": 2.7504,
      "step": 54600
    },
    {
      "epoch": 13.754085994468191,
      "grad_norm": 3.007678508758545,
      "learning_rate": 4.9337106918239e-05,
      "loss": 2.7345,
      "step": 54700
    },
    {
      "epoch": 13.779230575810912,
      "grad_norm": 2.826658010482788,
      "learning_rate": 4.933584905660378e-05,
      "loss": 2.75,
      "step": 54800
    },
    {
      "epoch": 13.779230575810912,
      "eval_loss": 2.7104125022888184,
      "eval_runtime": 17.1802,
      "eval_samples_per_second": 779.62,
      "eval_steps_per_second": 12.223,
      "step": 54800
    },
    {
      "epoch": 13.804375157153633,
      "grad_norm": 2.9033985137939453,
      "learning_rate": 4.9334591194968555e-05,
      "loss": 2.7343,
      "step": 54900
    },
    {
      "epoch": 13.829519738496353,
      "grad_norm": 3.1122000217437744,
      "learning_rate": 4.933333333333334e-05,
      "loss": 2.7449,
      "step": 55000
    },
    {
      "epoch": 13.854664319839074,
      "grad_norm": 3.1255035400390625,
      "learning_rate": 4.933207547169811e-05,
      "loss": 2.7459,
      "step": 55100
    },
    {
      "epoch": 13.879808901181795,
      "grad_norm": 2.4808552265167236,
      "learning_rate": 4.9330817610062895e-05,
      "loss": 2.7342,
      "step": 55200
    },
    {
      "epoch": 13.879808901181795,
      "eval_loss": 2.709855794906616,
      "eval_runtime": 17.2293,
      "eval_samples_per_second": 777.399,
      "eval_steps_per_second": 12.189,
      "step": 55200
    },
    {
      "epoch": 13.904953482524515,
      "grad_norm": 2.752444267272949,
      "learning_rate": 4.932955974842767e-05,
      "loss": 2.7374,
      "step": 55300
    },
    {
      "epoch": 13.930098063867236,
      "grad_norm": 2.5422539710998535,
      "learning_rate": 4.932830188679245e-05,
      "loss": 2.7363,
      "step": 55400
    },
    {
      "epoch": 13.955242645209957,
      "grad_norm": 2.7706127166748047,
      "learning_rate": 4.9327044025157235e-05,
      "loss": 2.7474,
      "step": 55500
    },
    {
      "epoch": 13.980387226552677,
      "grad_norm": 2.7473137378692627,
      "learning_rate": 4.932578616352202e-05,
      "loss": 2.7464,
      "step": 55600
    },
    {
      "epoch": 13.980387226552677,
      "eval_loss": 2.7086641788482666,
      "eval_runtime": 17.2926,
      "eval_samples_per_second": 774.549,
      "eval_steps_per_second": 12.144,
      "step": 55600
    },
    {
      "epoch": 14.005531807895398,
      "grad_norm": 2.8716883659362793,
      "learning_rate": 4.93245283018868e-05,
      "loss": 2.7357,
      "step": 55700
    },
    {
      "epoch": 14.030676389238119,
      "grad_norm": 2.8326478004455566,
      "learning_rate": 4.9323270440251575e-05,
      "loss": 2.7043,
      "step": 55800
    },
    {
      "epoch": 14.05582097058084,
      "grad_norm": 2.6206326484680176,
      "learning_rate": 4.932201257861636e-05,
      "loss": 2.7003,
      "step": 55900
    },
    {
      "epoch": 14.08096555192356,
      "grad_norm": 3.1308670043945312,
      "learning_rate": 4.932075471698113e-05,
      "loss": 2.7095,
      "step": 56000
    },
    {
      "epoch": 14.08096555192356,
      "eval_loss": 2.71010160446167,
      "eval_runtime": 17.3468,
      "eval_samples_per_second": 772.133,
      "eval_steps_per_second": 12.106,
      "step": 56000
    },
    {
      "epoch": 14.10611013326628,
      "grad_norm": 2.8550658226013184,
      "learning_rate": 4.9319496855345915e-05,
      "loss": 2.7019,
      "step": 56100
    },
    {
      "epoch": 14.131254714609002,
      "grad_norm": 2.936612367630005,
      "learning_rate": 4.931823899371069e-05,
      "loss": 2.7101,
      "step": 56200
    },
    {
      "epoch": 14.156399295951722,
      "grad_norm": 2.829465389251709,
      "learning_rate": 4.931698113207547e-05,
      "loss": 2.7102,
      "step": 56300
    },
    {
      "epoch": 14.181543877294443,
      "grad_norm": 3.1277072429656982,
      "learning_rate": 4.9315723270440255e-05,
      "loss": 2.7205,
      "step": 56400
    },
    {
      "epoch": 14.181543877294443,
      "eval_loss": 2.7103123664855957,
      "eval_runtime": 17.2673,
      "eval_samples_per_second": 775.687,
      "eval_steps_per_second": 12.162,
      "step": 56400
    },
    {
      "epoch": 14.206688458637164,
      "grad_norm": 2.8786842823028564,
      "learning_rate": 4.931446540880503e-05,
      "loss": 2.7248,
      "step": 56500
    },
    {
      "epoch": 14.231833039979884,
      "grad_norm": 2.8368701934814453,
      "learning_rate": 4.931320754716981e-05,
      "loss": 2.726,
      "step": 56600
    },
    {
      "epoch": 14.256977621322605,
      "grad_norm": 2.7589986324310303,
      "learning_rate": 4.9311949685534595e-05,
      "loss": 2.7194,
      "step": 56700
    },
    {
      "epoch": 14.282122202665326,
      "grad_norm": 2.7424654960632324,
      "learning_rate": 4.931069182389938e-05,
      "loss": 2.7184,
      "step": 56800
    },
    {
      "epoch": 14.282122202665326,
      "eval_loss": 2.7110910415649414,
      "eval_runtime": 17.3186,
      "eval_samples_per_second": 773.389,
      "eval_steps_per_second": 12.126,
      "step": 56800
    },
    {
      "epoch": 14.307266784008046,
      "grad_norm": 2.6316399574279785,
      "learning_rate": 4.930943396226415e-05,
      "loss": 2.7184,
      "step": 56900
    },
    {
      "epoch": 14.332411365350767,
      "grad_norm": 2.647080659866333,
      "learning_rate": 4.9308176100628935e-05,
      "loss": 2.7213,
      "step": 57000
    },
    {
      "epoch": 14.357555946693488,
      "grad_norm": 2.7004077434539795,
      "learning_rate": 4.930691823899372e-05,
      "loss": 2.7231,
      "step": 57100
    },
    {
      "epoch": 14.382700528036208,
      "grad_norm": 2.8845596313476562,
      "learning_rate": 4.930566037735849e-05,
      "loss": 2.7153,
      "step": 57200
    },
    {
      "epoch": 14.382700528036208,
      "eval_loss": 2.7085745334625244,
      "eval_runtime": 17.3866,
      "eval_samples_per_second": 770.364,
      "eval_steps_per_second": 12.078,
      "step": 57200
    },
    {
      "epoch": 14.407845109378929,
      "grad_norm": 2.8206863403320312,
      "learning_rate": 4.9304402515723275e-05,
      "loss": 2.7159,
      "step": 57300
    },
    {
      "epoch": 14.43298969072165,
      "grad_norm": 2.785552978515625,
      "learning_rate": 4.930314465408805e-05,
      "loss": 2.7191,
      "step": 57400
    },
    {
      "epoch": 14.45813427206437,
      "grad_norm": 2.6475090980529785,
      "learning_rate": 4.930188679245283e-05,
      "loss": 2.7128,
      "step": 57500
    },
    {
      "epoch": 14.483278853407091,
      "grad_norm": 2.7124276161193848,
      "learning_rate": 4.930062893081761e-05,
      "loss": 2.7326,
      "step": 57600
    },
    {
      "epoch": 14.483278853407091,
      "eval_loss": 2.7080185413360596,
      "eval_runtime": 17.3224,
      "eval_samples_per_second": 773.219,
      "eval_steps_per_second": 12.123,
      "step": 57600
    },
    {
      "epoch": 14.508423434749812,
      "grad_norm": 2.8921942710876465,
      "learning_rate": 4.929937106918239e-05,
      "loss": 2.7288,
      "step": 57700
    },
    {
      "epoch": 14.533568016092532,
      "grad_norm": 2.92561674118042,
      "learning_rate": 4.929811320754717e-05,
      "loss": 2.7155,
      "step": 57800
    },
    {
      "epoch": 14.558712597435253,
      "grad_norm": 2.8783040046691895,
      "learning_rate": 4.9296855345911955e-05,
      "loss": 2.7261,
      "step": 57900
    },
    {
      "epoch": 14.583857178777974,
      "grad_norm": 2.966825485229492,
      "learning_rate": 4.929559748427674e-05,
      "loss": 2.7281,
      "step": 58000
    },
    {
      "epoch": 14.583857178777974,
      "eval_loss": 2.703486204147339,
      "eval_runtime": 17.3094,
      "eval_samples_per_second": 773.8,
      "eval_steps_per_second": 12.132,
      "step": 58000
    },
    {
      "epoch": 14.609001760120695,
      "grad_norm": 2.8753769397735596,
      "learning_rate": 4.929433962264151e-05,
      "loss": 2.7248,
      "step": 58100
    },
    {
      "epoch": 14.634146341463415,
      "grad_norm": 2.5932440757751465,
      "learning_rate": 4.9293081761006294e-05,
      "loss": 2.7096,
      "step": 58200
    },
    {
      "epoch": 14.659290922806136,
      "grad_norm": 2.600262403488159,
      "learning_rate": 4.929182389937107e-05,
      "loss": 2.7264,
      "step": 58300
    },
    {
      "epoch": 14.684435504148857,
      "grad_norm": 2.783888101577759,
      "learning_rate": 4.929056603773585e-05,
      "loss": 2.7368,
      "step": 58400
    },
    {
      "epoch": 14.684435504148857,
      "eval_loss": 2.701838254928589,
      "eval_runtime": 17.2383,
      "eval_samples_per_second": 776.992,
      "eval_steps_per_second": 12.182,
      "step": 58400
    },
    {
      "epoch": 14.709580085491577,
      "grad_norm": 2.866851568222046,
      "learning_rate": 4.928930817610063e-05,
      "loss": 2.7213,
      "step": 58500
    },
    {
      "epoch": 14.734724666834298,
      "grad_norm": 2.9107608795166016,
      "learning_rate": 4.928805031446541e-05,
      "loss": 2.7261,
      "step": 58600
    },
    {
      "epoch": 14.759869248177019,
      "grad_norm": 2.7636821269989014,
      "learning_rate": 4.9286792452830185e-05,
      "loss": 2.7291,
      "step": 58700
    },
    {
      "epoch": 14.78501382951974,
      "grad_norm": 2.9609720706939697,
      "learning_rate": 4.928553459119497e-05,
      "loss": 2.7407,
      "step": 58800
    },
    {
      "epoch": 14.78501382951974,
      "eval_loss": 2.7013957500457764,
      "eval_runtime": 17.4487,
      "eval_samples_per_second": 767.621,
      "eval_steps_per_second": 12.035,
      "step": 58800
    },
    {
      "epoch": 14.81015841086246,
      "grad_norm": 2.6076769828796387,
      "learning_rate": 4.928427672955975e-05,
      "loss": 2.7113,
      "step": 58900
    },
    {
      "epoch": 14.835302992205179,
      "grad_norm": 2.7776899337768555,
      "learning_rate": 4.928301886792453e-05,
      "loss": 2.7275,
      "step": 59000
    },
    {
      "epoch": 14.860447573547901,
      "grad_norm": 2.8571863174438477,
      "learning_rate": 4.9281761006289314e-05,
      "loss": 2.7024,
      "step": 59100
    },
    {
      "epoch": 14.88559215489062,
      "grad_norm": 2.6583597660064697,
      "learning_rate": 4.928050314465409e-05,
      "loss": 2.7306,
      "step": 59200
    },
    {
      "epoch": 14.88559215489062,
      "eval_loss": 2.699665069580078,
      "eval_runtime": 17.2349,
      "eval_samples_per_second": 777.146,
      "eval_steps_per_second": 12.185,
      "step": 59200
    },
    {
      "epoch": 14.910736736233341,
      "grad_norm": 2.6024420261383057,
      "learning_rate": 4.927924528301887e-05,
      "loss": 2.7249,
      "step": 59300
    },
    {
      "epoch": 14.935881317576062,
      "grad_norm": 2.6951944828033447,
      "learning_rate": 4.927798742138365e-05,
      "loss": 2.7378,
      "step": 59400
    },
    {
      "epoch": 14.961025898918782,
      "grad_norm": 2.7434639930725098,
      "learning_rate": 4.927672955974843e-05,
      "loss": 2.7371,
      "step": 59500
    },
    {
      "epoch": 14.986170480261503,
      "grad_norm": 2.8276236057281494,
      "learning_rate": 4.927547169811321e-05,
      "loss": 2.7326,
      "step": 59600
    },
    {
      "epoch": 14.986170480261503,
      "eval_loss": 2.699355125427246,
      "eval_runtime": 17.3663,
      "eval_samples_per_second": 771.264,
      "eval_steps_per_second": 12.092,
      "step": 59600
    },
    {
      "epoch": 15.011315061604224,
      "grad_norm": 2.6462271213531494,
      "learning_rate": 4.927421383647799e-05,
      "loss": 2.716,
      "step": 59700
    },
    {
      "epoch": 15.036459642946944,
      "grad_norm": 2.90348744392395,
      "learning_rate": 4.927295597484277e-05,
      "loss": 2.6961,
      "step": 59800
    },
    {
      "epoch": 15.061604224289665,
      "grad_norm": 2.754289388656616,
      "learning_rate": 4.927169811320755e-05,
      "loss": 2.6927,
      "step": 59900
    },
    {
      "epoch": 15.086748805632386,
      "grad_norm": 3.135054111480713,
      "learning_rate": 4.9270440251572334e-05,
      "loss": 2.7008,
      "step": 60000
    },
    {
      "epoch": 15.086748805632386,
      "eval_loss": 2.6995840072631836,
      "eval_runtime": 17.2689,
      "eval_samples_per_second": 775.616,
      "eval_steps_per_second": 12.161,
      "step": 60000
    },
    {
      "epoch": 15.111893386975106,
      "grad_norm": 2.8550190925598145,
      "learning_rate": 4.926918238993711e-05,
      "loss": 2.7042,
      "step": 60100
    },
    {
      "epoch": 15.137037968317827,
      "grad_norm": 3.097838878631592,
      "learning_rate": 4.926792452830189e-05,
      "loss": 2.7015,
      "step": 60200
    },
    {
      "epoch": 15.162182549660548,
      "grad_norm": 3.230670690536499,
      "learning_rate": 4.926666666666667e-05,
      "loss": 2.7082,
      "step": 60300
    },
    {
      "epoch": 15.187327131003268,
      "grad_norm": 3.10626482963562,
      "learning_rate": 4.926540880503145e-05,
      "loss": 2.692,
      "step": 60400
    },
    {
      "epoch": 15.187327131003268,
      "eval_loss": 2.697826623916626,
      "eval_runtime": 17.2308,
      "eval_samples_per_second": 777.331,
      "eval_steps_per_second": 12.188,
      "step": 60400
    },
    {
      "epoch": 15.21247171234599,
      "grad_norm": 2.489342212677002,
      "learning_rate": 4.926415094339623e-05,
      "loss": 2.7068,
      "step": 60500
    },
    {
      "epoch": 15.23761629368871,
      "grad_norm": 2.6303701400756836,
      "learning_rate": 4.926289308176101e-05,
      "loss": 2.696,
      "step": 60600
    },
    {
      "epoch": 15.26276087503143,
      "grad_norm": 2.726410150527954,
      "learning_rate": 4.926163522012579e-05,
      "loss": 2.7038,
      "step": 60700
    },
    {
      "epoch": 15.287905456374151,
      "grad_norm": 2.780485153198242,
      "learning_rate": 4.9260377358490565e-05,
      "loss": 2.713,
      "step": 60800
    },
    {
      "epoch": 15.287905456374151,
      "eval_loss": 2.7004776000976562,
      "eval_runtime": 17.2095,
      "eval_samples_per_second": 778.293,
      "eval_steps_per_second": 12.203,
      "step": 60800
    },
    {
      "epoch": 15.313050037716872,
      "grad_norm": 3.0196361541748047,
      "learning_rate": 4.925911949685535e-05,
      "loss": 2.712,
      "step": 60900
    },
    {
      "epoch": 15.338194619059593,
      "grad_norm": 2.6576406955718994,
      "learning_rate": 4.925786163522013e-05,
      "loss": 2.7061,
      "step": 61000
    },
    {
      "epoch": 15.363339200402313,
      "grad_norm": 2.8366098403930664,
      "learning_rate": 4.925660377358491e-05,
      "loss": 2.7058,
      "step": 61100
    },
    {
      "epoch": 15.388483781745034,
      "grad_norm": 2.7625811100006104,
      "learning_rate": 4.925534591194969e-05,
      "loss": 2.7122,
      "step": 61200
    },
    {
      "epoch": 15.388483781745034,
      "eval_loss": 2.7000505924224854,
      "eval_runtime": 17.2427,
      "eval_samples_per_second": 776.792,
      "eval_steps_per_second": 12.179,
      "step": 61200
    },
    {
      "epoch": 15.413628363087755,
      "grad_norm": 2.9356086254119873,
      "learning_rate": 4.925408805031447e-05,
      "loss": 2.6952,
      "step": 61300
    },
    {
      "epoch": 15.438772944430475,
      "grad_norm": 2.8865065574645996,
      "learning_rate": 4.925283018867925e-05,
      "loss": 2.7068,
      "step": 61400
    },
    {
      "epoch": 15.463917525773196,
      "grad_norm": 2.948911428451538,
      "learning_rate": 4.925157232704403e-05,
      "loss": 2.7096,
      "step": 61500
    },
    {
      "epoch": 15.489062107115917,
      "grad_norm": 2.7718560695648193,
      "learning_rate": 4.925031446540881e-05,
      "loss": 2.7036,
      "step": 61600
    },
    {
      "epoch": 15.489062107115917,
      "eval_loss": 2.6958727836608887,
      "eval_runtime": 17.3757,
      "eval_samples_per_second": 770.848,
      "eval_steps_per_second": 12.086,
      "step": 61600
    },
    {
      "epoch": 15.514206688458637,
      "grad_norm": 2.7970588207244873,
      "learning_rate": 4.9249056603773584e-05,
      "loss": 2.7063,
      "step": 61700
    },
    {
      "epoch": 15.539351269801358,
      "grad_norm": 2.757239818572998,
      "learning_rate": 4.924779874213837e-05,
      "loss": 2.7084,
      "step": 61800
    },
    {
      "epoch": 15.564495851144079,
      "grad_norm": 2.8620712757110596,
      "learning_rate": 4.924654088050314e-05,
      "loss": 2.7019,
      "step": 61900
    },
    {
      "epoch": 15.5896404324868,
      "grad_norm": 2.685335874557495,
      "learning_rate": 4.9245283018867924e-05,
      "loss": 2.7061,
      "step": 62000
    },
    {
      "epoch": 15.5896404324868,
      "eval_loss": 2.6964902877807617,
      "eval_runtime": 17.2701,
      "eval_samples_per_second": 775.558,
      "eval_steps_per_second": 12.16,
      "step": 62000
    },
    {
      "epoch": 15.61478501382952,
      "grad_norm": 2.8257997035980225,
      "learning_rate": 4.9244025157232707e-05,
      "loss": 2.7114,
      "step": 62100
    },
    {
      "epoch": 15.63992959517224,
      "grad_norm": 3.010803461074829,
      "learning_rate": 4.924276729559749e-05,
      "loss": 2.7026,
      "step": 62200
    },
    {
      "epoch": 15.665074176514961,
      "grad_norm": 3.074199914932251,
      "learning_rate": 4.924150943396227e-05,
      "loss": 2.7105,
      "step": 62300
    },
    {
      "epoch": 15.690218757857682,
      "grad_norm": 2.884316921234131,
      "learning_rate": 4.9240251572327046e-05,
      "loss": 2.7032,
      "step": 62400
    },
    {
      "epoch": 15.690218757857682,
      "eval_loss": 2.6944022178649902,
      "eval_runtime": 17.3529,
      "eval_samples_per_second": 771.859,
      "eval_steps_per_second": 12.102,
      "step": 62400
    },
    {
      "epoch": 15.715363339200403,
      "grad_norm": 2.8618807792663574,
      "learning_rate": 4.923899371069183e-05,
      "loss": 2.7154,
      "step": 62500
    },
    {
      "epoch": 15.740507920543124,
      "grad_norm": 2.9291393756866455,
      "learning_rate": 4.9237735849056604e-05,
      "loss": 2.7086,
      "step": 62600
    },
    {
      "epoch": 15.765652501885844,
      "grad_norm": 2.8065850734710693,
      "learning_rate": 4.9236477987421386e-05,
      "loss": 2.7095,
      "step": 62700
    },
    {
      "epoch": 15.790797083228565,
      "grad_norm": 2.6160099506378174,
      "learning_rate": 4.923522012578616e-05,
      "loss": 2.7106,
      "step": 62800
    },
    {
      "epoch": 15.790797083228565,
      "eval_loss": 2.691502571105957,
      "eval_runtime": 17.3762,
      "eval_samples_per_second": 770.823,
      "eval_steps_per_second": 12.085,
      "step": 62800
    },
    {
      "epoch": 15.815941664571286,
      "grad_norm": 2.765817880630493,
      "learning_rate": 4.9233962264150944e-05,
      "loss": 2.7123,
      "step": 62900
    },
    {
      "epoch": 15.841086245914006,
      "grad_norm": 2.9005069732666016,
      "learning_rate": 4.9232704402515726e-05,
      "loss": 2.7152,
      "step": 63000
    },
    {
      "epoch": 15.866230827256727,
      "grad_norm": 2.7633116245269775,
      "learning_rate": 4.92314465408805e-05,
      "loss": 2.7137,
      "step": 63100
    },
    {
      "epoch": 15.891375408599448,
      "grad_norm": 2.4830143451690674,
      "learning_rate": 4.9230188679245284e-05,
      "loss": 2.7054,
      "step": 63200
    },
    {
      "epoch": 15.891375408599448,
      "eval_loss": 2.6916608810424805,
      "eval_runtime": 17.4107,
      "eval_samples_per_second": 769.298,
      "eval_steps_per_second": 12.062,
      "step": 63200
    },
    {
      "epoch": 15.916519989942167,
      "grad_norm": 2.85998272895813,
      "learning_rate": 4.9228930817610066e-05,
      "loss": 2.722,
      "step": 63300
    },
    {
      "epoch": 15.941664571284889,
      "grad_norm": 2.536174774169922,
      "learning_rate": 4.922767295597485e-05,
      "loss": 2.7104,
      "step": 63400
    },
    {
      "epoch": 15.966809152627608,
      "grad_norm": 3.0074303150177,
      "learning_rate": 4.9226415094339624e-05,
      "loss": 2.6947,
      "step": 63500
    },
    {
      "epoch": 15.991953733970329,
      "grad_norm": 2.6087288856506348,
      "learning_rate": 4.9225157232704406e-05,
      "loss": 2.6997,
      "step": 63600
    },
    {
      "epoch": 15.991953733970329,
      "eval_loss": 2.689389705657959,
      "eval_runtime": 17.4245,
      "eval_samples_per_second": 768.689,
      "eval_steps_per_second": 12.052,
      "step": 63600
    },
    {
      "epoch": 16.01709831531305,
      "grad_norm": 2.693249225616455,
      "learning_rate": 4.922389937106918e-05,
      "loss": 2.6913,
      "step": 63700
    },
    {
      "epoch": 16.04224289665577,
      "grad_norm": 2.6771886348724365,
      "learning_rate": 4.9222641509433964e-05,
      "loss": 2.6812,
      "step": 63800
    },
    {
      "epoch": 16.06738747799849,
      "grad_norm": 2.6845850944519043,
      "learning_rate": 4.9221383647798746e-05,
      "loss": 2.6804,
      "step": 63900
    },
    {
      "epoch": 16.092532059341213,
      "grad_norm": 2.9349796772003174,
      "learning_rate": 4.922012578616352e-05,
      "loss": 2.6903,
      "step": 64000
    },
    {
      "epoch": 16.092532059341213,
      "eval_loss": 2.691448926925659,
      "eval_runtime": 17.275,
      "eval_samples_per_second": 775.342,
      "eval_steps_per_second": 12.156,
      "step": 64000
    },
    {
      "epoch": 16.117676640683932,
      "grad_norm": 2.7451670169830322,
      "learning_rate": 4.9218867924528304e-05,
      "loss": 2.6782,
      "step": 64100
    },
    {
      "epoch": 16.142821222026654,
      "grad_norm": 2.645718574523926,
      "learning_rate": 4.921761006289308e-05,
      "loss": 2.6765,
      "step": 64200
    },
    {
      "epoch": 16.167965803369373,
      "grad_norm": 2.9226489067077637,
      "learning_rate": 4.921635220125787e-05,
      "loss": 2.6781,
      "step": 64300
    },
    {
      "epoch": 16.193110384712096,
      "grad_norm": 2.8248074054718018,
      "learning_rate": 4.9215094339622644e-05,
      "loss": 2.6975,
      "step": 64400
    },
    {
      "epoch": 16.193110384712096,
      "eval_loss": 2.6906423568725586,
      "eval_runtime": 17.2406,
      "eval_samples_per_second": 776.886,
      "eval_steps_per_second": 12.181,
      "step": 64400
    },
    {
      "epoch": 16.218254966054815,
      "grad_norm": 2.971569538116455,
      "learning_rate": 4.9213836477987426e-05,
      "loss": 2.6916,
      "step": 64500
    },
    {
      "epoch": 16.243399547397537,
      "grad_norm": 2.7333173751831055,
      "learning_rate": 4.921257861635221e-05,
      "loss": 2.6906,
      "step": 64600
    },
    {
      "epoch": 16.268544128740256,
      "grad_norm": 2.725102424621582,
      "learning_rate": 4.9211320754716984e-05,
      "loss": 2.6954,
      "step": 64700
    },
    {
      "epoch": 16.29368871008298,
      "grad_norm": 2.8177318572998047,
      "learning_rate": 4.9210062893081766e-05,
      "loss": 2.6965,
      "step": 64800
    },
    {
      "epoch": 16.29368871008298,
      "eval_loss": 2.6942434310913086,
      "eval_runtime": 17.2392,
      "eval_samples_per_second": 776.95,
      "eval_steps_per_second": 12.182,
      "step": 64800
    },
    {
      "epoch": 16.318833291425697,
      "grad_norm": 2.949079990386963,
      "learning_rate": 4.920880503144654e-05,
      "loss": 2.6952,
      "step": 64900
    },
    {
      "epoch": 16.34397787276842,
      "grad_norm": 2.9608700275421143,
      "learning_rate": 4.9207547169811323e-05,
      "loss": 2.69,
      "step": 65000
    },
    {
      "epoch": 16.36912245411114,
      "grad_norm": 2.891474962234497,
      "learning_rate": 4.92062893081761e-05,
      "loss": 2.6799,
      "step": 65100
    },
    {
      "epoch": 16.39426703545386,
      "grad_norm": 2.781522274017334,
      "learning_rate": 4.920503144654088e-05,
      "loss": 2.6916,
      "step": 65200
    },
    {
      "epoch": 16.39426703545386,
      "eval_loss": 2.6886627674102783,
      "eval_runtime": 17.1981,
      "eval_samples_per_second": 778.808,
      "eval_steps_per_second": 12.211,
      "step": 65200
    },
    {
      "epoch": 16.41941161679658,
      "grad_norm": 2.60947847366333,
      "learning_rate": 4.920377358490566e-05,
      "loss": 2.7,
      "step": 65300
    },
    {
      "epoch": 16.444556198139303,
      "grad_norm": 2.862095832824707,
      "learning_rate": 4.9202515723270446e-05,
      "loss": 2.7016,
      "step": 65400
    },
    {
      "epoch": 16.46970077948202,
      "grad_norm": 2.753135919570923,
      "learning_rate": 4.920125786163523e-05,
      "loss": 2.7042,
      "step": 65500
    },
    {
      "epoch": 16.49484536082474,
      "grad_norm": 2.817699432373047,
      "learning_rate": 4.92e-05,
      "loss": 2.6884,
      "step": 65600
    },
    {
      "epoch": 16.49484536082474,
      "eval_loss": 2.6867873668670654,
      "eval_runtime": 17.4031,
      "eval_samples_per_second": 769.633,
      "eval_steps_per_second": 12.067,
      "step": 65600
    },
    {
      "epoch": 16.519989942167463,
      "grad_norm": 2.767061233520508,
      "learning_rate": 4.9198742138364786e-05,
      "loss": 2.7027,
      "step": 65700
    },
    {
      "epoch": 16.545134523510182,
      "grad_norm": 2.7309255599975586,
      "learning_rate": 4.919748427672956e-05,
      "loss": 2.6936,
      "step": 65800
    },
    {
      "epoch": 16.570279104852904,
      "grad_norm": 2.8696093559265137,
      "learning_rate": 4.919622641509434e-05,
      "loss": 2.6932,
      "step": 65900
    },
    {
      "epoch": 16.595423686195623,
      "grad_norm": 2.9617390632629395,
      "learning_rate": 4.919496855345912e-05,
      "loss": 2.6901,
      "step": 66000
    },
    {
      "epoch": 16.595423686195623,
      "eval_loss": 2.6868865489959717,
      "eval_runtime": 17.1825,
      "eval_samples_per_second": 779.516,
      "eval_steps_per_second": 12.222,
      "step": 66000
    },
    {
      "epoch": 16.620568267538346,
      "grad_norm": 2.75217866897583,
      "learning_rate": 4.91937106918239e-05,
      "loss": 2.6902,
      "step": 66100
    },
    {
      "epoch": 16.645712848881065,
      "grad_norm": 2.6201980113983154,
      "learning_rate": 4.9192452830188676e-05,
      "loss": 2.688,
      "step": 66200
    },
    {
      "epoch": 16.670857430223787,
      "grad_norm": 2.7351531982421875,
      "learning_rate": 4.919119496855346e-05,
      "loss": 2.694,
      "step": 66300
    },
    {
      "epoch": 16.696002011566506,
      "grad_norm": 2.659667730331421,
      "learning_rate": 4.918993710691824e-05,
      "loss": 2.6896,
      "step": 66400
    },
    {
      "epoch": 16.696002011566506,
      "eval_loss": 2.684405565261841,
      "eval_runtime": 17.3356,
      "eval_samples_per_second": 772.629,
      "eval_steps_per_second": 12.114,
      "step": 66400
    },
    {
      "epoch": 16.72114659290923,
      "grad_norm": 2.678738594055176,
      "learning_rate": 4.918867924528302e-05,
      "loss": 2.6858,
      "step": 66500
    },
    {
      "epoch": 16.746291174251947,
      "grad_norm": 2.625990390777588,
      "learning_rate": 4.9187421383647805e-05,
      "loss": 2.6903,
      "step": 66600
    },
    {
      "epoch": 16.77143575559467,
      "grad_norm": 2.6056790351867676,
      "learning_rate": 4.918616352201258e-05,
      "loss": 2.6945,
      "step": 66700
    },
    {
      "epoch": 16.79658033693739,
      "grad_norm": 2.8313510417938232,
      "learning_rate": 4.918490566037736e-05,
      "loss": 2.6985,
      "step": 66800
    },
    {
      "epoch": 16.79658033693739,
      "eval_loss": 2.6838467121124268,
      "eval_runtime": 17.2356,
      "eval_samples_per_second": 777.114,
      "eval_steps_per_second": 12.184,
      "step": 66800
    },
    {
      "epoch": 16.82172491828011,
      "grad_norm": 2.5711042881011963,
      "learning_rate": 4.918364779874214e-05,
      "loss": 2.7085,
      "step": 66900
    },
    {
      "epoch": 16.84686949962283,
      "grad_norm": 2.631377696990967,
      "learning_rate": 4.918238993710692e-05,
      "loss": 2.7031,
      "step": 67000
    },
    {
      "epoch": 16.872014080965553,
      "grad_norm": 2.643298625946045,
      "learning_rate": 4.91811320754717e-05,
      "loss": 2.6951,
      "step": 67100
    },
    {
      "epoch": 16.89715866230827,
      "grad_norm": 2.6832096576690674,
      "learning_rate": 4.917987421383648e-05,
      "loss": 2.6931,
      "step": 67200
    },
    {
      "epoch": 16.89715866230827,
      "eval_loss": 2.6848065853118896,
      "eval_runtime": 17.3376,
      "eval_samples_per_second": 772.54,
      "eval_steps_per_second": 12.112,
      "step": 67200
    },
    {
      "epoch": 16.922303243650994,
      "grad_norm": 2.744900941848755,
      "learning_rate": 4.917861635220126e-05,
      "loss": 2.6969,
      "step": 67300
    },
    {
      "epoch": 16.947447824993713,
      "grad_norm": 2.84122633934021,
      "learning_rate": 4.9177358490566036e-05,
      "loss": 2.7037,
      "step": 67400
    },
    {
      "epoch": 16.972592406336435,
      "grad_norm": 2.9415669441223145,
      "learning_rate": 4.917610062893082e-05,
      "loss": 2.701,
      "step": 67500
    },
    {
      "epoch": 16.997736987679154,
      "grad_norm": 2.6232171058654785,
      "learning_rate": 4.91748427672956e-05,
      "loss": 2.6989,
      "step": 67600
    },
    {
      "epoch": 16.997736987679154,
      "eval_loss": 2.6814353466033936,
      "eval_runtime": 17.3796,
      "eval_samples_per_second": 770.674,
      "eval_steps_per_second": 12.083,
      "step": 67600
    },
    {
      "epoch": 17.022881569021877,
      "grad_norm": 2.719024658203125,
      "learning_rate": 4.917358490566038e-05,
      "loss": 2.6677,
      "step": 67700
    },
    {
      "epoch": 17.048026150364596,
      "grad_norm": 2.683467149734497,
      "learning_rate": 4.917232704402516e-05,
      "loss": 2.663,
      "step": 67800
    },
    {
      "epoch": 17.073170731707318,
      "grad_norm": 2.7199783325195312,
      "learning_rate": 4.917106918238994e-05,
      "loss": 2.6761,
      "step": 67900
    },
    {
      "epoch": 17.098315313050037,
      "grad_norm": 2.6320414543151855,
      "learning_rate": 4.916981132075472e-05,
      "loss": 2.6742,
      "step": 68000
    },
    {
      "epoch": 17.098315313050037,
      "eval_loss": 2.684781789779663,
      "eval_runtime": 17.2895,
      "eval_samples_per_second": 774.688,
      "eval_steps_per_second": 12.146,
      "step": 68000
    },
    {
      "epoch": 17.12345989439276,
      "grad_norm": 2.7814857959747314,
      "learning_rate": 4.91685534591195e-05,
      "loss": 2.6744,
      "step": 68100
    },
    {
      "epoch": 17.14860447573548,
      "grad_norm": 2.878861904144287,
      "learning_rate": 4.916729559748428e-05,
      "loss": 2.6589,
      "step": 68200
    },
    {
      "epoch": 17.1737490570782,
      "grad_norm": 2.980494976043701,
      "learning_rate": 4.9166037735849056e-05,
      "loss": 2.6599,
      "step": 68300
    },
    {
      "epoch": 17.19889363842092,
      "grad_norm": 3.0333011150360107,
      "learning_rate": 4.916477987421384e-05,
      "loss": 2.681,
      "step": 68400
    },
    {
      "epoch": 17.19889363842092,
      "eval_loss": 2.682097911834717,
      "eval_runtime": 17.3782,
      "eval_samples_per_second": 770.735,
      "eval_steps_per_second": 12.084,
      "step": 68400
    },
    {
      "epoch": 17.224038219763642,
      "grad_norm": 2.920267105102539,
      "learning_rate": 4.9163522012578613e-05,
      "loss": 2.6642,
      "step": 68500
    },
    {
      "epoch": 17.24918280110636,
      "grad_norm": 2.6599385738372803,
      "learning_rate": 4.9162264150943396e-05,
      "loss": 2.6854,
      "step": 68600
    },
    {
      "epoch": 17.274327382449084,
      "grad_norm": 2.5952186584472656,
      "learning_rate": 4.916100628930818e-05,
      "loss": 2.678,
      "step": 68700
    },
    {
      "epoch": 17.299471963791802,
      "grad_norm": 2.604602813720703,
      "learning_rate": 4.915974842767296e-05,
      "loss": 2.6617,
      "step": 68800
    },
    {
      "epoch": 17.299471963791802,
      "eval_loss": 2.6817708015441895,
      "eval_runtime": 17.2794,
      "eval_samples_per_second": 775.142,
      "eval_steps_per_second": 12.153,
      "step": 68800
    },
    {
      "epoch": 17.324616545134525,
      "grad_norm": 2.704176664352417,
      "learning_rate": 4.915849056603774e-05,
      "loss": 2.6896,
      "step": 68900
    },
    {
      "epoch": 17.349761126477244,
      "grad_norm": 2.8658225536346436,
      "learning_rate": 4.915723270440252e-05,
      "loss": 2.6631,
      "step": 69000
    },
    {
      "epoch": 17.374905707819966,
      "grad_norm": 2.9316930770874023,
      "learning_rate": 4.91559748427673e-05,
      "loss": 2.6737,
      "step": 69100
    },
    {
      "epoch": 17.400050289162685,
      "grad_norm": 2.6803877353668213,
      "learning_rate": 4.9154716981132075e-05,
      "loss": 2.6768,
      "step": 69200
    },
    {
      "epoch": 17.400050289162685,
      "eval_loss": 2.6811108589172363,
      "eval_runtime": 17.328,
      "eval_samples_per_second": 772.969,
      "eval_steps_per_second": 12.119,
      "step": 69200
    },
    {
      "epoch": 17.425194870505408,
      "grad_norm": 2.6167945861816406,
      "learning_rate": 4.915345911949686e-05,
      "loss": 2.6787,
      "step": 69300
    },
    {
      "epoch": 17.450339451848127,
      "grad_norm": 2.971527099609375,
      "learning_rate": 4.915220125786163e-05,
      "loss": 2.6883,
      "step": 69400
    },
    {
      "epoch": 17.47548403319085,
      "grad_norm": 2.8958165645599365,
      "learning_rate": 4.9150943396226415e-05,
      "loss": 2.6792,
      "step": 69500
    },
    {
      "epoch": 17.500628614533568,
      "grad_norm": 2.804250955581665,
      "learning_rate": 4.91496855345912e-05,
      "loss": 2.673,
      "step": 69600
    },
    {
      "epoch": 17.500628614533568,
      "eval_loss": 2.6809279918670654,
      "eval_runtime": 17.5381,
      "eval_samples_per_second": 763.71,
      "eval_steps_per_second": 11.974,
      "step": 69600
    },
    {
      "epoch": 17.52577319587629,
      "grad_norm": 2.7584052085876465,
      "learning_rate": 4.914842767295598e-05,
      "loss": 2.6893,
      "step": 69700
    },
    {
      "epoch": 17.55091777721901,
      "grad_norm": 2.680605888366699,
      "learning_rate": 4.914716981132076e-05,
      "loss": 2.6778,
      "step": 69800
    },
    {
      "epoch": 17.576062358561728,
      "grad_norm": 2.7942183017730713,
      "learning_rate": 4.914591194968554e-05,
      "loss": 2.6812,
      "step": 69900
    },
    {
      "epoch": 17.60120693990445,
      "grad_norm": 2.75494122505188,
      "learning_rate": 4.914465408805032e-05,
      "loss": 2.6772,
      "step": 70000
    },
    {
      "epoch": 17.60120693990445,
      "eval_loss": 2.678072214126587,
      "eval_runtime": 17.3949,
      "eval_samples_per_second": 769.994,
      "eval_steps_per_second": 12.072,
      "step": 70000
    },
    {
      "epoch": 17.62635152124717,
      "grad_norm": 2.6948018074035645,
      "learning_rate": 4.9143396226415095e-05,
      "loss": 2.6792,
      "step": 70100
    },
    {
      "epoch": 17.651496102589892,
      "grad_norm": 3.0617496967315674,
      "learning_rate": 4.914213836477988e-05,
      "loss": 2.6813,
      "step": 70200
    },
    {
      "epoch": 17.67664068393261,
      "grad_norm": 2.7594046592712402,
      "learning_rate": 4.914088050314465e-05,
      "loss": 2.6924,
      "step": 70300
    },
    {
      "epoch": 17.701785265275333,
      "grad_norm": 2.8414318561553955,
      "learning_rate": 4.9139622641509435e-05,
      "loss": 2.6932,
      "step": 70400
    },
    {
      "epoch": 17.701785265275333,
      "eval_loss": 2.6767804622650146,
      "eval_runtime": 17.3392,
      "eval_samples_per_second": 772.467,
      "eval_steps_per_second": 12.111,
      "step": 70400
    },
    {
      "epoch": 17.726929846618052,
      "grad_norm": 2.6478347778320312,
      "learning_rate": 4.913836477987422e-05,
      "loss": 2.6759,
      "step": 70500
    },
    {
      "epoch": 17.752074427960775,
      "grad_norm": 2.654031276702881,
      "learning_rate": 4.913710691823899e-05,
      "loss": 2.679,
      "step": 70600
    },
    {
      "epoch": 17.777219009303494,
      "grad_norm": 2.9163482189178467,
      "learning_rate": 4.9135849056603775e-05,
      "loss": 2.6843,
      "step": 70700
    },
    {
      "epoch": 17.802363590646216,
      "grad_norm": 2.724787950515747,
      "learning_rate": 4.913459119496856e-05,
      "loss": 2.6925,
      "step": 70800
    },
    {
      "epoch": 17.802363590646216,
      "eval_loss": 2.6771814823150635,
      "eval_runtime": 17.4215,
      "eval_samples_per_second": 768.82,
      "eval_steps_per_second": 12.054,
      "step": 70800
    },
    {
      "epoch": 17.827508171988935,
      "grad_norm": 2.6309144496917725,
      "learning_rate": 4.913333333333334e-05,
      "loss": 2.6862,
      "step": 70900
    },
    {
      "epoch": 17.852652753331657,
      "grad_norm": 2.845564603805542,
      "learning_rate": 4.9132075471698115e-05,
      "loss": 2.6859,
      "step": 71000
    },
    {
      "epoch": 17.877797334674376,
      "grad_norm": 2.8339531421661377,
      "learning_rate": 4.91308176100629e-05,
      "loss": 2.7036,
      "step": 71100
    },
    {
      "epoch": 17.9029419160171,
      "grad_norm": 2.5986547470092773,
      "learning_rate": 4.912955974842767e-05,
      "loss": 2.691,
      "step": 71200
    },
    {
      "epoch": 17.9029419160171,
      "eval_loss": 2.6738967895507812,
      "eval_runtime": 17.2533,
      "eval_samples_per_second": 776.315,
      "eval_steps_per_second": 12.172,
      "step": 71200
    },
    {
      "epoch": 17.928086497359818,
      "grad_norm": 2.6566827297210693,
      "learning_rate": 4.9128301886792455e-05,
      "loss": 2.6815,
      "step": 71300
    },
    {
      "epoch": 17.95323107870254,
      "grad_norm": 2.623412609100342,
      "learning_rate": 4.912704402515724e-05,
      "loss": 2.6763,
      "step": 71400
    },
    {
      "epoch": 17.97837566004526,
      "grad_norm": 2.5821385383605957,
      "learning_rate": 4.912578616352201e-05,
      "loss": 2.6822,
      "step": 71500
    },
    {
      "epoch": 18.00352024138798,
      "grad_norm": 2.7495946884155273,
      "learning_rate": 4.9124528301886795e-05,
      "loss": 2.6768,
      "step": 71600
    },
    {
      "epoch": 18.00352024138798,
      "eval_loss": 2.674964427947998,
      "eval_runtime": 17.2285,
      "eval_samples_per_second": 777.432,
      "eval_steps_per_second": 12.189,
      "step": 71600
    },
    {
      "epoch": 18.0286648227307,
      "grad_norm": 2.7087392807006836,
      "learning_rate": 4.912327044025157e-05,
      "loss": 2.6477,
      "step": 71700
    },
    {
      "epoch": 18.053809404073423,
      "grad_norm": 2.641181707382202,
      "learning_rate": 4.912201257861635e-05,
      "loss": 2.6492,
      "step": 71800
    },
    {
      "epoch": 18.078953985416142,
      "grad_norm": 2.8682358264923096,
      "learning_rate": 4.9120754716981135e-05,
      "loss": 2.6626,
      "step": 71900
    },
    {
      "epoch": 18.104098566758864,
      "grad_norm": 2.8109867572784424,
      "learning_rate": 4.911949685534592e-05,
      "loss": 2.6436,
      "step": 72000
    },
    {
      "epoch": 18.104098566758864,
      "eval_loss": 2.676281213760376,
      "eval_runtime": 17.3307,
      "eval_samples_per_second": 772.848,
      "eval_steps_per_second": 12.117,
      "step": 72000
    },
    {
      "epoch": 18.129243148101583,
      "grad_norm": 2.8990471363067627,
      "learning_rate": 4.91182389937107e-05,
      "loss": 2.6662,
      "step": 72100
    },
    {
      "epoch": 18.154387729444306,
      "grad_norm": 2.720104455947876,
      "learning_rate": 4.9116981132075475e-05,
      "loss": 2.6605,
      "step": 72200
    },
    {
      "epoch": 18.179532310787025,
      "grad_norm": 2.606503963470459,
      "learning_rate": 4.911572327044026e-05,
      "loss": 2.6746,
      "step": 72300
    },
    {
      "epoch": 18.204676892129747,
      "grad_norm": 2.685718059539795,
      "learning_rate": 4.911446540880503e-05,
      "loss": 2.6534,
      "step": 72400
    },
    {
      "epoch": 18.204676892129747,
      "eval_loss": 2.676621198654175,
      "eval_runtime": 17.2423,
      "eval_samples_per_second": 776.81,
      "eval_steps_per_second": 12.179,
      "step": 72400
    },
    {
      "epoch": 18.229821473472466,
      "grad_norm": 2.7456905841827393,
      "learning_rate": 4.9113207547169815e-05,
      "loss": 2.6563,
      "step": 72500
    },
    {
      "epoch": 18.25496605481519,
      "grad_norm": 2.808281421661377,
      "learning_rate": 4.911194968553459e-05,
      "loss": 2.6483,
      "step": 72600
    },
    {
      "epoch": 18.280110636157907,
      "grad_norm": 2.689302682876587,
      "learning_rate": 4.911069182389937e-05,
      "loss": 2.6673,
      "step": 72700
    },
    {
      "epoch": 18.30525521750063,
      "grad_norm": 2.689138412475586,
      "learning_rate": 4.910943396226415e-05,
      "loss": 2.6617,
      "step": 72800
    },
    {
      "epoch": 18.30525521750063,
      "eval_loss": 2.6767406463623047,
      "eval_runtime": 17.2324,
      "eval_samples_per_second": 777.255,
      "eval_steps_per_second": 12.186,
      "step": 72800
    },
    {
      "epoch": 18.33039979884335,
      "grad_norm": 3.0585696697235107,
      "learning_rate": 4.910817610062893e-05,
      "loss": 2.6687,
      "step": 72900
    },
    {
      "epoch": 18.35554438018607,
      "grad_norm": 2.636044979095459,
      "learning_rate": 4.910691823899371e-05,
      "loss": 2.653,
      "step": 73000
    },
    {
      "epoch": 18.38068896152879,
      "grad_norm": 2.687739372253418,
      "learning_rate": 4.9105660377358494e-05,
      "loss": 2.6784,
      "step": 73100
    },
    {
      "epoch": 18.405833542871513,
      "grad_norm": 2.7704098224639893,
      "learning_rate": 4.9104402515723277e-05,
      "loss": 2.6856,
      "step": 73200
    },
    {
      "epoch": 18.405833542871513,
      "eval_loss": 2.67507004737854,
      "eval_runtime": 17.1639,
      "eval_samples_per_second": 780.357,
      "eval_steps_per_second": 12.235,
      "step": 73200
    },
    {
      "epoch": 18.43097812421423,
      "grad_norm": 2.8401317596435547,
      "learning_rate": 4.910314465408805e-05,
      "loss": 2.6608,
      "step": 73300
    },
    {
      "epoch": 18.456122705556954,
      "grad_norm": 2.6302027702331543,
      "learning_rate": 4.9101886792452834e-05,
      "loss": 2.6686,
      "step": 73400
    },
    {
      "epoch": 18.481267286899673,
      "grad_norm": 2.724426746368408,
      "learning_rate": 4.910062893081761e-05,
      "loss": 2.6671,
      "step": 73500
    },
    {
      "epoch": 18.506411868242395,
      "grad_norm": 2.8855133056640625,
      "learning_rate": 4.909937106918239e-05,
      "loss": 2.6696,
      "step": 73600
    },
    {
      "epoch": 18.506411868242395,
      "eval_loss": 2.6739113330841064,
      "eval_runtime": 17.0728,
      "eval_samples_per_second": 784.522,
      "eval_steps_per_second": 12.3,
      "step": 73600
    },
    {
      "epoch": 18.531556449585114,
      "grad_norm": 3.0891122817993164,
      "learning_rate": 4.909811320754717e-05,
      "loss": 2.6672,
      "step": 73700
    },
    {
      "epoch": 18.556701030927837,
      "grad_norm": 2.682681083679199,
      "learning_rate": 4.909685534591195e-05,
      "loss": 2.6801,
      "step": 73800
    },
    {
      "epoch": 18.581845612270556,
      "grad_norm": 2.7431764602661133,
      "learning_rate": 4.909559748427673e-05,
      "loss": 2.6667,
      "step": 73900
    },
    {
      "epoch": 18.606990193613278,
      "grad_norm": 2.759903907775879,
      "learning_rate": 4.9094339622641514e-05,
      "loss": 2.6688,
      "step": 74000
    },
    {
      "epoch": 18.606990193613278,
      "eval_loss": 2.6716816425323486,
      "eval_runtime": 17.3544,
      "eval_samples_per_second": 771.791,
      "eval_steps_per_second": 12.101,
      "step": 74000
    },
    {
      "epoch": 18.632134774955997,
      "grad_norm": 2.931920289993286,
      "learning_rate": 4.9093081761006296e-05,
      "loss": 2.6694,
      "step": 74100
    },
    {
      "epoch": 18.657279356298716,
      "grad_norm": 2.804086923599243,
      "learning_rate": 4.909182389937107e-05,
      "loss": 2.667,
      "step": 74200
    },
    {
      "epoch": 18.68242393764144,
      "grad_norm": 2.718914270401001,
      "learning_rate": 4.9090566037735854e-05,
      "loss": 2.6806,
      "step": 74300
    },
    {
      "epoch": 18.70756851898416,
      "grad_norm": 2.6215980052948,
      "learning_rate": 4.908930817610063e-05,
      "loss": 2.6655,
      "step": 74400
    },
    {
      "epoch": 18.70756851898416,
      "eval_loss": 2.6721513271331787,
      "eval_runtime": 17.023,
      "eval_samples_per_second": 786.82,
      "eval_steps_per_second": 12.336,
      "step": 74400
    },
    {
      "epoch": 18.73271310032688,
      "grad_norm": 2.757657766342163,
      "learning_rate": 4.908805031446541e-05,
      "loss": 2.668,
      "step": 74500
    },
    {
      "epoch": 18.7578576816696,
      "grad_norm": 2.6705188751220703,
      "learning_rate": 4.9086792452830194e-05,
      "loss": 2.6768,
      "step": 74600
    },
    {
      "epoch": 18.78300226301232,
      "grad_norm": 2.665872573852539,
      "learning_rate": 4.908553459119497e-05,
      "loss": 2.6785,
      "step": 74700
    },
    {
      "epoch": 18.80814684435504,
      "grad_norm": 2.814082384109497,
      "learning_rate": 4.908427672955975e-05,
      "loss": 2.6743,
      "step": 74800
    },
    {
      "epoch": 18.80814684435504,
      "eval_loss": 2.671231508255005,
      "eval_runtime": 17.1135,
      "eval_samples_per_second": 782.657,
      "eval_steps_per_second": 12.271,
      "step": 74800
    },
    {
      "epoch": 18.833291425697762,
      "grad_norm": 2.65059232711792,
      "learning_rate": 4.908301886792453e-05,
      "loss": 2.657,
      "step": 74900
    },
    {
      "epoch": 18.85843600704048,
      "grad_norm": 2.918865203857422,
      "learning_rate": 4.908176100628931e-05,
      "loss": 2.6757,
      "step": 75000
    },
    {
      "epoch": 18.883580588383204,
      "grad_norm": 2.6576955318450928,
      "learning_rate": 4.908050314465409e-05,
      "loss": 2.6723,
      "step": 75100
    },
    {
      "epoch": 18.908725169725923,
      "grad_norm": 2.703779458999634,
      "learning_rate": 4.9079245283018874e-05,
      "loss": 2.6822,
      "step": 75200
    },
    {
      "epoch": 18.908725169725923,
      "eval_loss": 2.6683340072631836,
      "eval_runtime": 17.5027,
      "eval_samples_per_second": 765.252,
      "eval_steps_per_second": 11.998,
      "step": 75200
    },
    {
      "epoch": 18.933869751068645,
      "grad_norm": 2.7019124031066895,
      "learning_rate": 4.907798742138365e-05,
      "loss": 2.6743,
      "step": 75300
    },
    {
      "epoch": 18.959014332411364,
      "grad_norm": 2.572330951690674,
      "learning_rate": 4.907672955974843e-05,
      "loss": 2.6668,
      "step": 75400
    },
    {
      "epoch": 18.984158913754086,
      "grad_norm": 2.6742992401123047,
      "learning_rate": 4.9075471698113214e-05,
      "loss": 2.672,
      "step": 75500
    },
    {
      "epoch": 19.009303495096805,
      "grad_norm": 2.5398457050323486,
      "learning_rate": 4.907421383647799e-05,
      "loss": 2.6636,
      "step": 75600
    },
    {
      "epoch": 19.009303495096805,
      "eval_loss": 2.668139696121216,
      "eval_runtime": 17.2596,
      "eval_samples_per_second": 776.033,
      "eval_steps_per_second": 12.167,
      "step": 75600
    },
    {
      "epoch": 19.034448076439528,
      "grad_norm": 2.489924430847168,
      "learning_rate": 4.907295597484277e-05,
      "loss": 2.631,
      "step": 75700
    },
    {
      "epoch": 19.059592657782247,
      "grad_norm": 2.6256942749023438,
      "learning_rate": 4.907169811320755e-05,
      "loss": 2.657,
      "step": 75800
    },
    {
      "epoch": 19.08473723912497,
      "grad_norm": 2.644335985183716,
      "learning_rate": 4.907044025157233e-05,
      "loss": 2.6378,
      "step": 75900
    },
    {
      "epoch": 19.109881820467688,
      "grad_norm": 2.704618453979492,
      "learning_rate": 4.9069182389937105e-05,
      "loss": 2.6583,
      "step": 76000
    },
    {
      "epoch": 19.109881820467688,
      "eval_loss": 2.66953182220459,
      "eval_runtime": 17.196,
      "eval_samples_per_second": 778.902,
      "eval_steps_per_second": 12.212,
      "step": 76000
    },
    {
      "epoch": 19.13502640181041,
      "grad_norm": 2.800539255142212,
      "learning_rate": 4.906792452830189e-05,
      "loss": 2.6501,
      "step": 76100
    },
    {
      "epoch": 19.16017098315313,
      "grad_norm": 2.7589027881622314,
      "learning_rate": 4.906666666666667e-05,
      "loss": 2.6483,
      "step": 76200
    },
    {
      "epoch": 19.185315564495852,
      "grad_norm": 2.6092329025268555,
      "learning_rate": 4.906540880503145e-05,
      "loss": 2.6402,
      "step": 76300
    },
    {
      "epoch": 19.21046014583857,
      "grad_norm": 2.7848868370056152,
      "learning_rate": 4.9064150943396233e-05,
      "loss": 2.634,
      "step": 76400
    },
    {
      "epoch": 19.21046014583857,
      "eval_loss": 2.6690094470977783,
      "eval_runtime": 17.2922,
      "eval_samples_per_second": 774.569,
      "eval_steps_per_second": 12.144,
      "step": 76400
    },
    {
      "epoch": 19.235604727181293,
      "grad_norm": 2.6855406761169434,
      "learning_rate": 4.906289308176101e-05,
      "loss": 2.6495,
      "step": 76500
    },
    {
      "epoch": 19.260749308524012,
      "grad_norm": 2.979768991470337,
      "learning_rate": 4.906163522012579e-05,
      "loss": 2.6541,
      "step": 76600
    },
    {
      "epoch": 19.285893889866735,
      "grad_norm": 2.8487656116485596,
      "learning_rate": 4.9060377358490567e-05,
      "loss": 2.6511,
      "step": 76700
    },
    {
      "epoch": 19.311038471209454,
      "grad_norm": 2.762650728225708,
      "learning_rate": 4.905911949685535e-05,
      "loss": 2.6499,
      "step": 76800
    },
    {
      "epoch": 19.311038471209454,
      "eval_loss": 2.672008514404297,
      "eval_runtime": 17.1025,
      "eval_samples_per_second": 783.163,
      "eval_steps_per_second": 12.279,
      "step": 76800
    },
    {
      "epoch": 19.336183052552176,
      "grad_norm": 2.6133251190185547,
      "learning_rate": 4.9057861635220124e-05,
      "loss": 2.6605,
      "step": 76900
    },
    {
      "epoch": 19.361327633894895,
      "grad_norm": 2.623727560043335,
      "learning_rate": 4.9056603773584906e-05,
      "loss": 2.6499,
      "step": 77000
    },
    {
      "epoch": 19.386472215237617,
      "grad_norm": 2.819002389907837,
      "learning_rate": 4.905534591194969e-05,
      "loss": 2.6519,
      "step": 77100
    },
    {
      "epoch": 19.411616796580336,
      "grad_norm": 2.4148120880126953,
      "learning_rate": 4.9054088050314464e-05,
      "loss": 2.6565,
      "step": 77200
    },
    {
      "epoch": 19.411616796580336,
      "eval_loss": 2.6711227893829346,
      "eval_runtime": 17.2786,
      "eval_samples_per_second": 775.18,
      "eval_steps_per_second": 12.154,
      "step": 77200
    },
    {
      "epoch": 19.43676137792306,
      "grad_norm": 2.6963565349578857,
      "learning_rate": 4.9052830188679246e-05,
      "loss": 2.6563,
      "step": 77300
    },
    {
      "epoch": 19.461905959265778,
      "grad_norm": 2.4900829792022705,
      "learning_rate": 4.905157232704403e-05,
      "loss": 2.6443,
      "step": 77400
    },
    {
      "epoch": 19.4870505406085,
      "grad_norm": 2.831148862838745,
      "learning_rate": 4.905031446540881e-05,
      "loss": 2.6527,
      "step": 77500
    },
    {
      "epoch": 19.51219512195122,
      "grad_norm": 2.7024879455566406,
      "learning_rate": 4.9049056603773586e-05,
      "loss": 2.664,
      "step": 77600
    },
    {
      "epoch": 19.51219512195122,
      "eval_loss": 2.6681861877441406,
      "eval_runtime": 17.1464,
      "eval_samples_per_second": 781.155,
      "eval_steps_per_second": 12.247,
      "step": 77600
    },
    {
      "epoch": 19.53733970329394,
      "grad_norm": 2.645200490951538,
      "learning_rate": 4.904779874213837e-05,
      "loss": 2.6551,
      "step": 77700
    },
    {
      "epoch": 19.56248428463666,
      "grad_norm": 2.62261962890625,
      "learning_rate": 4.9046540880503144e-05,
      "loss": 2.6555,
      "step": 77800
    },
    {
      "epoch": 19.587628865979383,
      "grad_norm": 2.639523983001709,
      "learning_rate": 4.9045283018867926e-05,
      "loss": 2.6707,
      "step": 77900
    },
    {
      "epoch": 19.612773447322102,
      "grad_norm": 2.8502771854400635,
      "learning_rate": 4.904402515723271e-05,
      "loss": 2.6487,
      "step": 78000
    },
    {
      "epoch": 19.612773447322102,
      "eval_loss": 2.6658873558044434,
      "eval_runtime": 17.201,
      "eval_samples_per_second": 778.677,
      "eval_steps_per_second": 12.209,
      "step": 78000
    },
    {
      "epoch": 19.637918028664824,
      "grad_norm": 2.867466449737549,
      "learning_rate": 4.9042767295597484e-05,
      "loss": 2.6637,
      "step": 78100
    },
    {
      "epoch": 19.663062610007543,
      "grad_norm": 2.7745883464813232,
      "learning_rate": 4.9041509433962266e-05,
      "loss": 2.6677,
      "step": 78200
    },
    {
      "epoch": 19.688207191350266,
      "grad_norm": 2.837749481201172,
      "learning_rate": 4.904025157232705e-05,
      "loss": 2.6513,
      "step": 78300
    },
    {
      "epoch": 19.713351772692985,
      "grad_norm": 2.5289957523345947,
      "learning_rate": 4.903899371069183e-05,
      "loss": 2.6766,
      "step": 78400
    },
    {
      "epoch": 19.713351772692985,
      "eval_loss": 2.6665468215942383,
      "eval_runtime": 17.2107,
      "eval_samples_per_second": 778.238,
      "eval_steps_per_second": 12.202,
      "step": 78400
    },
    {
      "epoch": 19.738496354035707,
      "grad_norm": 2.683802843093872,
      "learning_rate": 4.9037735849056606e-05,
      "loss": 2.653,
      "step": 78500
    },
    {
      "epoch": 19.763640935378426,
      "grad_norm": 2.653308868408203,
      "learning_rate": 4.903647798742139e-05,
      "loss": 2.6616,
      "step": 78600
    },
    {
      "epoch": 19.78878551672115,
      "grad_norm": 2.4670779705047607,
      "learning_rate": 4.9035220125786164e-05,
      "loss": 2.6667,
      "step": 78700
    },
    {
      "epoch": 19.813930098063867,
      "grad_norm": 2.6282296180725098,
      "learning_rate": 4.9033962264150946e-05,
      "loss": 2.6519,
      "step": 78800
    },
    {
      "epoch": 19.813930098063867,
      "eval_loss": 2.6649298667907715,
      "eval_runtime": 17.2039,
      "eval_samples_per_second": 778.546,
      "eval_steps_per_second": 12.207,
      "step": 78800
    },
    {
      "epoch": 19.839074679406586,
      "grad_norm": 2.6358718872070312,
      "learning_rate": 4.903270440251573e-05,
      "loss": 2.6637,
      "step": 78900
    },
    {
      "epoch": 19.86421926074931,
      "grad_norm": 2.7194511890411377,
      "learning_rate": 4.9031446540880504e-05,
      "loss": 2.6791,
      "step": 79000
    },
    {
      "epoch": 19.889363842092028,
      "grad_norm": 2.6550700664520264,
      "learning_rate": 4.9030188679245286e-05,
      "loss": 2.6792,
      "step": 79100
    },
    {
      "epoch": 19.91450842343475,
      "grad_norm": 2.7904629707336426,
      "learning_rate": 4.902893081761006e-05,
      "loss": 2.6669,
      "step": 79200
    },
    {
      "epoch": 19.91450842343475,
      "eval_loss": 2.6625406742095947,
      "eval_runtime": 17.152,
      "eval_samples_per_second": 780.9,
      "eval_steps_per_second": 12.243,
      "step": 79200
    },
    {
      "epoch": 19.93965300477747,
      "grad_norm": 2.8914740085601807,
      "learning_rate": 4.9027672955974844e-05,
      "loss": 2.6549,
      "step": 79300
    },
    {
      "epoch": 19.96479758612019,
      "grad_norm": 2.7533977031707764,
      "learning_rate": 4.9026415094339626e-05,
      "loss": 2.6528,
      "step": 79400
    },
    {
      "epoch": 19.98994216746291,
      "grad_norm": 2.5102663040161133,
      "learning_rate": 4.902515723270441e-05,
      "loss": 2.6515,
      "step": 79500
    },
    {
      "epoch": 20.015086748805633,
      "grad_norm": 2.613740921020508,
      "learning_rate": 4.902389937106919e-05,
      "loss": 2.639,
      "step": 79600
    },
    {
      "epoch": 20.015086748805633,
      "eval_loss": 2.662975311279297,
      "eval_runtime": 17.3473,
      "eval_samples_per_second": 772.107,
      "eval_steps_per_second": 12.106,
      "step": 79600
    },
    {
      "epoch": 20.04023133014835,
      "grad_norm": 2.572580099105835,
      "learning_rate": 4.9022641509433966e-05,
      "loss": 2.6297,
      "step": 79700
    },
    {
      "epoch": 20.065375911491074,
      "grad_norm": 2.713728427886963,
      "learning_rate": 4.902138364779875e-05,
      "loss": 2.6261,
      "step": 79800
    },
    {
      "epoch": 20.090520492833793,
      "grad_norm": 2.7119693756103516,
      "learning_rate": 4.9020125786163523e-05,
      "loss": 2.6191,
      "step": 79900
    },
    {
      "epoch": 20.115665074176516,
      "grad_norm": 2.667121648788452,
      "learning_rate": 4.9018867924528306e-05,
      "loss": 2.6403,
      "step": 80000
    },
    {
      "epoch": 20.115665074176516,
      "eval_loss": 2.6637277603149414,
      "eval_runtime": 17.1089,
      "eval_samples_per_second": 782.867,
      "eval_steps_per_second": 12.274,
      "step": 80000
    },
    {
      "epoch": 20.140809655519234,
      "grad_norm": 2.765291690826416,
      "learning_rate": 4.901761006289308e-05,
      "loss": 2.6271,
      "step": 80100
    },
    {
      "epoch": 20.165954236861957,
      "grad_norm": 2.90777325630188,
      "learning_rate": 4.901635220125786e-05,
      "loss": 2.6467,
      "step": 80200
    },
    {
      "epoch": 20.191098818204676,
      "grad_norm": 2.757455587387085,
      "learning_rate": 4.901509433962264e-05,
      "loss": 2.6509,
      "step": 80300
    },
    {
      "epoch": 20.2162433995474,
      "grad_norm": 2.8603432178497314,
      "learning_rate": 4.901383647798742e-05,
      "loss": 2.6404,
      "step": 80400
    },
    {
      "epoch": 20.2162433995474,
      "eval_loss": 2.6633894443511963,
      "eval_runtime": 17.1072,
      "eval_samples_per_second": 782.945,
      "eval_steps_per_second": 12.276,
      "step": 80400
    },
    {
      "epoch": 20.241387980890117,
      "grad_norm": 3.0393805503845215,
      "learning_rate": 4.90125786163522e-05,
      "loss": 2.6403,
      "step": 80500
    },
    {
      "epoch": 20.26653256223284,
      "grad_norm": 2.632340669631958,
      "learning_rate": 4.9011320754716985e-05,
      "loss": 2.64,
      "step": 80600
    },
    {
      "epoch": 20.29167714357556,
      "grad_norm": 2.611675977706909,
      "learning_rate": 4.901006289308177e-05,
      "loss": 2.6455,
      "step": 80700
    },
    {
      "epoch": 20.31682172491828,
      "grad_norm": 2.707791328430176,
      "learning_rate": 4.900880503144654e-05,
      "loss": 2.6328,
      "step": 80800
    },
    {
      "epoch": 20.31682172491828,
      "eval_loss": 2.6650960445404053,
      "eval_runtime": 17.2543,
      "eval_samples_per_second": 776.271,
      "eval_steps_per_second": 12.171,
      "step": 80800
    },
    {
      "epoch": 20.341966306261,
      "grad_norm": 2.6800413131713867,
      "learning_rate": 4.9007547169811325e-05,
      "loss": 2.6555,
      "step": 80900
    },
    {
      "epoch": 20.367110887603722,
      "grad_norm": 2.7531700134277344,
      "learning_rate": 4.90062893081761e-05,
      "loss": 2.6279,
      "step": 81000
    },
    {
      "epoch": 20.39225546894644,
      "grad_norm": 2.7835638523101807,
      "learning_rate": 4.900503144654088e-05,
      "loss": 2.6445,
      "step": 81100
    },
    {
      "epoch": 20.417400050289164,
      "grad_norm": 2.7055985927581787,
      "learning_rate": 4.900377358490566e-05,
      "loss": 2.6566,
      "step": 81200
    },
    {
      "epoch": 20.417400050289164,
      "eval_loss": 2.6620850563049316,
      "eval_runtime": 17.2927,
      "eval_samples_per_second": 774.547,
      "eval_steps_per_second": 12.144,
      "step": 81200
    },
    {
      "epoch": 20.442544631631883,
      "grad_norm": 2.8845767974853516,
      "learning_rate": 4.900251572327044e-05,
      "loss": 2.6459,
      "step": 81300
    },
    {
      "epoch": 20.467689212974605,
      "grad_norm": 2.4784839153289795,
      "learning_rate": 4.900125786163522e-05,
      "loss": 2.6439,
      "step": 81400
    },
    {
      "epoch": 20.492833794317324,
      "grad_norm": 2.842369556427002,
      "learning_rate": 4.9e-05,
      "loss": 2.6398,
      "step": 81500
    },
    {
      "epoch": 20.517978375660046,
      "grad_norm": 2.568988800048828,
      "learning_rate": 4.899874213836478e-05,
      "loss": 2.6486,
      "step": 81600
    },
    {
      "epoch": 20.517978375660046,
      "eval_loss": 2.6631016731262207,
      "eval_runtime": 17.2218,
      "eval_samples_per_second": 777.734,
      "eval_steps_per_second": 12.194,
      "step": 81600
    },
    {
      "epoch": 20.543122957002765,
      "grad_norm": 2.5681138038635254,
      "learning_rate": 4.899748427672956e-05,
      "loss": 2.6421,
      "step": 81700
    },
    {
      "epoch": 20.568267538345488,
      "grad_norm": 2.7522284984588623,
      "learning_rate": 4.8996226415094345e-05,
      "loss": 2.6569,
      "step": 81800
    },
    {
      "epoch": 20.593412119688207,
      "grad_norm": 2.8185553550720215,
      "learning_rate": 4.899496855345912e-05,
      "loss": 2.6361,
      "step": 81900
    },
    {
      "epoch": 20.61855670103093,
      "grad_norm": 2.9378538131713867,
      "learning_rate": 4.89937106918239e-05,
      "loss": 2.6638,
      "step": 82000
    },
    {
      "epoch": 20.61855670103093,
      "eval_loss": 2.65887713432312,
      "eval_runtime": 17.0908,
      "eval_samples_per_second": 783.695,
      "eval_steps_per_second": 12.287,
      "step": 82000
    },
    {
      "epoch": 20.643701282373648,
      "grad_norm": 2.7621543407440186,
      "learning_rate": 4.8992452830188685e-05,
      "loss": 2.6477,
      "step": 82100
    },
    {
      "epoch": 20.66884586371637,
      "grad_norm": 2.779876470565796,
      "learning_rate": 4.899119496855346e-05,
      "loss": 2.6523,
      "step": 82200
    },
    {
      "epoch": 20.69399044505909,
      "grad_norm": 2.4682869911193848,
      "learning_rate": 4.898993710691824e-05,
      "loss": 2.6433,
      "step": 82300
    },
    {
      "epoch": 20.719135026401812,
      "grad_norm": 2.851715087890625,
      "learning_rate": 4.898867924528302e-05,
      "loss": 2.6557,
      "step": 82400
    },
    {
      "epoch": 20.719135026401812,
      "eval_loss": 2.6593587398529053,
      "eval_runtime": 17.1315,
      "eval_samples_per_second": 781.834,
      "eval_steps_per_second": 12.258,
      "step": 82400
    },
    {
      "epoch": 20.74427960774453,
      "grad_norm": 2.617713689804077,
      "learning_rate": 4.89874213836478e-05,
      "loss": 2.6479,
      "step": 82500
    },
    {
      "epoch": 20.769424189087253,
      "grad_norm": 2.883336067199707,
      "learning_rate": 4.8986163522012576e-05,
      "loss": 2.6474,
      "step": 82600
    },
    {
      "epoch": 20.794568770429972,
      "grad_norm": 2.6985104084014893,
      "learning_rate": 4.8984905660377365e-05,
      "loss": 2.6428,
      "step": 82700
    },
    {
      "epoch": 20.819713351772695,
      "grad_norm": 2.630919933319092,
      "learning_rate": 4.898364779874214e-05,
      "loss": 2.6476,
      "step": 82800
    },
    {
      "epoch": 20.819713351772695,
      "eval_loss": 2.658478021621704,
      "eval_runtime": 17.1327,
      "eval_samples_per_second": 781.779,
      "eval_steps_per_second": 12.257,
      "step": 82800
    },
    {
      "epoch": 20.844857933115414,
      "grad_norm": 2.6130459308624268,
      "learning_rate": 4.898238993710692e-05,
      "loss": 2.6522,
      "step": 82900
    },
    {
      "epoch": 20.870002514458136,
      "grad_norm": 2.7175347805023193,
      "learning_rate": 4.8981132075471705e-05,
      "loss": 2.6523,
      "step": 83000
    },
    {
      "epoch": 20.895147095800855,
      "grad_norm": 2.9448046684265137,
      "learning_rate": 4.897987421383648e-05,
      "loss": 2.6542,
      "step": 83100
    },
    {
      "epoch": 20.920291677143574,
      "grad_norm": 2.589920997619629,
      "learning_rate": 4.897861635220126e-05,
      "loss": 2.6523,
      "step": 83200
    },
    {
      "epoch": 20.920291677143574,
      "eval_loss": 2.658694267272949,
      "eval_runtime": 17.1078,
      "eval_samples_per_second": 782.917,
      "eval_steps_per_second": 12.275,
      "step": 83200
    },
    {
      "epoch": 20.945436258486296,
      "grad_norm": 2.724515914916992,
      "learning_rate": 4.897735849056604e-05,
      "loss": 2.6544,
      "step": 83300
    },
    {
      "epoch": 20.970580839829015,
      "grad_norm": 2.5801565647125244,
      "learning_rate": 4.897610062893082e-05,
      "loss": 2.6627,
      "step": 83400
    },
    {
      "epoch": 20.995725421171738,
      "grad_norm": 2.3935022354125977,
      "learning_rate": 4.8974842767295596e-05,
      "loss": 2.6533,
      "step": 83500
    },
    {
      "epoch": 21.020870002514457,
      "grad_norm": 2.575824499130249,
      "learning_rate": 4.897358490566038e-05,
      "loss": 2.622,
      "step": 83600
    },
    {
      "epoch": 21.020870002514457,
      "eval_loss": 2.6566827297210693,
      "eval_runtime": 17.1116,
      "eval_samples_per_second": 782.743,
      "eval_steps_per_second": 12.272,
      "step": 83600
    },
    {
      "epoch": 21.04601458385718,
      "grad_norm": 2.497368812561035,
      "learning_rate": 4.897232704402516e-05,
      "loss": 2.6155,
      "step": 83700
    },
    {
      "epoch": 21.071159165199898,
      "grad_norm": 2.9649527072906494,
      "learning_rate": 4.897106918238994e-05,
      "loss": 2.6172,
      "step": 83800
    },
    {
      "epoch": 21.09630374654262,
      "grad_norm": 2.628817081451416,
      "learning_rate": 4.8969811320754725e-05,
      "loss": 2.6115,
      "step": 83900
    },
    {
      "epoch": 21.12144832788534,
      "grad_norm": 2.7474372386932373,
      "learning_rate": 4.89685534591195e-05,
      "loss": 2.638,
      "step": 84000
    },
    {
      "epoch": 21.12144832788534,
      "eval_loss": 2.657578706741333,
      "eval_runtime": 17.226,
      "eval_samples_per_second": 777.546,
      "eval_steps_per_second": 12.191,
      "step": 84000
    },
    {
      "epoch": 21.146592909228062,
      "grad_norm": 2.705566883087158,
      "learning_rate": 4.896729559748428e-05,
      "loss": 2.6195,
      "step": 84100
    },
    {
      "epoch": 21.17173749057078,
      "grad_norm": 2.964998960494995,
      "learning_rate": 4.896603773584906e-05,
      "loss": 2.6375,
      "step": 84200
    },
    {
      "epoch": 21.196882071913503,
      "grad_norm": 2.529991388320923,
      "learning_rate": 4.896477987421384e-05,
      "loss": 2.632,
      "step": 84300
    },
    {
      "epoch": 21.222026653256222,
      "grad_norm": 2.830880641937256,
      "learning_rate": 4.8963522012578615e-05,
      "loss": 2.6419,
      "step": 84400
    },
    {
      "epoch": 21.222026653256222,
      "eval_loss": 2.658503293991089,
      "eval_runtime": 17.1627,
      "eval_samples_per_second": 780.414,
      "eval_steps_per_second": 12.236,
      "step": 84400
    },
    {
      "epoch": 21.247171234598945,
      "grad_norm": 2.775334119796753,
      "learning_rate": 4.89622641509434e-05,
      "loss": 2.6231,
      "step": 84500
    },
    {
      "epoch": 21.272315815941663,
      "grad_norm": 2.6506593227386475,
      "learning_rate": 4.896100628930818e-05,
      "loss": 2.6264,
      "step": 84600
    },
    {
      "epoch": 21.297460397284386,
      "grad_norm": 2.749399185180664,
      "learning_rate": 4.8959748427672955e-05,
      "loss": 2.6287,
      "step": 84700
    },
    {
      "epoch": 21.322604978627105,
      "grad_norm": 2.6412320137023926,
      "learning_rate": 4.895849056603774e-05,
      "loss": 2.6283,
      "step": 84800
    },
    {
      "epoch": 21.322604978627105,
      "eval_loss": 2.657306432723999,
      "eval_runtime": 17.2382,
      "eval_samples_per_second": 776.997,
      "eval_steps_per_second": 12.182,
      "step": 84800
    },
    {
      "epoch": 21.347749559969827,
      "grad_norm": 2.600748300552368,
      "learning_rate": 4.895723270440252e-05,
      "loss": 2.6312,
      "step": 84900
    },
    {
      "epoch": 21.372894141312546,
      "grad_norm": 2.6419384479522705,
      "learning_rate": 4.89559748427673e-05,
      "loss": 2.641,
      "step": 85000
    },
    {
      "epoch": 21.39803872265527,
      "grad_norm": 2.6934921741485596,
      "learning_rate": 4.895471698113208e-05,
      "loss": 2.6439,
      "step": 85100
    },
    {
      "epoch": 21.423183303997988,
      "grad_norm": 2.682079315185547,
      "learning_rate": 4.895345911949686e-05,
      "loss": 2.6324,
      "step": 85200
    },
    {
      "epoch": 21.423183303997988,
      "eval_loss": 2.654423236846924,
      "eval_runtime": 17.112,
      "eval_samples_per_second": 782.724,
      "eval_steps_per_second": 12.272,
      "step": 85200
    },
    {
      "epoch": 21.44832788534071,
      "grad_norm": 2.6388604640960693,
      "learning_rate": 4.8952201257861635e-05,
      "loss": 2.6389,
      "step": 85300
    },
    {
      "epoch": 21.47347246668343,
      "grad_norm": 2.6433167457580566,
      "learning_rate": 4.895094339622642e-05,
      "loss": 2.6378,
      "step": 85400
    },
    {
      "epoch": 21.49861704802615,
      "grad_norm": 2.8340671062469482,
      "learning_rate": 4.89496855345912e-05,
      "loss": 2.6381,
      "step": 85500
    },
    {
      "epoch": 21.52376162936887,
      "grad_norm": 2.4933881759643555,
      "learning_rate": 4.8948427672955975e-05,
      "loss": 2.6388,
      "step": 85600
    },
    {
      "epoch": 21.52376162936887,
      "eval_loss": 2.656078815460205,
      "eval_runtime": 17.2316,
      "eval_samples_per_second": 777.295,
      "eval_steps_per_second": 12.187,
      "step": 85600
    },
    {
      "epoch": 21.548906210711593,
      "grad_norm": 2.7052717208862305,
      "learning_rate": 4.894716981132076e-05,
      "loss": 2.6393,
      "step": 85700
    },
    {
      "epoch": 21.57405079205431,
      "grad_norm": 2.462263345718384,
      "learning_rate": 4.894591194968553e-05,
      "loss": 2.636,
      "step": 85800
    },
    {
      "epoch": 21.599195373397034,
      "grad_norm": 2.419691324234009,
      "learning_rate": 4.8944654088050315e-05,
      "loss": 2.6483,
      "step": 85900
    },
    {
      "epoch": 21.624339954739753,
      "grad_norm": 2.6727378368377686,
      "learning_rate": 4.89433962264151e-05,
      "loss": 2.6279,
      "step": 86000
    },
    {
      "epoch": 21.624339954739753,
      "eval_loss": 2.654782772064209,
      "eval_runtime": 17.1155,
      "eval_samples_per_second": 782.566,
      "eval_steps_per_second": 12.27,
      "step": 86000
    },
    {
      "epoch": 21.649484536082475,
      "grad_norm": 2.581949234008789,
      "learning_rate": 4.894213836477988e-05,
      "loss": 2.6365,
      "step": 86100
    },
    {
      "epoch": 21.674629117425194,
      "grad_norm": 2.7510528564453125,
      "learning_rate": 4.8940880503144655e-05,
      "loss": 2.6346,
      "step": 86200
    },
    {
      "epoch": 21.699773698767917,
      "grad_norm": 2.6973319053649902,
      "learning_rate": 4.893962264150944e-05,
      "loss": 2.6265,
      "step": 86300
    },
    {
      "epoch": 21.724918280110636,
      "grad_norm": 2.731496572494507,
      "learning_rate": 4.893836477987422e-05,
      "loss": 2.6338,
      "step": 86400
    },
    {
      "epoch": 21.724918280110636,
      "eval_loss": 2.6559691429138184,
      "eval_runtime": 17.0722,
      "eval_samples_per_second": 784.549,
      "eval_steps_per_second": 12.301,
      "step": 86400
    },
    {
      "epoch": 21.750062861453358,
      "grad_norm": 2.8161466121673584,
      "learning_rate": 4.8937106918238995e-05,
      "loss": 2.6515,
      "step": 86500
    },
    {
      "epoch": 21.775207442796077,
      "grad_norm": 2.6221768856048584,
      "learning_rate": 4.893584905660378e-05,
      "loss": 2.6375,
      "step": 86600
    },
    {
      "epoch": 21.8003520241388,
      "grad_norm": 2.683839797973633,
      "learning_rate": 4.893459119496855e-05,
      "loss": 2.6479,
      "step": 86700
    },
    {
      "epoch": 21.82549660548152,
      "grad_norm": 2.5633063316345215,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 2.6388,
      "step": 86800
    },
    {
      "epoch": 21.82549660548152,
      "eval_loss": 2.6533336639404297,
      "eval_runtime": 17.2552,
      "eval_samples_per_second": 776.231,
      "eval_steps_per_second": 12.17,
      "step": 86800
    },
    {
      "epoch": 21.85064118682424,
      "grad_norm": 2.628037691116333,
      "learning_rate": 4.893207547169811e-05,
      "loss": 2.6437,
      "step": 86900
    },
    {
      "epoch": 21.87578576816696,
      "grad_norm": 2.724202871322632,
      "learning_rate": 4.893081761006289e-05,
      "loss": 2.6366,
      "step": 87000
    },
    {
      "epoch": 21.900930349509682,
      "grad_norm": 2.712219476699829,
      "learning_rate": 4.892955974842768e-05,
      "loss": 2.6353,
      "step": 87100
    },
    {
      "epoch": 21.9260749308524,
      "grad_norm": 2.5912961959838867,
      "learning_rate": 4.892830188679246e-05,
      "loss": 2.6421,
      "step": 87200
    },
    {
      "epoch": 21.9260749308524,
      "eval_loss": 2.6526975631713867,
      "eval_runtime": 17.3675,
      "eval_samples_per_second": 771.211,
      "eval_steps_per_second": 12.092,
      "step": 87200
    },
    {
      "epoch": 21.951219512195124,
      "grad_norm": 2.6253480911254883,
      "learning_rate": 4.892704402515724e-05,
      "loss": 2.6377,
      "step": 87300
    },
    {
      "epoch": 21.976364093537843,
      "grad_norm": 2.723362684249878,
      "learning_rate": 4.8925786163522014e-05,
      "loss": 2.6389,
      "step": 87400
    },
    {
      "epoch": 22.00150867488056,
      "grad_norm": 2.713003396987915,
      "learning_rate": 4.89245283018868e-05,
      "loss": 2.6534,
      "step": 87500
    },
    {
      "epoch": 22.026653256223284,
      "grad_norm": 2.746997594833374,
      "learning_rate": 4.892327044025157e-05,
      "loss": 2.6106,
      "step": 87600
    },
    {
      "epoch": 22.026653256223284,
      "eval_loss": 2.6549644470214844,
      "eval_runtime": 17.1262,
      "eval_samples_per_second": 782.077,
      "eval_steps_per_second": 12.262,
      "step": 87600
    },
    {
      "epoch": 22.051797837566003,
      "grad_norm": 2.583796739578247,
      "learning_rate": 4.8922012578616354e-05,
      "loss": 2.6161,
      "step": 87700
    },
    {
      "epoch": 22.076942418908725,
      "grad_norm": 2.6566615104675293,
      "learning_rate": 4.892075471698113e-05,
      "loss": 2.6079,
      "step": 87800
    },
    {
      "epoch": 22.102087000251444,
      "grad_norm": 2.570685625076294,
      "learning_rate": 4.891949685534591e-05,
      "loss": 2.6151,
      "step": 87900
    },
    {
      "epoch": 22.127231581594167,
      "grad_norm": 2.5428483486175537,
      "learning_rate": 4.8918238993710694e-05,
      "loss": 2.6093,
      "step": 88000
    },
    {
      "epoch": 22.127231581594167,
      "eval_loss": 2.6538753509521484,
      "eval_runtime": 17.3997,
      "eval_samples_per_second": 769.784,
      "eval_steps_per_second": 12.069,
      "step": 88000
    },
    {
      "epoch": 22.152376162936886,
      "grad_norm": 2.4811792373657227,
      "learning_rate": 4.8916981132075477e-05,
      "loss": 2.6133,
      "step": 88100
    },
    {
      "epoch": 22.177520744279608,
      "grad_norm": 2.6509485244750977,
      "learning_rate": 4.891572327044026e-05,
      "loss": 2.6187,
      "step": 88200
    },
    {
      "epoch": 22.202665325622327,
      "grad_norm": 2.7652969360351562,
      "learning_rate": 4.8914465408805034e-05,
      "loss": 2.6151,
      "step": 88300
    },
    {
      "epoch": 22.22780990696505,
      "grad_norm": 2.6684999465942383,
      "learning_rate": 4.8913207547169816e-05,
      "loss": 2.6254,
      "step": 88400
    },
    {
      "epoch": 22.22780990696505,
      "eval_loss": 2.6524529457092285,
      "eval_runtime": 17.2311,
      "eval_samples_per_second": 777.315,
      "eval_steps_per_second": 12.187,
      "step": 88400
    },
    {
      "epoch": 22.25295448830777,
      "grad_norm": 2.6857383251190186,
      "learning_rate": 4.891194968553459e-05,
      "loss": 2.6263,
      "step": 88500
    },
    {
      "epoch": 22.27809906965049,
      "grad_norm": 2.696211338043213,
      "learning_rate": 4.8910691823899374e-05,
      "loss": 2.6137,
      "step": 88600
    },
    {
      "epoch": 22.30324365099321,
      "grad_norm": 2.6275858879089355,
      "learning_rate": 4.890943396226415e-05,
      "loss": 2.628,
      "step": 88700
    },
    {
      "epoch": 22.328388232335932,
      "grad_norm": 2.68782377243042,
      "learning_rate": 4.890817610062893e-05,
      "loss": 2.6247,
      "step": 88800
    },
    {
      "epoch": 22.328388232335932,
      "eval_loss": 2.6526780128479004,
      "eval_runtime": 17.1327,
      "eval_samples_per_second": 781.778,
      "eval_steps_per_second": 12.257,
      "step": 88800
    },
    {
      "epoch": 22.35353281367865,
      "grad_norm": 2.6490368843078613,
      "learning_rate": 4.8906918238993714e-05,
      "loss": 2.6309,
      "step": 88900
    },
    {
      "epoch": 22.378677395021374,
      "grad_norm": 2.719658613204956,
      "learning_rate": 4.890566037735849e-05,
      "loss": 2.6328,
      "step": 89000
    },
    {
      "epoch": 22.403821976364092,
      "grad_norm": 2.51275897026062,
      "learning_rate": 4.890440251572327e-05,
      "loss": 2.6217,
      "step": 89100
    },
    {
      "epoch": 22.428966557706815,
      "grad_norm": 2.758840799331665,
      "learning_rate": 4.8903144654088054e-05,
      "loss": 2.6306,
      "step": 89200
    },
    {
      "epoch": 22.428966557706815,
      "eval_loss": 2.652951240539551,
      "eval_runtime": 17.1941,
      "eval_samples_per_second": 778.989,
      "eval_steps_per_second": 12.213,
      "step": 89200
    },
    {
      "epoch": 22.454111139049534,
      "grad_norm": 2.752401828765869,
      "learning_rate": 4.8901886792452836e-05,
      "loss": 2.6107,
      "step": 89300
    },
    {
      "epoch": 22.479255720392256,
      "grad_norm": 2.4894630908966064,
      "learning_rate": 4.890062893081761e-05,
      "loss": 2.628,
      "step": 89400
    },
    {
      "epoch": 22.504400301734975,
      "grad_norm": 2.5400843620300293,
      "learning_rate": 4.8899371069182394e-05,
      "loss": 2.622,
      "step": 89500
    },
    {
      "epoch": 22.529544883077698,
      "grad_norm": 2.674536943435669,
      "learning_rate": 4.8898113207547176e-05,
      "loss": 2.6372,
      "step": 89600
    },
    {
      "epoch": 22.529544883077698,
      "eval_loss": 2.6518094539642334,
      "eval_runtime": 17.1703,
      "eval_samples_per_second": 780.069,
      "eval_steps_per_second": 12.23,
      "step": 89600
    },
    {
      "epoch": 22.554689464420417,
      "grad_norm": 2.6398744583129883,
      "learning_rate": 4.889685534591195e-05,
      "loss": 2.6197,
      "step": 89700
    },
    {
      "epoch": 22.57983404576314,
      "grad_norm": 2.5272228717803955,
      "learning_rate": 4.8895597484276734e-05,
      "loss": 2.6137,
      "step": 89800
    },
    {
      "epoch": 22.604978627105858,
      "grad_norm": 2.726759910583496,
      "learning_rate": 4.889433962264151e-05,
      "loss": 2.6284,
      "step": 89900
    },
    {
      "epoch": 22.63012320844858,
      "grad_norm": 2.7689359188079834,
      "learning_rate": 4.889308176100629e-05,
      "loss": 2.6217,
      "step": 90000
    },
    {
      "epoch": 22.63012320844858,
      "eval_loss": 2.650761842727661,
      "eval_runtime": 17.1143,
      "eval_samples_per_second": 782.619,
      "eval_steps_per_second": 12.27,
      "step": 90000
    },
    {
      "epoch": 22.6552677897913,
      "grad_norm": 2.7074973583221436,
      "learning_rate": 4.889182389937107e-05,
      "loss": 2.6347,
      "step": 90100
    },
    {
      "epoch": 22.68041237113402,
      "grad_norm": 2.500903844833374,
      "learning_rate": 4.889056603773585e-05,
      "loss": 2.623,
      "step": 90200
    },
    {
      "epoch": 22.70555695247674,
      "grad_norm": 2.72259521484375,
      "learning_rate": 4.888930817610063e-05,
      "loss": 2.637,
      "step": 90300
    },
    {
      "epoch": 22.730701533819463,
      "grad_norm": 2.6303951740264893,
      "learning_rate": 4.8888050314465414e-05,
      "loss": 2.6331,
      "step": 90400
    },
    {
      "epoch": 22.730701533819463,
      "eval_loss": 2.6508142948150635,
      "eval_runtime": 17.214,
      "eval_samples_per_second": 778.086,
      "eval_steps_per_second": 12.199,
      "step": 90400
    },
    {
      "epoch": 22.755846115162182,
      "grad_norm": 2.627554178237915,
      "learning_rate": 4.8886792452830196e-05,
      "loss": 2.6309,
      "step": 90500
    },
    {
      "epoch": 22.780990696504904,
      "grad_norm": 2.5639808177948,
      "learning_rate": 4.888553459119497e-05,
      "loss": 2.6236,
      "step": 90600
    },
    {
      "epoch": 22.806135277847623,
      "grad_norm": 2.8489787578582764,
      "learning_rate": 4.8884276729559754e-05,
      "loss": 2.6391,
      "step": 90700
    },
    {
      "epoch": 22.831279859190346,
      "grad_norm": 2.545074939727783,
      "learning_rate": 4.888301886792453e-05,
      "loss": 2.6343,
      "step": 90800
    },
    {
      "epoch": 22.831279859190346,
      "eval_loss": 2.6493959426879883,
      "eval_runtime": 17.2462,
      "eval_samples_per_second": 776.634,
      "eval_steps_per_second": 12.177,
      "step": 90800
    },
    {
      "epoch": 22.856424440533065,
      "grad_norm": 2.4931132793426514,
      "learning_rate": 4.888176100628931e-05,
      "loss": 2.6211,
      "step": 90900
    },
    {
      "epoch": 22.881569021875787,
      "grad_norm": 2.7146666049957275,
      "learning_rate": 4.888050314465409e-05,
      "loss": 2.6353,
      "step": 91000
    },
    {
      "epoch": 22.906713603218506,
      "grad_norm": 2.8427398204803467,
      "learning_rate": 4.887924528301887e-05,
      "loss": 2.6261,
      "step": 91100
    },
    {
      "epoch": 22.93185818456123,
      "grad_norm": 2.5492923259735107,
      "learning_rate": 4.8877987421383644e-05,
      "loss": 2.6278,
      "step": 91200
    },
    {
      "epoch": 22.93185818456123,
      "eval_loss": 2.64912748336792,
      "eval_runtime": 17.0795,
      "eval_samples_per_second": 784.215,
      "eval_steps_per_second": 12.295,
      "step": 91200
    },
    {
      "epoch": 22.957002765903948,
      "grad_norm": 2.7083945274353027,
      "learning_rate": 4.8876729559748427e-05,
      "loss": 2.6424,
      "step": 91300
    },
    {
      "epoch": 22.98214734724667,
      "grad_norm": 2.5203115940093994,
      "learning_rate": 4.887547169811321e-05,
      "loss": 2.6355,
      "step": 91400
    },
    {
      "epoch": 23.00729192858939,
      "grad_norm": 2.659644603729248,
      "learning_rate": 4.887421383647799e-05,
      "loss": 2.6244,
      "step": 91500
    },
    {
      "epoch": 23.03243650993211,
      "grad_norm": 2.7395472526550293,
      "learning_rate": 4.887295597484277e-05,
      "loss": 2.602,
      "step": 91600
    },
    {
      "epoch": 23.03243650993211,
      "eval_loss": 2.6493959426879883,
      "eval_runtime": 17.0802,
      "eval_samples_per_second": 784.181,
      "eval_steps_per_second": 12.295,
      "step": 91600
    },
    {
      "epoch": 23.05758109127483,
      "grad_norm": 2.807272434234619,
      "learning_rate": 4.887169811320755e-05,
      "loss": 2.6039,
      "step": 91700
    },
    {
      "epoch": 23.08272567261755,
      "grad_norm": 2.768467903137207,
      "learning_rate": 4.887044025157233e-05,
      "loss": 2.6049,
      "step": 91800
    },
    {
      "epoch": 23.10787025396027,
      "grad_norm": 2.69862961769104,
      "learning_rate": 4.8869182389937106e-05,
      "loss": 2.5975,
      "step": 91900
    },
    {
      "epoch": 23.13301483530299,
      "grad_norm": 2.704493284225464,
      "learning_rate": 4.886792452830189e-05,
      "loss": 2.6059,
      "step": 92000
    },
    {
      "epoch": 23.13301483530299,
      "eval_loss": 2.650834083557129,
      "eval_runtime": 17.2592,
      "eval_samples_per_second": 776.048,
      "eval_steps_per_second": 12.167,
      "step": 92000
    },
    {
      "epoch": 23.158159416645713,
      "grad_norm": 2.7922680377960205,
      "learning_rate": 4.886666666666667e-05,
      "loss": 2.6071,
      "step": 92100
    },
    {
      "epoch": 23.183303997988432,
      "grad_norm": 2.730684518814087,
      "learning_rate": 4.8865408805031446e-05,
      "loss": 2.6054,
      "step": 92200
    },
    {
      "epoch": 23.208448579331154,
      "grad_norm": 2.5508058071136475,
      "learning_rate": 4.886415094339623e-05,
      "loss": 2.6024,
      "step": 92300
    },
    {
      "epoch": 23.233593160673873,
      "grad_norm": 2.5296425819396973,
      "learning_rate": 4.886289308176101e-05,
      "loss": 2.6032,
      "step": 92400
    },
    {
      "epoch": 23.233593160673873,
      "eval_loss": 2.6504926681518555,
      "eval_runtime": 17.4804,
      "eval_samples_per_second": 766.228,
      "eval_steps_per_second": 12.013,
      "step": 92400
    },
    {
      "epoch": 23.258737742016596,
      "grad_norm": 2.6867668628692627,
      "learning_rate": 4.886163522012579e-05,
      "loss": 2.6186,
      "step": 92500
    },
    {
      "epoch": 23.283882323359315,
      "grad_norm": 2.5804405212402344,
      "learning_rate": 4.886037735849057e-05,
      "loss": 2.6106,
      "step": 92600
    },
    {
      "epoch": 23.309026904702037,
      "grad_norm": 2.6227047443389893,
      "learning_rate": 4.885911949685535e-05,
      "loss": 2.6072,
      "step": 92700
    },
    {
      "epoch": 23.334171486044756,
      "grad_norm": 2.660317897796631,
      "learning_rate": 4.8857861635220126e-05,
      "loss": 2.6086,
      "step": 92800
    },
    {
      "epoch": 23.334171486044756,
      "eval_loss": 2.6497554779052734,
      "eval_runtime": 17.0406,
      "eval_samples_per_second": 786.004,
      "eval_steps_per_second": 12.323,
      "step": 92800
    },
    {
      "epoch": 23.35931606738748,
      "grad_norm": 2.8291890621185303,
      "learning_rate": 4.885660377358491e-05,
      "loss": 2.6103,
      "step": 92900
    },
    {
      "epoch": 23.384460648730197,
      "grad_norm": 2.734651803970337,
      "learning_rate": 4.885534591194969e-05,
      "loss": 2.6111,
      "step": 93000
    },
    {
      "epoch": 23.40960523007292,
      "grad_norm": 2.8022284507751465,
      "learning_rate": 4.8854088050314466e-05,
      "loss": 2.6209,
      "step": 93100
    },
    {
      "epoch": 23.43474981141564,
      "grad_norm": 2.5526371002197266,
      "learning_rate": 4.885283018867925e-05,
      "loss": 2.6169,
      "step": 93200
    },
    {
      "epoch": 23.43474981141564,
      "eval_loss": 2.6480276584625244,
      "eval_runtime": 17.3761,
      "eval_samples_per_second": 770.828,
      "eval_steps_per_second": 12.086,
      "step": 93200
    },
    {
      "epoch": 23.45989439275836,
      "grad_norm": 2.7278997898101807,
      "learning_rate": 4.8851572327044024e-05,
      "loss": 2.6089,
      "step": 93300
    },
    {
      "epoch": 23.48503897410108,
      "grad_norm": 2.6520750522613525,
      "learning_rate": 4.8850314465408806e-05,
      "loss": 2.6141,
      "step": 93400
    },
    {
      "epoch": 23.510183555443803,
      "grad_norm": 2.6514029502868652,
      "learning_rate": 4.884905660377359e-05,
      "loss": 2.6139,
      "step": 93500
    },
    {
      "epoch": 23.53532813678652,
      "grad_norm": 2.460792064666748,
      "learning_rate": 4.884779874213837e-05,
      "loss": 2.6026,
      "step": 93600
    },
    {
      "epoch": 23.53532813678652,
      "eval_loss": 2.6465466022491455,
      "eval_runtime": 17.1807,
      "eval_samples_per_second": 779.597,
      "eval_steps_per_second": 12.223,
      "step": 93600
    },
    {
      "epoch": 23.560472718129244,
      "grad_norm": 2.64410662651062,
      "learning_rate": 4.8846540880503146e-05,
      "loss": 2.6317,
      "step": 93700
    },
    {
      "epoch": 23.585617299471963,
      "grad_norm": 2.8814706802368164,
      "learning_rate": 4.884528301886793e-05,
      "loss": 2.6038,
      "step": 93800
    },
    {
      "epoch": 23.610761880814685,
      "grad_norm": 2.6445181369781494,
      "learning_rate": 4.884402515723271e-05,
      "loss": 2.6237,
      "step": 93900
    },
    {
      "epoch": 23.635906462157404,
      "grad_norm": 2.654979705810547,
      "learning_rate": 4.8842767295597486e-05,
      "loss": 2.63,
      "step": 94000
    },
    {
      "epoch": 23.635906462157404,
      "eval_loss": 2.6480014324188232,
      "eval_runtime": 17.2183,
      "eval_samples_per_second": 777.893,
      "eval_steps_per_second": 12.196,
      "step": 94000
    },
    {
      "epoch": 23.661051043500127,
      "grad_norm": 2.6190383434295654,
      "learning_rate": 4.884150943396227e-05,
      "loss": 2.6294,
      "step": 94100
    },
    {
      "epoch": 23.686195624842846,
      "grad_norm": 2.607438325881958,
      "learning_rate": 4.8840251572327044e-05,
      "loss": 2.6242,
      "step": 94200
    },
    {
      "epoch": 23.711340206185568,
      "grad_norm": 2.6642439365386963,
      "learning_rate": 4.8838993710691826e-05,
      "loss": 2.6244,
      "step": 94300
    },
    {
      "epoch": 23.736484787528287,
      "grad_norm": 2.4039320945739746,
      "learning_rate": 4.88377358490566e-05,
      "loss": 2.6299,
      "step": 94400
    },
    {
      "epoch": 23.736484787528287,
      "eval_loss": 2.648528814315796,
      "eval_runtime": 17.2155,
      "eval_samples_per_second": 778.022,
      "eval_steps_per_second": 12.198,
      "step": 94400
    },
    {
      "epoch": 23.76162936887101,
      "grad_norm": 2.737607479095459,
      "learning_rate": 4.8836477987421383e-05,
      "loss": 2.6278,
      "step": 94500
    },
    {
      "epoch": 23.78677395021373,
      "grad_norm": 2.748948335647583,
      "learning_rate": 4.8835220125786166e-05,
      "loss": 2.6216,
      "step": 94600
    },
    {
      "epoch": 23.81191853155645,
      "grad_norm": 2.3873448371887207,
      "learning_rate": 4.883396226415095e-05,
      "loss": 2.6192,
      "step": 94700
    },
    {
      "epoch": 23.83706311289917,
      "grad_norm": 2.579841136932373,
      "learning_rate": 4.883270440251573e-05,
      "loss": 2.6303,
      "step": 94800
    },
    {
      "epoch": 23.83706311289917,
      "eval_loss": 2.6432864665985107,
      "eval_runtime": 17.0166,
      "eval_samples_per_second": 787.115,
      "eval_steps_per_second": 12.341,
      "step": 94800
    },
    {
      "epoch": 23.862207694241892,
      "grad_norm": 2.7583978176116943,
      "learning_rate": 4.8831446540880506e-05,
      "loss": 2.6236,
      "step": 94900
    },
    {
      "epoch": 23.88735227558461,
      "grad_norm": 2.778428077697754,
      "learning_rate": 4.883018867924529e-05,
      "loss": 2.6274,
      "step": 95000
    },
    {
      "epoch": 23.912496856927334,
      "grad_norm": 2.5050599575042725,
      "learning_rate": 4.882893081761006e-05,
      "loss": 2.6153,
      "step": 95100
    },
    {
      "epoch": 23.937641438270052,
      "grad_norm": 2.628161668777466,
      "learning_rate": 4.8827672955974845e-05,
      "loss": 2.6145,
      "step": 95200
    },
    {
      "epoch": 23.937641438270052,
      "eval_loss": 2.6454384326934814,
      "eval_runtime": 17.3099,
      "eval_samples_per_second": 773.776,
      "eval_steps_per_second": 12.132,
      "step": 95200
    },
    {
      "epoch": 23.962786019612775,
      "grad_norm": 2.6799192428588867,
      "learning_rate": 4.882641509433962e-05,
      "loss": 2.6337,
      "step": 95300
    },
    {
      "epoch": 23.987930600955494,
      "grad_norm": 2.683566093444824,
      "learning_rate": 4.88251572327044e-05,
      "loss": 2.6176,
      "step": 95400
    },
    {
      "epoch": 24.013075182298216,
      "grad_norm": 2.7358336448669434,
      "learning_rate": 4.8823899371069185e-05,
      "loss": 2.6119,
      "step": 95500
    },
    {
      "epoch": 24.038219763640935,
      "grad_norm": 2.890446186065674,
      "learning_rate": 4.882264150943396e-05,
      "loss": 2.5739,
      "step": 95600
    },
    {
      "epoch": 24.038219763640935,
      "eval_loss": 2.6464009284973145,
      "eval_runtime": 17.1775,
      "eval_samples_per_second": 779.743,
      "eval_steps_per_second": 12.225,
      "step": 95600
    },
    {
      "epoch": 24.063364344983658,
      "grad_norm": 2.5648961067199707,
      "learning_rate": 4.882138364779874e-05,
      "loss": 2.5851,
      "step": 95700
    },
    {
      "epoch": 24.088508926326377,
      "grad_norm": 2.657442808151245,
      "learning_rate": 4.8820125786163525e-05,
      "loss": 2.5923,
      "step": 95800
    },
    {
      "epoch": 24.1136535076691,
      "grad_norm": 2.693528413772583,
      "learning_rate": 4.881886792452831e-05,
      "loss": 2.5979,
      "step": 95900
    },
    {
      "epoch": 24.138798089011818,
      "grad_norm": 2.518183469772339,
      "learning_rate": 4.881761006289308e-05,
      "loss": 2.5926,
      "step": 96000
    },
    {
      "epoch": 24.138798089011818,
      "eval_loss": 2.645250082015991,
      "eval_runtime": 17.0805,
      "eval_samples_per_second": 784.169,
      "eval_steps_per_second": 12.295,
      "step": 96000
    },
    {
      "epoch": 24.16394267035454,
      "grad_norm": 2.709749221801758,
      "learning_rate": 4.8816352201257865e-05,
      "loss": 2.6,
      "step": 96100
    },
    {
      "epoch": 24.18908725169726,
      "grad_norm": 2.7577030658721924,
      "learning_rate": 4.881509433962264e-05,
      "loss": 2.5911,
      "step": 96200
    },
    {
      "epoch": 24.214231833039978,
      "grad_norm": 2.559087038040161,
      "learning_rate": 4.881383647798742e-05,
      "loss": 2.5912,
      "step": 96300
    },
    {
      "epoch": 24.2393764143827,
      "grad_norm": 2.694502830505371,
      "learning_rate": 4.8812578616352205e-05,
      "loss": 2.599,
      "step": 96400
    },
    {
      "epoch": 24.2393764143827,
      "eval_loss": 2.646448850631714,
      "eval_runtime": 17.0428,
      "eval_samples_per_second": 785.904,
      "eval_steps_per_second": 12.322,
      "step": 96400
    },
    {
      "epoch": 24.26452099572542,
      "grad_norm": 2.6894900798797607,
      "learning_rate": 4.881132075471698e-05,
      "loss": 2.5957,
      "step": 96500
    },
    {
      "epoch": 24.289665577068142,
      "grad_norm": 2.7784440517425537,
      "learning_rate": 4.881006289308176e-05,
      "loss": 2.5974,
      "step": 96600
    },
    {
      "epoch": 24.31481015841086,
      "grad_norm": 2.784010887145996,
      "learning_rate": 4.8808805031446545e-05,
      "loss": 2.6016,
      "step": 96700
    },
    {
      "epoch": 24.339954739753583,
      "grad_norm": 2.5940074920654297,
      "learning_rate": 4.880754716981133e-05,
      "loss": 2.6108,
      "step": 96800
    },
    {
      "epoch": 24.339954739753583,
      "eval_loss": 2.6446597576141357,
      "eval_runtime": 16.985,
      "eval_samples_per_second": 788.577,
      "eval_steps_per_second": 12.364,
      "step": 96800
    },
    {
      "epoch": 24.365099321096302,
      "grad_norm": 2.696972131729126,
      "learning_rate": 4.88062893081761e-05,
      "loss": 2.6101,
      "step": 96900
    },
    {
      "epoch": 24.390243902439025,
      "grad_norm": 2.8741819858551025,
      "learning_rate": 4.8805031446540885e-05,
      "loss": 2.603,
      "step": 97000
    },
    {
      "epoch": 24.415388483781744,
      "grad_norm": 2.6818299293518066,
      "learning_rate": 4.880377358490567e-05,
      "loss": 2.6016,
      "step": 97100
    },
    {
      "epoch": 24.440533065124466,
      "grad_norm": 2.6522367000579834,
      "learning_rate": 4.880251572327044e-05,
      "loss": 2.6134,
      "step": 97200
    },
    {
      "epoch": 24.440533065124466,
      "eval_loss": 2.643195390701294,
      "eval_runtime": 17.1704,
      "eval_samples_per_second": 780.062,
      "eval_steps_per_second": 12.23,
      "step": 97200
    },
    {
      "epoch": 24.465677646467185,
      "grad_norm": 2.5447123050689697,
      "learning_rate": 4.8801257861635225e-05,
      "loss": 2.6071,
      "step": 97300
    },
    {
      "epoch": 24.490822227809907,
      "grad_norm": 2.6642117500305176,
      "learning_rate": 4.88e-05,
      "loss": 2.6151,
      "step": 97400
    },
    {
      "epoch": 24.515966809152626,
      "grad_norm": 2.5618507862091064,
      "learning_rate": 4.879874213836478e-05,
      "loss": 2.6091,
      "step": 97500
    },
    {
      "epoch": 24.54111139049535,
      "grad_norm": 2.5741965770721436,
      "learning_rate": 4.879748427672956e-05,
      "loss": 2.6128,
      "step": 97600
    },
    {
      "epoch": 24.54111139049535,
      "eval_loss": 2.6437454223632812,
      "eval_runtime": 17.0588,
      "eval_samples_per_second": 785.164,
      "eval_steps_per_second": 12.31,
      "step": 97600
    },
    {
      "epoch": 24.566255971838068,
      "grad_norm": 2.4570162296295166,
      "learning_rate": 4.879622641509434e-05,
      "loss": 2.6046,
      "step": 97700
    },
    {
      "epoch": 24.59140055318079,
      "grad_norm": 2.590876579284668,
      "learning_rate": 4.879496855345912e-05,
      "loss": 2.6053,
      "step": 97800
    },
    {
      "epoch": 24.61654513452351,
      "grad_norm": 2.6219871044158936,
      "learning_rate": 4.8793710691823905e-05,
      "loss": 2.6122,
      "step": 97900
    },
    {
      "epoch": 24.64168971586623,
      "grad_norm": 2.6692917346954346,
      "learning_rate": 4.879245283018869e-05,
      "loss": 2.6223,
      "step": 98000
    },
    {
      "epoch": 24.64168971586623,
      "eval_loss": 2.6416969299316406,
      "eval_runtime": 17.0886,
      "eval_samples_per_second": 783.797,
      "eval_steps_per_second": 12.289,
      "step": 98000
    },
    {
      "epoch": 24.66683429720895,
      "grad_norm": 2.7069523334503174,
      "learning_rate": 4.879119496855346e-05,
      "loss": 2.6032,
      "step": 98100
    },
    {
      "epoch": 24.691978878551673,
      "grad_norm": 2.6384055614471436,
      "learning_rate": 4.8789937106918245e-05,
      "loss": 2.6285,
      "step": 98200
    },
    {
      "epoch": 24.717123459894392,
      "grad_norm": 2.6359968185424805,
      "learning_rate": 4.878867924528302e-05,
      "loss": 2.6083,
      "step": 98300
    },
    {
      "epoch": 24.742268041237114,
      "grad_norm": 2.632133722305298,
      "learning_rate": 4.87874213836478e-05,
      "loss": 2.6124,
      "step": 98400
    },
    {
      "epoch": 24.742268041237114,
      "eval_loss": 2.6410865783691406,
      "eval_runtime": 17.3816,
      "eval_samples_per_second": 770.586,
      "eval_steps_per_second": 12.082,
      "step": 98400
    },
    {
      "epoch": 24.767412622579833,
      "grad_norm": 2.6253530979156494,
      "learning_rate": 4.878616352201258e-05,
      "loss": 2.6103,
      "step": 98500
    },
    {
      "epoch": 24.792557203922556,
      "grad_norm": 2.5413389205932617,
      "learning_rate": 4.878490566037736e-05,
      "loss": 2.6248,
      "step": 98600
    },
    {
      "epoch": 24.817701785265275,
      "grad_norm": 2.3888895511627197,
      "learning_rate": 4.8783647798742135e-05,
      "loss": 2.6148,
      "step": 98700
    },
    {
      "epoch": 24.842846366607997,
      "grad_norm": 2.727034568786621,
      "learning_rate": 4.878238993710692e-05,
      "loss": 2.6365,
      "step": 98800
    },
    {
      "epoch": 24.842846366607997,
      "eval_loss": 2.6402761936187744,
      "eval_runtime": 17.1435,
      "eval_samples_per_second": 781.286,
      "eval_steps_per_second": 12.25,
      "step": 98800
    },
    {
      "epoch": 24.867990947950716,
      "grad_norm": 2.5398452281951904,
      "learning_rate": 4.87811320754717e-05,
      "loss": 2.6107,
      "step": 98900
    },
    {
      "epoch": 24.89313552929344,
      "grad_norm": 2.491377115249634,
      "learning_rate": 4.877987421383648e-05,
      "loss": 2.6066,
      "step": 99000
    },
    {
      "epoch": 24.918280110636157,
      "grad_norm": 2.490553379058838,
      "learning_rate": 4.8778616352201264e-05,
      "loss": 2.6189,
      "step": 99100
    },
    {
      "epoch": 24.94342469197888,
      "grad_norm": 2.6823954582214355,
      "learning_rate": 4.877735849056604e-05,
      "loss": 2.6114,
      "step": 99200
    },
    {
      "epoch": 24.94342469197888,
      "eval_loss": 2.641878604888916,
      "eval_runtime": 17.2518,
      "eval_samples_per_second": 776.383,
      "eval_steps_per_second": 12.173,
      "step": 99200
    },
    {
      "epoch": 24.9685692733216,
      "grad_norm": 2.6246631145477295,
      "learning_rate": 4.877610062893082e-05,
      "loss": 2.6269,
      "step": 99300
    },
    {
      "epoch": 24.99371385466432,
      "grad_norm": 2.782538414001465,
      "learning_rate": 4.87748427672956e-05,
      "loss": 2.622,
      "step": 99400
    },
    {
      "epoch": 25.01885843600704,
      "grad_norm": 2.5581462383270264,
      "learning_rate": 4.877358490566038e-05,
      "loss": 2.585,
      "step": 99500
    },
    {
      "epoch": 25.044003017349763,
      "grad_norm": 2.6473758220672607,
      "learning_rate": 4.877232704402516e-05,
      "loss": 2.577,
      "step": 99600
    },
    {
      "epoch": 25.044003017349763,
      "eval_loss": 2.6418399810791016,
      "eval_runtime": 17.0586,
      "eval_samples_per_second": 785.174,
      "eval_steps_per_second": 12.31,
      "step": 99600
    },
    {
      "epoch": 25.06914759869248,
      "grad_norm": 2.576418399810791,
      "learning_rate": 4.877106918238994e-05,
      "loss": 2.575,
      "step": 99700
    },
    {
      "epoch": 25.094292180035204,
      "grad_norm": 2.480058431625366,
      "learning_rate": 4.876981132075472e-05,
      "loss": 2.5875,
      "step": 99800
    },
    {
      "epoch": 25.119436761377923,
      "grad_norm": 2.6127846240997314,
      "learning_rate": 4.8768553459119495e-05,
      "loss": 2.5997,
      "step": 99900
    },
    {
      "epoch": 25.144581342720645,
      "grad_norm": 2.6768550872802734,
      "learning_rate": 4.876729559748428e-05,
      "loss": 2.5921,
      "step": 100000
    },
    {
      "epoch": 25.144581342720645,
      "eval_loss": 2.6429150104522705,
      "eval_runtime": 17.1047,
      "eval_samples_per_second": 783.058,
      "eval_steps_per_second": 12.277,
      "step": 100000
    },
    {
      "epoch": 25.169725924063364,
      "grad_norm": 2.557373523712158,
      "learning_rate": 4.876603773584906e-05,
      "loss": 2.5838,
      "step": 100100
    },
    {
      "epoch": 25.194870505406087,
      "grad_norm": 2.6387746334075928,
      "learning_rate": 4.876477987421384e-05,
      "loss": 2.5892,
      "step": 100200
    },
    {
      "epoch": 25.220015086748806,
      "grad_norm": 2.817467212677002,
      "learning_rate": 4.876352201257862e-05,
      "loss": 2.6004,
      "step": 100300
    },
    {
      "epoch": 25.245159668091528,
      "grad_norm": 2.7068417072296143,
      "learning_rate": 4.87622641509434e-05,
      "loss": 2.5963,
      "step": 100400
    },
    {
      "epoch": 25.245159668091528,
      "eval_loss": 2.6447277069091797,
      "eval_runtime": 17.0926,
      "eval_samples_per_second": 783.613,
      "eval_steps_per_second": 12.286,
      "step": 100400
    },
    {
      "epoch": 25.270304249434247,
      "grad_norm": 2.9092659950256348,
      "learning_rate": 4.876100628930818e-05,
      "loss": 2.5949,
      "step": 100500
    },
    {
      "epoch": 25.295448830776966,
      "grad_norm": 2.660163402557373,
      "learning_rate": 4.875974842767296e-05,
      "loss": 2.5939,
      "step": 100600
    },
    {
      "epoch": 25.32059341211969,
      "grad_norm": 2.5833017826080322,
      "learning_rate": 4.875849056603774e-05,
      "loss": 2.5861,
      "step": 100700
    },
    {
      "epoch": 25.345737993462407,
      "grad_norm": 2.735337972640991,
      "learning_rate": 4.8757232704402515e-05,
      "loss": 2.6062,
      "step": 100800
    },
    {
      "epoch": 25.345737993462407,
      "eval_loss": 2.643085479736328,
      "eval_runtime": 17.1233,
      "eval_samples_per_second": 782.208,
      "eval_steps_per_second": 12.264,
      "step": 100800
    },
    {
      "epoch": 25.37088257480513,
      "grad_norm": 2.617171287536621,
      "learning_rate": 4.87559748427673e-05,
      "loss": 2.598,
      "step": 100900
    },
    {
      "epoch": 25.39602715614785,
      "grad_norm": 2.7832775115966797,
      "learning_rate": 4.875471698113207e-05,
      "loss": 2.5999,
      "step": 101000
    },
    {
      "epoch": 25.42117173749057,
      "grad_norm": 2.75545334815979,
      "learning_rate": 4.875345911949686e-05,
      "loss": 2.597,
      "step": 101100
    },
    {
      "epoch": 25.44631631883329,
      "grad_norm": 2.4733939170837402,
      "learning_rate": 4.875220125786164e-05,
      "loss": 2.6051,
      "step": 101200
    },
    {
      "epoch": 25.44631631883329,
      "eval_loss": 2.641345500946045,
      "eval_runtime": 17.0689,
      "eval_samples_per_second": 784.701,
      "eval_steps_per_second": 12.303,
      "step": 101200
    },
    {
      "epoch": 25.471460900176012,
      "grad_norm": 2.674769639968872,
      "learning_rate": 4.875094339622642e-05,
      "loss": 2.596,
      "step": 101300
    },
    {
      "epoch": 25.49660548151873,
      "grad_norm": 2.735694408416748,
      "learning_rate": 4.87496855345912e-05,
      "loss": 2.5861,
      "step": 101400
    },
    {
      "epoch": 25.521750062861454,
      "grad_norm": 2.633697271347046,
      "learning_rate": 4.874842767295598e-05,
      "loss": 2.6073,
      "step": 101500
    },
    {
      "epoch": 25.546894644204173,
      "grad_norm": 2.40207576751709,
      "learning_rate": 4.874716981132076e-05,
      "loss": 2.618,
      "step": 101600
    },
    {
      "epoch": 25.546894644204173,
      "eval_loss": 2.6425702571868896,
      "eval_runtime": 17.3548,
      "eval_samples_per_second": 771.773,
      "eval_steps_per_second": 12.1,
      "step": 101600
    },
    {
      "epoch": 25.572039225546895,
      "grad_norm": 2.7042698860168457,
      "learning_rate": 4.8745911949685535e-05,
      "loss": 2.6087,
      "step": 101700
    },
    {
      "epoch": 25.597183806889614,
      "grad_norm": 2.8268206119537354,
      "learning_rate": 4.874465408805032e-05,
      "loss": 2.5877,
      "step": 101800
    },
    {
      "epoch": 25.622328388232336,
      "grad_norm": 2.6790812015533447,
      "learning_rate": 4.874339622641509e-05,
      "loss": 2.5997,
      "step": 101900
    },
    {
      "epoch": 25.647472969575055,
      "grad_norm": 2.6600582599639893,
      "learning_rate": 4.8742138364779875e-05,
      "loss": 2.6098,
      "step": 102000
    },
    {
      "epoch": 25.647472969575055,
      "eval_loss": 2.6396896839141846,
      "eval_runtime": 17.1927,
      "eval_samples_per_second": 779.052,
      "eval_steps_per_second": 12.214,
      "step": 102000
    },
    {
      "epoch": 25.672617550917778,
      "grad_norm": 2.5565502643585205,
      "learning_rate": 4.874088050314466e-05,
      "loss": 2.6069,
      "step": 102100
    },
    {
      "epoch": 25.697762132260497,
      "grad_norm": 2.4991812705993652,
      "learning_rate": 4.873962264150944e-05,
      "loss": 2.6048,
      "step": 102200
    },
    {
      "epoch": 25.72290671360322,
      "grad_norm": 2.530897378921509,
      "learning_rate": 4.873836477987422e-05,
      "loss": 2.6046,
      "step": 102300
    },
    {
      "epoch": 25.748051294945938,
      "grad_norm": 2.485630989074707,
      "learning_rate": 4.8737106918239e-05,
      "loss": 2.6087,
      "step": 102400
    },
    {
      "epoch": 25.748051294945938,
      "eval_loss": 2.637444496154785,
      "eval_runtime": 17.1421,
      "eval_samples_per_second": 781.353,
      "eval_steps_per_second": 12.251,
      "step": 102400
    },
    {
      "epoch": 25.77319587628866,
      "grad_norm": 2.463689088821411,
      "learning_rate": 4.873584905660378e-05,
      "loss": 2.5964,
      "step": 102500
    },
    {
      "epoch": 25.79834045763138,
      "grad_norm": 2.845472812652588,
      "learning_rate": 4.8734591194968554e-05,
      "loss": 2.6062,
      "step": 102600
    },
    {
      "epoch": 25.823485038974102,
      "grad_norm": 2.504185438156128,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 2.599,
      "step": 102700
    },
    {
      "epoch": 25.84862962031682,
      "grad_norm": 2.7520291805267334,
      "learning_rate": 4.873207547169811e-05,
      "loss": 2.5984,
      "step": 102800
    },
    {
      "epoch": 25.84862962031682,
      "eval_loss": 2.6386454105377197,
      "eval_runtime": 17.0971,
      "eval_samples_per_second": 783.406,
      "eval_steps_per_second": 12.283,
      "step": 102800
    },
    {
      "epoch": 25.873774201659543,
      "grad_norm": 2.5479421615600586,
      "learning_rate": 4.8730817610062894e-05,
      "loss": 2.6011,
      "step": 102900
    },
    {
      "epoch": 25.898918783002262,
      "grad_norm": 2.4597697257995605,
      "learning_rate": 4.8729559748427676e-05,
      "loss": 2.598,
      "step": 103000
    },
    {
      "epoch": 25.924063364344985,
      "grad_norm": 2.5102975368499756,
      "learning_rate": 4.872830188679245e-05,
      "loss": 2.6017,
      "step": 103100
    },
    {
      "epoch": 25.949207945687704,
      "grad_norm": 2.652268171310425,
      "learning_rate": 4.8727044025157234e-05,
      "loss": 2.6115,
      "step": 103200
    },
    {
      "epoch": 25.949207945687704,
      "eval_loss": 2.6389691829681396,
      "eval_runtime": 17.0297,
      "eval_samples_per_second": 786.507,
      "eval_steps_per_second": 12.331,
      "step": 103200
    },
    {
      "epoch": 25.974352527030426,
      "grad_norm": 2.8228886127471924,
      "learning_rate": 4.8725786163522016e-05,
      "loss": 2.6117,
      "step": 103300
    },
    {
      "epoch": 25.999497108373145,
      "grad_norm": 2.5833163261413574,
      "learning_rate": 4.87245283018868e-05,
      "loss": 2.6021,
      "step": 103400
    },
    {
      "epoch": 26.024641689715867,
      "grad_norm": 2.551588773727417,
      "learning_rate": 4.8723270440251574e-05,
      "loss": 2.5675,
      "step": 103500
    },
    {
      "epoch": 26.049786271058586,
      "grad_norm": 2.9432923793792725,
      "learning_rate": 4.8722012578616356e-05,
      "loss": 2.5861,
      "step": 103600
    },
    {
      "epoch": 26.049786271058586,
      "eval_loss": 2.6376540660858154,
      "eval_runtime": 17.0493,
      "eval_samples_per_second": 785.605,
      "eval_steps_per_second": 12.317,
      "step": 103600
    },
    {
      "epoch": 26.07493085240131,
      "grad_norm": 2.651533603668213,
      "learning_rate": 4.872075471698113e-05,
      "loss": 2.5775,
      "step": 103700
    },
    {
      "epoch": 26.100075433744028,
      "grad_norm": 2.6788713932037354,
      "learning_rate": 4.8719496855345914e-05,
      "loss": 2.5794,
      "step": 103800
    },
    {
      "epoch": 26.12522001508675,
      "grad_norm": 2.8565268516540527,
      "learning_rate": 4.8718238993710696e-05,
      "loss": 2.5787,
      "step": 103900
    },
    {
      "epoch": 26.15036459642947,
      "grad_norm": 2.8743486404418945,
      "learning_rate": 4.871698113207547e-05,
      "loss": 2.5837,
      "step": 104000
    },
    {
      "epoch": 26.15036459642947,
      "eval_loss": 2.638244152069092,
      "eval_runtime": 17.163,
      "eval_samples_per_second": 780.4,
      "eval_steps_per_second": 12.236,
      "step": 104000
    },
    {
      "epoch": 26.17550917777219,
      "grad_norm": 2.6719179153442383,
      "learning_rate": 4.8715723270440254e-05,
      "loss": 2.5823,
      "step": 104100
    },
    {
      "epoch": 26.20065375911491,
      "grad_norm": 2.475165367126465,
      "learning_rate": 4.871446540880503e-05,
      "loss": 2.5894,
      "step": 104200
    },
    {
      "epoch": 26.225798340457633,
      "grad_norm": 2.6920201778411865,
      "learning_rate": 4.871320754716981e-05,
      "loss": 2.5925,
      "step": 104300
    },
    {
      "epoch": 26.250942921800352,
      "grad_norm": 2.636362075805664,
      "learning_rate": 4.8711949685534594e-05,
      "loss": 2.5787,
      "step": 104400
    },
    {
      "epoch": 26.250942921800352,
      "eval_loss": 2.638901710510254,
      "eval_runtime": 17.1594,
      "eval_samples_per_second": 780.562,
      "eval_steps_per_second": 12.238,
      "step": 104400
    },
    {
      "epoch": 26.276087503143074,
      "grad_norm": 2.6966750621795654,
      "learning_rate": 4.8710691823899376e-05,
      "loss": 2.5838,
      "step": 104500
    },
    {
      "epoch": 26.301232084485793,
      "grad_norm": 2.923482894897461,
      "learning_rate": 4.870943396226416e-05,
      "loss": 2.601,
      "step": 104600
    },
    {
      "epoch": 26.326376665828516,
      "grad_norm": 2.718114137649536,
      "learning_rate": 4.8708176100628934e-05,
      "loss": 2.5938,
      "step": 104700
    },
    {
      "epoch": 26.351521247171235,
      "grad_norm": 2.7520461082458496,
      "learning_rate": 4.8706918238993716e-05,
      "loss": 2.5811,
      "step": 104800
    },
    {
      "epoch": 26.351521247171235,
      "eval_loss": 2.6377227306365967,
      "eval_runtime": 17.0798,
      "eval_samples_per_second": 784.199,
      "eval_steps_per_second": 12.295,
      "step": 104800
    },
    {
      "epoch": 26.376665828513957,
      "grad_norm": 2.5769474506378174,
      "learning_rate": 4.870566037735849e-05,
      "loss": 2.5707,
      "step": 104900
    },
    {
      "epoch": 26.401810409856676,
      "grad_norm": 2.745797872543335,
      "learning_rate": 4.8704402515723274e-05,
      "loss": 2.5887,
      "step": 105000
    },
    {
      "epoch": 26.426954991199395,
      "grad_norm": 2.5805439949035645,
      "learning_rate": 4.870314465408805e-05,
      "loss": 2.5949,
      "step": 105100
    },
    {
      "epoch": 26.452099572542117,
      "grad_norm": 2.571646213531494,
      "learning_rate": 4.870188679245283e-05,
      "loss": 2.5906,
      "step": 105200
    },
    {
      "epoch": 26.452099572542117,
      "eval_loss": 2.63862943649292,
      "eval_runtime": 17.0716,
      "eval_samples_per_second": 784.578,
      "eval_steps_per_second": 12.301,
      "step": 105200
    },
    {
      "epoch": 26.477244153884836,
      "grad_norm": 2.6504809856414795,
      "learning_rate": 4.870062893081761e-05,
      "loss": 2.5818,
      "step": 105300
    },
    {
      "epoch": 26.50238873522756,
      "grad_norm": 2.7599823474884033,
      "learning_rate": 4.869937106918239e-05,
      "loss": 2.6059,
      "step": 105400
    },
    {
      "epoch": 26.527533316570278,
      "grad_norm": 2.70890736579895,
      "learning_rate": 4.869811320754718e-05,
      "loss": 2.5893,
      "step": 105500
    },
    {
      "epoch": 26.552677897913,
      "grad_norm": 2.6149027347564697,
      "learning_rate": 4.8696855345911953e-05,
      "loss": 2.5964,
      "step": 105600
    },
    {
      "epoch": 26.552677897913,
      "eval_loss": 2.6373395919799805,
      "eval_runtime": 17.0562,
      "eval_samples_per_second": 785.285,
      "eval_steps_per_second": 12.312,
      "step": 105600
    },
    {
      "epoch": 26.57782247925572,
      "grad_norm": 2.544346332550049,
      "learning_rate": 4.8695597484276736e-05,
      "loss": 2.5925,
      "step": 105700
    },
    {
      "epoch": 26.60296706059844,
      "grad_norm": 2.6882548332214355,
      "learning_rate": 4.869433962264151e-05,
      "loss": 2.59,
      "step": 105800
    },
    {
      "epoch": 26.62811164194116,
      "grad_norm": 2.6707804203033447,
      "learning_rate": 4.8693081761006293e-05,
      "loss": 2.5938,
      "step": 105900
    },
    {
      "epoch": 26.653256223283883,
      "grad_norm": 2.5997869968414307,
      "learning_rate": 4.869182389937107e-05,
      "loss": 2.584,
      "step": 106000
    },
    {
      "epoch": 26.653256223283883,
      "eval_loss": 2.635951042175293,
      "eval_runtime": 17.279,
      "eval_samples_per_second": 775.159,
      "eval_steps_per_second": 12.153,
      "step": 106000
    },
    {
      "epoch": 26.6784008046266,
      "grad_norm": 2.626427412033081,
      "learning_rate": 4.869056603773585e-05,
      "loss": 2.5983,
      "step": 106100
    },
    {
      "epoch": 26.703545385969324,
      "grad_norm": 2.3389432430267334,
      "learning_rate": 4.8689308176100627e-05,
      "loss": 2.5851,
      "step": 106200
    },
    {
      "epoch": 26.728689967312043,
      "grad_norm": 2.3843510150909424,
      "learning_rate": 4.868805031446541e-05,
      "loss": 2.6013,
      "step": 106300
    },
    {
      "epoch": 26.753834548654766,
      "grad_norm": 2.7130722999572754,
      "learning_rate": 4.868679245283019e-05,
      "loss": 2.6093,
      "step": 106400
    },
    {
      "epoch": 26.753834548654766,
      "eval_loss": 2.6348161697387695,
      "eval_runtime": 17.3407,
      "eval_samples_per_second": 772.404,
      "eval_steps_per_second": 12.11,
      "step": 106400
    },
    {
      "epoch": 26.778979129997484,
      "grad_norm": 2.7620339393615723,
      "learning_rate": 4.868553459119497e-05,
      "loss": 2.5925,
      "step": 106500
    },
    {
      "epoch": 26.804123711340207,
      "grad_norm": 2.4818527698516846,
      "learning_rate": 4.8684276729559755e-05,
      "loss": 2.5919,
      "step": 106600
    },
    {
      "epoch": 26.829268292682926,
      "grad_norm": 2.6585583686828613,
      "learning_rate": 4.868301886792453e-05,
      "loss": 2.6018,
      "step": 106700
    },
    {
      "epoch": 26.85441287402565,
      "grad_norm": 2.553683042526245,
      "learning_rate": 4.868176100628931e-05,
      "loss": 2.5887,
      "step": 106800
    },
    {
      "epoch": 26.85441287402565,
      "eval_loss": 2.6354289054870605,
      "eval_runtime": 17.6491,
      "eval_samples_per_second": 758.906,
      "eval_steps_per_second": 11.899,
      "step": 106800
    },
    {
      "epoch": 26.879557455368367,
      "grad_norm": 2.611058473587036,
      "learning_rate": 4.868050314465409e-05,
      "loss": 2.5962,
      "step": 106900
    },
    {
      "epoch": 26.90470203671109,
      "grad_norm": 2.591167688369751,
      "learning_rate": 4.867924528301887e-05,
      "loss": 2.605,
      "step": 107000
    },
    {
      "epoch": 26.92984661805381,
      "grad_norm": 2.668384552001953,
      "learning_rate": 4.867798742138365e-05,
      "loss": 2.5934,
      "step": 107100
    },
    {
      "epoch": 26.95499119939653,
      "grad_norm": 2.4409937858581543,
      "learning_rate": 4.867672955974843e-05,
      "loss": 2.6106,
      "step": 107200
    },
    {
      "epoch": 26.95499119939653,
      "eval_loss": 2.6330325603485107,
      "eval_runtime": 17.2816,
      "eval_samples_per_second": 775.046,
      "eval_steps_per_second": 12.152,
      "step": 107200
    },
    {
      "epoch": 26.98013578073925,
      "grad_norm": 2.7171027660369873,
      "learning_rate": 4.867547169811321e-05,
      "loss": 2.5991,
      "step": 107300
    },
    {
      "epoch": 27.005280362081972,
      "grad_norm": 2.5381555557250977,
      "learning_rate": 4.8674213836477986e-05,
      "loss": 2.582,
      "step": 107400
    },
    {
      "epoch": 27.03042494342469,
      "grad_norm": 2.763223171234131,
      "learning_rate": 4.867295597484277e-05,
      "loss": 2.564,
      "step": 107500
    },
    {
      "epoch": 27.055569524767414,
      "grad_norm": 2.767199993133545,
      "learning_rate": 4.867169811320755e-05,
      "loss": 2.5684,
      "step": 107600
    },
    {
      "epoch": 27.055569524767414,
      "eval_loss": 2.6375231742858887,
      "eval_runtime": 17.1133,
      "eval_samples_per_second": 782.664,
      "eval_steps_per_second": 12.271,
      "step": 107600
    },
    {
      "epoch": 27.080714106110133,
      "grad_norm": 2.6433961391448975,
      "learning_rate": 4.867044025157233e-05,
      "loss": 2.5638,
      "step": 107700
    },
    {
      "epoch": 27.105858687452855,
      "grad_norm": 3.0382580757141113,
      "learning_rate": 4.866918238993711e-05,
      "loss": 2.5631,
      "step": 107800
    },
    {
      "epoch": 27.131003268795574,
      "grad_norm": 2.599461555480957,
      "learning_rate": 4.866792452830189e-05,
      "loss": 2.5755,
      "step": 107900
    },
    {
      "epoch": 27.156147850138296,
      "grad_norm": 2.8592889308929443,
      "learning_rate": 4.866666666666667e-05,
      "loss": 2.5693,
      "step": 108000
    },
    {
      "epoch": 27.156147850138296,
      "eval_loss": 2.6395466327667236,
      "eval_runtime": 17.1397,
      "eval_samples_per_second": 781.461,
      "eval_steps_per_second": 12.252,
      "step": 108000
    },
    {
      "epoch": 27.181292431481015,
      "grad_norm": 2.6080825328826904,
      "learning_rate": 4.866540880503145e-05,
      "loss": 2.5617,
      "step": 108100
    },
    {
      "epoch": 27.206437012823738,
      "grad_norm": 2.4972896575927734,
      "learning_rate": 4.866415094339623e-05,
      "loss": 2.5748,
      "step": 108200
    },
    {
      "epoch": 27.231581594166457,
      "grad_norm": 2.908690929412842,
      "learning_rate": 4.8662893081761006e-05,
      "loss": 2.5767,
      "step": 108300
    },
    {
      "epoch": 27.25672617550918,
      "grad_norm": 2.56173038482666,
      "learning_rate": 4.866163522012579e-05,
      "loss": 2.5718,
      "step": 108400
    },
    {
      "epoch": 27.25672617550918,
      "eval_loss": 2.6362931728363037,
      "eval_runtime": 16.9585,
      "eval_samples_per_second": 789.812,
      "eval_steps_per_second": 12.383,
      "step": 108400
    },
    {
      "epoch": 27.281870756851898,
      "grad_norm": 2.567147970199585,
      "learning_rate": 4.8660377358490564e-05,
      "loss": 2.5721,
      "step": 108500
    },
    {
      "epoch": 27.30701533819462,
      "grad_norm": 2.554939031600952,
      "learning_rate": 4.8659119496855346e-05,
      "loss": 2.5874,
      "step": 108600
    },
    {
      "epoch": 27.33215991953734,
      "grad_norm": 2.6459591388702393,
      "learning_rate": 4.865786163522013e-05,
      "loss": 2.5821,
      "step": 108700
    },
    {
      "epoch": 27.357304500880062,
      "grad_norm": 2.655015707015991,
      "learning_rate": 4.865660377358491e-05,
      "loss": 2.5821,
      "step": 108800
    },
    {
      "epoch": 27.357304500880062,
      "eval_loss": 2.6355433464050293,
      "eval_runtime": 17.1386,
      "eval_samples_per_second": 781.51,
      "eval_steps_per_second": 12.253,
      "step": 108800
    },
    {
      "epoch": 27.38244908222278,
      "grad_norm": 2.7732162475585938,
      "learning_rate": 4.865534591194969e-05,
      "loss": 2.5841,
      "step": 108900
    },
    {
      "epoch": 27.407593663565503,
      "grad_norm": 2.5590462684631348,
      "learning_rate": 4.865408805031447e-05,
      "loss": 2.5774,
      "step": 109000
    },
    {
      "epoch": 27.432738244908222,
      "grad_norm": 2.711970567703247,
      "learning_rate": 4.865283018867925e-05,
      "loss": 2.5885,
      "step": 109100
    },
    {
      "epoch": 27.457882826250945,
      "grad_norm": 2.607783317565918,
      "learning_rate": 4.8651572327044026e-05,
      "loss": 2.5848,
      "step": 109200
    },
    {
      "epoch": 27.457882826250945,
      "eval_loss": 2.635601043701172,
      "eval_runtime": 16.9463,
      "eval_samples_per_second": 790.381,
      "eval_steps_per_second": 12.392,
      "step": 109200
    },
    {
      "epoch": 27.483027407593664,
      "grad_norm": 2.7157511711120605,
      "learning_rate": 4.865031446540881e-05,
      "loss": 2.5802,
      "step": 109300
    },
    {
      "epoch": 27.508171988936382,
      "grad_norm": 2.6370551586151123,
      "learning_rate": 4.864905660377358e-05,
      "loss": 2.5889,
      "step": 109400
    },
    {
      "epoch": 27.533316570279105,
      "grad_norm": 2.377532482147217,
      "learning_rate": 4.8647798742138366e-05,
      "loss": 2.5711,
      "step": 109500
    },
    {
      "epoch": 27.558461151621824,
      "grad_norm": 2.6061768531799316,
      "learning_rate": 4.864654088050315e-05,
      "loss": 2.5961,
      "step": 109600
    },
    {
      "epoch": 27.558461151621824,
      "eval_loss": 2.6332929134368896,
      "eval_runtime": 16.9096,
      "eval_samples_per_second": 792.094,
      "eval_steps_per_second": 12.419,
      "step": 109600
    },
    {
      "epoch": 27.583605732964546,
      "grad_norm": 2.5297586917877197,
      "learning_rate": 4.864528301886792e-05,
      "loss": 2.5957,
      "step": 109700
    },
    {
      "epoch": 27.608750314307265,
      "grad_norm": 2.73781418800354,
      "learning_rate": 4.8644025157232706e-05,
      "loss": 2.5894,
      "step": 109800
    },
    {
      "epoch": 27.633894895649988,
      "grad_norm": 2.6749460697174072,
      "learning_rate": 4.864276729559749e-05,
      "loss": 2.5831,
      "step": 109900
    },
    {
      "epoch": 27.659039476992707,
      "grad_norm": 2.730551242828369,
      "learning_rate": 4.864150943396227e-05,
      "loss": 2.5777,
      "step": 110000
    },
    {
      "epoch": 27.659039476992707,
      "eval_loss": 2.6347644329071045,
      "eval_runtime": 16.9921,
      "eval_samples_per_second": 788.25,
      "eval_steps_per_second": 12.359,
      "step": 110000
    },
    {
      "epoch": 27.68418405833543,
      "grad_norm": 2.583420515060425,
      "learning_rate": 4.8640251572327045e-05,
      "loss": 2.5943,
      "step": 110100
    },
    {
      "epoch": 27.709328639678148,
      "grad_norm": 2.481628656387329,
      "learning_rate": 4.863899371069183e-05,
      "loss": 2.5965,
      "step": 110200
    },
    {
      "epoch": 27.73447322102087,
      "grad_norm": 2.6206953525543213,
      "learning_rate": 4.86377358490566e-05,
      "loss": 2.5893,
      "step": 110300
    },
    {
      "epoch": 27.75961780236359,
      "grad_norm": 2.5865354537963867,
      "learning_rate": 4.8636477987421385e-05,
      "loss": 2.591,
      "step": 110400
    },
    {
      "epoch": 27.75961780236359,
      "eval_loss": 2.6326465606689453,
      "eval_runtime": 17.086,
      "eval_samples_per_second": 783.915,
      "eval_steps_per_second": 12.291,
      "step": 110400
    },
    {
      "epoch": 27.784762383706312,
      "grad_norm": 2.6879003047943115,
      "learning_rate": 4.863522012578617e-05,
      "loss": 2.5882,
      "step": 110500
    },
    {
      "epoch": 27.80990696504903,
      "grad_norm": 2.685570240020752,
      "learning_rate": 4.863396226415094e-05,
      "loss": 2.5974,
      "step": 110600
    },
    {
      "epoch": 27.835051546391753,
      "grad_norm": 2.697439432144165,
      "learning_rate": 4.8632704402515725e-05,
      "loss": 2.5834,
      "step": 110700
    },
    {
      "epoch": 27.860196127734472,
      "grad_norm": 2.7422423362731934,
      "learning_rate": 4.863144654088051e-05,
      "loss": 2.5844,
      "step": 110800
    },
    {
      "epoch": 27.860196127734472,
      "eval_loss": 2.6322858333587646,
      "eval_runtime": 16.9296,
      "eval_samples_per_second": 791.158,
      "eval_steps_per_second": 12.404,
      "step": 110800
    },
    {
      "epoch": 27.885340709077195,
      "grad_norm": 2.397848129272461,
      "learning_rate": 4.863018867924529e-05,
      "loss": 2.5877,
      "step": 110900
    },
    {
      "epoch": 27.910485290419913,
      "grad_norm": 2.770726442337036,
      "learning_rate": 4.8628930817610065e-05,
      "loss": 2.5869,
      "step": 111000
    },
    {
      "epoch": 27.935629871762636,
      "grad_norm": 2.7137999534606934,
      "learning_rate": 4.862767295597485e-05,
      "loss": 2.5993,
      "step": 111100
    },
    {
      "epoch": 27.960774453105355,
      "grad_norm": 2.714876413345337,
      "learning_rate": 4.862641509433962e-05,
      "loss": 2.6008,
      "step": 111200
    },
    {
      "epoch": 27.960774453105355,
      "eval_loss": 2.6308486461639404,
      "eval_runtime": 16.9487,
      "eval_samples_per_second": 790.267,
      "eval_steps_per_second": 12.39,
      "step": 111200
    },
    {
      "epoch": 27.985919034448077,
      "grad_norm": 2.711937665939331,
      "learning_rate": 4.8625157232704405e-05,
      "loss": 2.5851,
      "step": 111300
    },
    {
      "epoch": 28.011063615790796,
      "grad_norm": 2.479013681411743,
      "learning_rate": 4.862389937106919e-05,
      "loss": 2.5627,
      "step": 111400
    },
    {
      "epoch": 28.03620819713352,
      "grad_norm": 2.528602123260498,
      "learning_rate": 4.862264150943396e-05,
      "loss": 2.566,
      "step": 111500
    },
    {
      "epoch": 28.061352778476238,
      "grad_norm": 2.5188660621643066,
      "learning_rate": 4.8621383647798745e-05,
      "loss": 2.563,
      "step": 111600
    },
    {
      "epoch": 28.061352778476238,
      "eval_loss": 2.6362040042877197,
      "eval_runtime": 16.8963,
      "eval_samples_per_second": 792.718,
      "eval_steps_per_second": 12.429,
      "step": 111600
    },
    {
      "epoch": 28.08649735981896,
      "grad_norm": 2.4860785007476807,
      "learning_rate": 4.862012578616352e-05,
      "loss": 2.5647,
      "step": 111700
    },
    {
      "epoch": 28.11164194116168,
      "grad_norm": 2.4388859272003174,
      "learning_rate": 4.86188679245283e-05,
      "loss": 2.5662,
      "step": 111800
    },
    {
      "epoch": 28.1367865225044,
      "grad_norm": 2.6539692878723145,
      "learning_rate": 4.8617610062893085e-05,
      "loss": 2.5645,
      "step": 111900
    },
    {
      "epoch": 28.16193110384712,
      "grad_norm": 2.6411499977111816,
      "learning_rate": 4.861635220125787e-05,
      "loss": 2.5542,
      "step": 112000
    },
    {
      "epoch": 28.16193110384712,
      "eval_loss": 2.6343746185302734,
      "eval_runtime": 16.9556,
      "eval_samples_per_second": 789.946,
      "eval_steps_per_second": 12.385,
      "step": 112000
    },
    {
      "epoch": 28.187075685189843,
      "grad_norm": 2.8817145824432373,
      "learning_rate": 4.861509433962265e-05,
      "loss": 2.5764,
      "step": 112100
    },
    {
      "epoch": 28.21222026653256,
      "grad_norm": 2.6018197536468506,
      "learning_rate": 4.8613836477987425e-05,
      "loss": 2.5691,
      "step": 112200
    },
    {
      "epoch": 28.237364847875284,
      "grad_norm": 2.799797534942627,
      "learning_rate": 4.861257861635221e-05,
      "loss": 2.5551,
      "step": 112300
    },
    {
      "epoch": 28.262509429218003,
      "grad_norm": 2.4192872047424316,
      "learning_rate": 4.861132075471698e-05,
      "loss": 2.5778,
      "step": 112400
    },
    {
      "epoch": 28.262509429218003,
      "eval_loss": 2.6327154636383057,
      "eval_runtime": 16.8968,
      "eval_samples_per_second": 792.694,
      "eval_steps_per_second": 12.428,
      "step": 112400
    },
    {
      "epoch": 28.287654010560725,
      "grad_norm": 2.463392496109009,
      "learning_rate": 4.8610062893081765e-05,
      "loss": 2.5691,
      "step": 112500
    },
    {
      "epoch": 28.312798591903444,
      "grad_norm": 2.4430553913116455,
      "learning_rate": 4.860880503144654e-05,
      "loss": 2.562,
      "step": 112600
    },
    {
      "epoch": 28.337943173246167,
      "grad_norm": 2.80657696723938,
      "learning_rate": 4.860754716981132e-05,
      "loss": 2.575,
      "step": 112700
    },
    {
      "epoch": 28.363087754588886,
      "grad_norm": 2.479999303817749,
      "learning_rate": 4.86062893081761e-05,
      "loss": 2.5673,
      "step": 112800
    },
    {
      "epoch": 28.363087754588886,
      "eval_loss": 2.633054256439209,
      "eval_runtime": 16.9323,
      "eval_samples_per_second": 791.035,
      "eval_steps_per_second": 12.402,
      "step": 112800
    },
    {
      "epoch": 28.388232335931608,
      "grad_norm": 2.654768705368042,
      "learning_rate": 4.860503144654088e-05,
      "loss": 2.5763,
      "step": 112900
    },
    {
      "epoch": 28.413376917274327,
      "grad_norm": 2.5888454914093018,
      "learning_rate": 4.860377358490566e-05,
      "loss": 2.5749,
      "step": 113000
    },
    {
      "epoch": 28.43852149861705,
      "grad_norm": 2.7738516330718994,
      "learning_rate": 4.8602515723270445e-05,
      "loss": 2.5654,
      "step": 113100
    },
    {
      "epoch": 28.46366607995977,
      "grad_norm": 2.6630542278289795,
      "learning_rate": 4.860125786163523e-05,
      "loss": 2.5775,
      "step": 113200
    },
    {
      "epoch": 28.46366607995977,
      "eval_loss": 2.634036064147949,
      "eval_runtime": 16.8532,
      "eval_samples_per_second": 794.745,
      "eval_steps_per_second": 12.461,
      "step": 113200
    },
    {
      "epoch": 28.48881066130249,
      "grad_norm": 2.519672393798828,
      "learning_rate": 4.86e-05,
      "loss": 2.5698,
      "step": 113300
    },
    {
      "epoch": 28.51395524264521,
      "grad_norm": 2.6905298233032227,
      "learning_rate": 4.8598742138364784e-05,
      "loss": 2.5897,
      "step": 113400
    },
    {
      "epoch": 28.539099823987932,
      "grad_norm": 2.520707845687866,
      "learning_rate": 4.859748427672956e-05,
      "loss": 2.5771,
      "step": 113500
    },
    {
      "epoch": 28.56424440533065,
      "grad_norm": 2.5961570739746094,
      "learning_rate": 4.859622641509434e-05,
      "loss": 2.5835,
      "step": 113600
    },
    {
      "epoch": 28.56424440533065,
      "eval_loss": 2.6314432621002197,
      "eval_runtime": 16.9241,
      "eval_samples_per_second": 791.415,
      "eval_steps_per_second": 12.408,
      "step": 113600
    },
    {
      "epoch": 28.58938898667337,
      "grad_norm": 2.6005687713623047,
      "learning_rate": 4.859496855345912e-05,
      "loss": 2.5792,
      "step": 113700
    },
    {
      "epoch": 28.614533568016093,
      "grad_norm": 2.834080457687378,
      "learning_rate": 4.85937106918239e-05,
      "loss": 2.586,
      "step": 113800
    },
    {
      "epoch": 28.63967814935881,
      "grad_norm": 2.5317533016204834,
      "learning_rate": 4.859245283018868e-05,
      "loss": 2.5775,
      "step": 113900
    },
    {
      "epoch": 28.664822730701534,
      "grad_norm": 2.569801092147827,
      "learning_rate": 4.859119496855346e-05,
      "loss": 2.5815,
      "step": 114000
    },
    {
      "epoch": 28.664822730701534,
      "eval_loss": 2.6336193084716797,
      "eval_runtime": 16.9773,
      "eval_samples_per_second": 788.934,
      "eval_steps_per_second": 12.369,
      "step": 114000
    },
    {
      "epoch": 28.689967312044253,
      "grad_norm": 2.7733190059661865,
      "learning_rate": 4.858993710691824e-05,
      "loss": 2.5808,
      "step": 114100
    },
    {
      "epoch": 28.715111893386975,
      "grad_norm": 2.6032307147979736,
      "learning_rate": 4.858867924528302e-05,
      "loss": 2.5814,
      "step": 114200
    },
    {
      "epoch": 28.740256474729694,
      "grad_norm": 2.8731517791748047,
      "learning_rate": 4.8587421383647804e-05,
      "loss": 2.573,
      "step": 114300
    },
    {
      "epoch": 28.765401056072417,
      "grad_norm": 2.5855872631073,
      "learning_rate": 4.858616352201258e-05,
      "loss": 2.5788,
      "step": 114400
    },
    {
      "epoch": 28.765401056072417,
      "eval_loss": 2.630760908126831,
      "eval_runtime": 16.9745,
      "eval_samples_per_second": 789.065,
      "eval_steps_per_second": 12.371,
      "step": 114400
    },
    {
      "epoch": 28.790545637415136,
      "grad_norm": 2.4095654487609863,
      "learning_rate": 4.858490566037736e-05,
      "loss": 2.5803,
      "step": 114500
    },
    {
      "epoch": 28.815690218757858,
      "grad_norm": 2.801560640335083,
      "learning_rate": 4.8583647798742144e-05,
      "loss": 2.573,
      "step": 114600
    },
    {
      "epoch": 28.840834800100577,
      "grad_norm": 2.5863428115844727,
      "learning_rate": 4.858238993710692e-05,
      "loss": 2.5752,
      "step": 114700
    },
    {
      "epoch": 28.8659793814433,
      "grad_norm": 2.5655324459075928,
      "learning_rate": 4.85811320754717e-05,
      "loss": 2.5753,
      "step": 114800
    },
    {
      "epoch": 28.8659793814433,
      "eval_loss": 2.630190849304199,
      "eval_runtime": 17.0563,
      "eval_samples_per_second": 785.283,
      "eval_steps_per_second": 12.312,
      "step": 114800
    },
    {
      "epoch": 28.89112396278602,
      "grad_norm": 2.4481353759765625,
      "learning_rate": 4.857987421383648e-05,
      "loss": 2.5819,
      "step": 114900
    },
    {
      "epoch": 28.91626854412874,
      "grad_norm": 2.9229085445404053,
      "learning_rate": 4.857861635220126e-05,
      "loss": 2.5876,
      "step": 115000
    },
    {
      "epoch": 28.94141312547146,
      "grad_norm": 2.610156774520874,
      "learning_rate": 4.857735849056604e-05,
      "loss": 2.5773,
      "step": 115100
    },
    {
      "epoch": 28.966557706814182,
      "grad_norm": 2.584514617919922,
      "learning_rate": 4.8576100628930824e-05,
      "loss": 2.5994,
      "step": 115200
    },
    {
      "epoch": 28.966557706814182,
      "eval_loss": 2.629812717437744,
      "eval_runtime": 16.9041,
      "eval_samples_per_second": 792.353,
      "eval_steps_per_second": 12.423,
      "step": 115200
    },
    {
      "epoch": 28.9917022881569,
      "grad_norm": 2.712754249572754,
      "learning_rate": 4.85748427672956e-05,
      "loss": 2.5846,
      "step": 115300
    },
    {
      "epoch": 29.016846869499624,
      "grad_norm": 2.454829216003418,
      "learning_rate": 4.857358490566038e-05,
      "loss": 2.5649,
      "step": 115400
    },
    {
      "epoch": 29.041991450842342,
      "grad_norm": 2.6396539211273193,
      "learning_rate": 4.8572327044025164e-05,
      "loss": 2.5652,
      "step": 115500
    },
    {
      "epoch": 29.067136032185065,
      "grad_norm": 2.7838075160980225,
      "learning_rate": 4.857106918238994e-05,
      "loss": 2.5512,
      "step": 115600
    },
    {
      "epoch": 29.067136032185065,
      "eval_loss": 2.633194923400879,
      "eval_runtime": 16.8699,
      "eval_samples_per_second": 793.957,
      "eval_steps_per_second": 12.448,
      "step": 115600
    },
    {
      "epoch": 29.092280613527784,
      "grad_norm": 2.6690943241119385,
      "learning_rate": 4.856981132075472e-05,
      "loss": 2.5485,
      "step": 115700
    },
    {
      "epoch": 29.117425194870506,
      "grad_norm": 2.5846364498138428,
      "learning_rate": 4.85685534591195e-05,
      "loss": 2.5499,
      "step": 115800
    },
    {
      "epoch": 29.142569776213225,
      "grad_norm": 2.6468801498413086,
      "learning_rate": 4.856729559748428e-05,
      "loss": 2.5557,
      "step": 115900
    },
    {
      "epoch": 29.167714357555948,
      "grad_norm": 2.68200421333313,
      "learning_rate": 4.8566037735849055e-05,
      "loss": 2.5511,
      "step": 116000
    },
    {
      "epoch": 29.167714357555948,
      "eval_loss": 2.632784128189087,
      "eval_runtime": 17.0349,
      "eval_samples_per_second": 786.268,
      "eval_steps_per_second": 12.328,
      "step": 116000
    },
    {
      "epoch": 29.192858938898667,
      "grad_norm": 2.493129014968872,
      "learning_rate": 4.856477987421384e-05,
      "loss": 2.5691,
      "step": 116100
    },
    {
      "epoch": 29.21800352024139,
      "grad_norm": 2.606600284576416,
      "learning_rate": 4.856352201257862e-05,
      "loss": 2.5677,
      "step": 116200
    },
    {
      "epoch": 29.243148101584108,
      "grad_norm": 2.5668301582336426,
      "learning_rate": 4.85622641509434e-05,
      "loss": 2.5642,
      "step": 116300
    },
    {
      "epoch": 29.26829268292683,
      "grad_norm": 2.6146647930145264,
      "learning_rate": 4.8561006289308184e-05,
      "loss": 2.5544,
      "step": 116400
    },
    {
      "epoch": 29.26829268292683,
      "eval_loss": 2.633847236633301,
      "eval_runtime": 16.9758,
      "eval_samples_per_second": 789.007,
      "eval_steps_per_second": 12.371,
      "step": 116400
    },
    {
      "epoch": 29.29343726426955,
      "grad_norm": 2.715851306915283,
      "learning_rate": 4.855974842767296e-05,
      "loss": 2.5601,
      "step": 116500
    },
    {
      "epoch": 29.31858184561227,
      "grad_norm": 2.7406413555145264,
      "learning_rate": 4.855849056603774e-05,
      "loss": 2.5678,
      "step": 116600
    },
    {
      "epoch": 29.34372642695499,
      "grad_norm": 2.5389254093170166,
      "learning_rate": 4.855723270440252e-05,
      "loss": 2.5706,
      "step": 116700
    },
    {
      "epoch": 29.368871008297713,
      "grad_norm": 2.8029708862304688,
      "learning_rate": 4.85559748427673e-05,
      "loss": 2.5747,
      "step": 116800
    },
    {
      "epoch": 29.368871008297713,
      "eval_loss": 2.632160186767578,
      "eval_runtime": 17.0219,
      "eval_samples_per_second": 786.867,
      "eval_steps_per_second": 12.337,
      "step": 116800
    },
    {
      "epoch": 29.394015589640432,
      "grad_norm": 2.629885196685791,
      "learning_rate": 4.8554716981132074e-05,
      "loss": 2.582,
      "step": 116900
    },
    {
      "epoch": 29.419160170983155,
      "grad_norm": 2.5142838954925537,
      "learning_rate": 4.855345911949686e-05,
      "loss": 2.5641,
      "step": 117000
    },
    {
      "epoch": 29.444304752325873,
      "grad_norm": 2.763274908065796,
      "learning_rate": 4.855220125786164e-05,
      "loss": 2.5725,
      "step": 117100
    },
    {
      "epoch": 29.469449333668596,
      "grad_norm": 2.604032039642334,
      "learning_rate": 4.8550943396226414e-05,
      "loss": 2.5598,
      "step": 117200
    },
    {
      "epoch": 29.469449333668596,
      "eval_loss": 2.629204511642456,
      "eval_runtime": 16.9155,
      "eval_samples_per_second": 791.82,
      "eval_steps_per_second": 12.415,
      "step": 117200
    },
    {
      "epoch": 29.494593915011315,
      "grad_norm": 2.5908186435699463,
      "learning_rate": 4.8549685534591197e-05,
      "loss": 2.5619,
      "step": 117300
    },
    {
      "epoch": 29.519738496354037,
      "grad_norm": 2.4910192489624023,
      "learning_rate": 4.854842767295598e-05,
      "loss": 2.5643,
      "step": 117400
    },
    {
      "epoch": 29.544883077696756,
      "grad_norm": 2.5085947513580322,
      "learning_rate": 4.854716981132076e-05,
      "loss": 2.5563,
      "step": 117500
    },
    {
      "epoch": 29.57002765903948,
      "grad_norm": 2.8301875591278076,
      "learning_rate": 4.8545911949685537e-05,
      "loss": 2.5638,
      "step": 117600
    },
    {
      "epoch": 29.57002765903948,
      "eval_loss": 2.632354736328125,
      "eval_runtime": 17.1364,
      "eval_samples_per_second": 781.61,
      "eval_steps_per_second": 12.255,
      "step": 117600
    },
    {
      "epoch": 29.595172240382198,
      "grad_norm": 2.553241729736328,
      "learning_rate": 4.854465408805032e-05,
      "loss": 2.5711,
      "step": 117700
    },
    {
      "epoch": 29.62031682172492,
      "grad_norm": 2.6555569171905518,
      "learning_rate": 4.8543396226415094e-05,
      "loss": 2.5699,
      "step": 117800
    },
    {
      "epoch": 29.64546140306764,
      "grad_norm": 2.6833620071411133,
      "learning_rate": 4.8542138364779876e-05,
      "loss": 2.5737,
      "step": 117900
    },
    {
      "epoch": 29.670605984410358,
      "grad_norm": 2.7007806301116943,
      "learning_rate": 4.854088050314466e-05,
      "loss": 2.5772,
      "step": 118000
    },
    {
      "epoch": 29.670605984410358,
      "eval_loss": 2.6287126541137695,
      "eval_runtime": 17.175,
      "eval_samples_per_second": 779.856,
      "eval_steps_per_second": 12.227,
      "step": 118000
    },
    {
      "epoch": 29.69575056575308,
      "grad_norm": 2.679823875427246,
      "learning_rate": 4.8539622641509434e-05,
      "loss": 2.5748,
      "step": 118100
    },
    {
      "epoch": 29.7208951470958,
      "grad_norm": 2.715874433517456,
      "learning_rate": 4.8538364779874216e-05,
      "loss": 2.5785,
      "step": 118200
    },
    {
      "epoch": 29.74603972843852,
      "grad_norm": 2.552950859069824,
      "learning_rate": 4.853710691823899e-05,
      "loss": 2.584,
      "step": 118300
    },
    {
      "epoch": 29.77118430978124,
      "grad_norm": 2.4925589561462402,
      "learning_rate": 4.8535849056603774e-05,
      "loss": 2.5761,
      "step": 118400
    },
    {
      "epoch": 29.77118430978124,
      "eval_loss": 2.627639055252075,
      "eval_runtime": 17.0384,
      "eval_samples_per_second": 786.106,
      "eval_steps_per_second": 12.325,
      "step": 118400
    },
    {
      "epoch": 29.796328891123963,
      "grad_norm": 2.502758026123047,
      "learning_rate": 4.8534591194968556e-05,
      "loss": 2.557,
      "step": 118500
    },
    {
      "epoch": 29.821473472466682,
      "grad_norm": 2.565175771713257,
      "learning_rate": 4.853333333333334e-05,
      "loss": 2.5771,
      "step": 118600
    },
    {
      "epoch": 29.846618053809404,
      "grad_norm": 2.562582015991211,
      "learning_rate": 4.8532075471698114e-05,
      "loss": 2.5808,
      "step": 118700
    },
    {
      "epoch": 29.871762635152123,
      "grad_norm": 2.5331413745880127,
      "learning_rate": 4.8530817610062896e-05,
      "loss": 2.5689,
      "step": 118800
    },
    {
      "epoch": 29.871762635152123,
      "eval_loss": 2.629265546798706,
      "eval_runtime": 17.1041,
      "eval_samples_per_second": 783.087,
      "eval_steps_per_second": 12.278,
      "step": 118800
    },
    {
      "epoch": 29.896907216494846,
      "grad_norm": 2.631457805633545,
      "learning_rate": 4.852955974842768e-05,
      "loss": 2.5668,
      "step": 118900
    },
    {
      "epoch": 29.922051797837565,
      "grad_norm": 2.4062483310699463,
      "learning_rate": 4.8528301886792454e-05,
      "loss": 2.5762,
      "step": 119000
    },
    {
      "epoch": 29.947196379180287,
      "grad_norm": 2.399892568588257,
      "learning_rate": 4.8527044025157236e-05,
      "loss": 2.5894,
      "step": 119100
    },
    {
      "epoch": 29.972340960523006,
      "grad_norm": 2.6681814193725586,
      "learning_rate": 4.852578616352201e-05,
      "loss": 2.5746,
      "step": 119200
    },
    {
      "epoch": 29.972340960523006,
      "eval_loss": 2.628981828689575,
      "eval_runtime": 16.9384,
      "eval_samples_per_second": 790.747,
      "eval_steps_per_second": 12.398,
      "step": 119200
    },
    {
      "epoch": 29.99748554186573,
      "grad_norm": 2.6820294857025146,
      "learning_rate": 4.8524528301886794e-05,
      "loss": 2.5768,
      "step": 119300
    },
    {
      "epoch": 30.022630123208447,
      "grad_norm": 2.585664987564087,
      "learning_rate": 4.852327044025157e-05,
      "loss": 2.5432,
      "step": 119400
    },
    {
      "epoch": 30.04777470455117,
      "grad_norm": 2.622262954711914,
      "learning_rate": 4.852201257861636e-05,
      "loss": 2.5346,
      "step": 119500
    },
    {
      "epoch": 30.07291928589389,
      "grad_norm": 2.7727653980255127,
      "learning_rate": 4.852075471698114e-05,
      "loss": 2.5432,
      "step": 119600
    },
    {
      "epoch": 30.07291928589389,
      "eval_loss": 2.6298656463623047,
      "eval_runtime": 17.0907,
      "eval_samples_per_second": 783.701,
      "eval_steps_per_second": 12.287,
      "step": 119600
    },
    {
      "epoch": 30.09806386723661,
      "grad_norm": 2.4919135570526123,
      "learning_rate": 4.8519496855345916e-05,
      "loss": 2.5416,
      "step": 119700
    },
    {
      "epoch": 30.12320844857933,
      "grad_norm": 2.5717108249664307,
      "learning_rate": 4.85182389937107e-05,
      "loss": 2.5605,
      "step": 119800
    },
    {
      "epoch": 30.148353029922053,
      "grad_norm": 2.578671932220459,
      "learning_rate": 4.8516981132075474e-05,
      "loss": 2.545,
      "step": 119900
    },
    {
      "epoch": 30.17349761126477,
      "grad_norm": 2.5972726345062256,
      "learning_rate": 4.8515723270440256e-05,
      "loss": 2.5456,
      "step": 120000
    },
    {
      "epoch": 30.17349761126477,
      "eval_loss": 2.6326634883880615,
      "eval_runtime": 16.8769,
      "eval_samples_per_second": 793.627,
      "eval_steps_per_second": 12.443,
      "step": 120000
    },
    {
      "epoch": 30.198642192607494,
      "grad_norm": 2.5051052570343018,
      "learning_rate": 4.851446540880503e-05,
      "loss": 2.5614,
      "step": 120100
    },
    {
      "epoch": 30.223786773950213,
      "grad_norm": 2.522350788116455,
      "learning_rate": 4.8513207547169814e-05,
      "loss": 2.5591,
      "step": 120200
    },
    {
      "epoch": 30.248931355292935,
      "grad_norm": 2.4072482585906982,
      "learning_rate": 4.851194968553459e-05,
      "loss": 2.549,
      "step": 120300
    },
    {
      "epoch": 30.274075936635654,
      "grad_norm": 2.935497760772705,
      "learning_rate": 4.851069182389937e-05,
      "loss": 2.5495,
      "step": 120400
    },
    {
      "epoch": 30.274075936635654,
      "eval_loss": 2.632040500640869,
      "eval_runtime": 17.0039,
      "eval_samples_per_second": 787.7,
      "eval_steps_per_second": 12.35,
      "step": 120400
    },
    {
      "epoch": 30.299220517978377,
      "grad_norm": 2.6152312755584717,
      "learning_rate": 4.8509433962264153e-05,
      "loss": 2.5642,
      "step": 120500
    },
    {
      "epoch": 30.324365099321096,
      "grad_norm": 2.710542917251587,
      "learning_rate": 4.8508176100628936e-05,
      "loss": 2.5635,
      "step": 120600
    },
    {
      "epoch": 30.349509680663818,
      "grad_norm": 2.81626033782959,
      "learning_rate": 4.850691823899372e-05,
      "loss": 2.5431,
      "step": 120700
    },
    {
      "epoch": 30.374654262006537,
      "grad_norm": 2.580857276916504,
      "learning_rate": 4.850566037735849e-05,
      "loss": 2.556,
      "step": 120800
    },
    {
      "epoch": 30.374654262006537,
      "eval_loss": 2.629572868347168,
      "eval_runtime": 16.9154,
      "eval_samples_per_second": 791.821,
      "eval_steps_per_second": 12.415,
      "step": 120800
    },
    {
      "epoch": 30.39979884334926,
      "grad_norm": 2.6821296215057373,
      "learning_rate": 4.8504402515723276e-05,
      "loss": 2.5675,
      "step": 120900
    },
    {
      "epoch": 30.42494342469198,
      "grad_norm": 2.598198890686035,
      "learning_rate": 4.850314465408805e-05,
      "loss": 2.5666,
      "step": 121000
    },
    {
      "epoch": 30.4500880060347,
      "grad_norm": 2.619216203689575,
      "learning_rate": 4.850188679245283e-05,
      "loss": 2.5508,
      "step": 121100
    },
    {
      "epoch": 30.47523258737742,
      "grad_norm": 2.624155282974243,
      "learning_rate": 4.850062893081761e-05,
      "loss": 2.5475,
      "step": 121200
    },
    {
      "epoch": 30.47523258737742,
      "eval_loss": 2.630910873413086,
      "eval_runtime": 16.8839,
      "eval_samples_per_second": 793.299,
      "eval_steps_per_second": 12.438,
      "step": 121200
    },
    {
      "epoch": 30.500377168720142,
      "grad_norm": 2.76892352104187,
      "learning_rate": 4.849937106918239e-05,
      "loss": 2.5623,
      "step": 121300
    },
    {
      "epoch": 30.52552175006286,
      "grad_norm": 2.7890257835388184,
      "learning_rate": 4.849811320754717e-05,
      "loss": 2.5653,
      "step": 121400
    },
    {
      "epoch": 30.550666331405584,
      "grad_norm": 2.611720323562622,
      "learning_rate": 4.849685534591195e-05,
      "loss": 2.5644,
      "step": 121500
    },
    {
      "epoch": 30.575810912748302,
      "grad_norm": 2.862818479537964,
      "learning_rate": 4.849559748427673e-05,
      "loss": 2.578,
      "step": 121600
    },
    {
      "epoch": 30.575810912748302,
      "eval_loss": 2.6286590099334717,
      "eval_runtime": 16.9527,
      "eval_samples_per_second": 790.082,
      "eval_steps_per_second": 12.387,
      "step": 121600
    },
    {
      "epoch": 30.600955494091025,
      "grad_norm": 2.4951858520507812,
      "learning_rate": 4.849433962264151e-05,
      "loss": 2.5644,
      "step": 121700
    },
    {
      "epoch": 30.626100075433744,
      "grad_norm": 2.612955093383789,
      "learning_rate": 4.8493081761006295e-05,
      "loss": 2.5677,
      "step": 121800
    },
    {
      "epoch": 30.651244656776466,
      "grad_norm": 2.7044827938079834,
      "learning_rate": 4.849182389937107e-05,
      "loss": 2.5608,
      "step": 121900
    },
    {
      "epoch": 30.676389238119185,
      "grad_norm": 2.669265031814575,
      "learning_rate": 4.849056603773585e-05,
      "loss": 2.5632,
      "step": 122000
    },
    {
      "epoch": 30.676389238119185,
      "eval_loss": 2.625701904296875,
      "eval_runtime": 17.168,
      "eval_samples_per_second": 780.172,
      "eval_steps_per_second": 12.232,
      "step": 122000
    },
    {
      "epoch": 30.701533819461908,
      "grad_norm": 2.526158094406128,
      "learning_rate": 4.848930817610063e-05,
      "loss": 2.5714,
      "step": 122100
    },
    {
      "epoch": 30.726678400804627,
      "grad_norm": 2.670243263244629,
      "learning_rate": 4.848805031446541e-05,
      "loss": 2.5669,
      "step": 122200
    },
    {
      "epoch": 30.751822982147345,
      "grad_norm": 2.882530450820923,
      "learning_rate": 4.848679245283019e-05,
      "loss": 2.5621,
      "step": 122300
    },
    {
      "epoch": 30.776967563490068,
      "grad_norm": 2.5942885875701904,
      "learning_rate": 4.848553459119497e-05,
      "loss": 2.5724,
      "step": 122400
    },
    {
      "epoch": 30.776967563490068,
      "eval_loss": 2.625837564468384,
      "eval_runtime": 17.0512,
      "eval_samples_per_second": 785.515,
      "eval_steps_per_second": 12.316,
      "step": 122400
    },
    {
      "epoch": 30.80211214483279,
      "grad_norm": 2.4369921684265137,
      "learning_rate": 4.848427672955975e-05,
      "loss": 2.574,
      "step": 122500
    },
    {
      "epoch": 30.82725672617551,
      "grad_norm": 2.6419737339019775,
      "learning_rate": 4.8483018867924526e-05,
      "loss": 2.5736,
      "step": 122600
    },
    {
      "epoch": 30.852401307518228,
      "grad_norm": 2.585444688796997,
      "learning_rate": 4.848176100628931e-05,
      "loss": 2.5646,
      "step": 122700
    },
    {
      "epoch": 30.87754588886095,
      "grad_norm": 2.5670244693756104,
      "learning_rate": 4.848050314465409e-05,
      "loss": 2.5705,
      "step": 122800
    },
    {
      "epoch": 30.87754588886095,
      "eval_loss": 2.623805046081543,
      "eval_runtime": 17.0582,
      "eval_samples_per_second": 785.193,
      "eval_steps_per_second": 12.311,
      "step": 122800
    },
    {
      "epoch": 30.90269047020367,
      "grad_norm": 2.6779532432556152,
      "learning_rate": 4.847924528301887e-05,
      "loss": 2.5624,
      "step": 122900
    },
    {
      "epoch": 30.927835051546392,
      "grad_norm": 2.6372785568237305,
      "learning_rate": 4.8477987421383655e-05,
      "loss": 2.5743,
      "step": 123000
    },
    {
      "epoch": 30.95297963288911,
      "grad_norm": 2.4051804542541504,
      "learning_rate": 4.847672955974843e-05,
      "loss": 2.5702,
      "step": 123100
    },
    {
      "epoch": 30.978124214231833,
      "grad_norm": 2.5318918228149414,
      "learning_rate": 4.847547169811321e-05,
      "loss": 2.5784,
      "step": 123200
    },
    {
      "epoch": 30.978124214231833,
      "eval_loss": 2.6241304874420166,
      "eval_runtime": 17.1519,
      "eval_samples_per_second": 780.907,
      "eval_steps_per_second": 12.244,
      "step": 123200
    },
    {
      "epoch": 31.003268795574552,
      "grad_norm": 2.776252508163452,
      "learning_rate": 4.847421383647799e-05,
      "loss": 2.562,
      "step": 123300
    },
    {
      "epoch": 31.028413376917275,
      "grad_norm": 2.6153602600097656,
      "learning_rate": 4.847295597484277e-05,
      "loss": 2.5224,
      "step": 123400
    },
    {
      "epoch": 31.053557958259994,
      "grad_norm": 2.365013837814331,
      "learning_rate": 4.8471698113207546e-05,
      "loss": 2.5232,
      "step": 123500
    },
    {
      "epoch": 31.078702539602716,
      "grad_norm": 2.840627670288086,
      "learning_rate": 4.847044025157233e-05,
      "loss": 2.535,
      "step": 123600
    },
    {
      "epoch": 31.078702539602716,
      "eval_loss": 2.629153251647949,
      "eval_runtime": 17.1968,
      "eval_samples_per_second": 778.866,
      "eval_steps_per_second": 12.212,
      "step": 123600
    },
    {
      "epoch": 31.103847120945435,
      "grad_norm": 2.5701420307159424,
      "learning_rate": 4.8469182389937103e-05,
      "loss": 2.5414,
      "step": 123700
    },
    {
      "epoch": 31.128991702288157,
      "grad_norm": 2.669454336166382,
      "learning_rate": 4.8467924528301886e-05,
      "loss": 2.5437,
      "step": 123800
    },
    {
      "epoch": 31.154136283630876,
      "grad_norm": 2.563131332397461,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 2.546,
      "step": 123900
    },
    {
      "epoch": 31.1792808649736,
      "grad_norm": 2.905642509460449,
      "learning_rate": 4.846540880503145e-05,
      "loss": 2.5468,
      "step": 124000
    },
    {
      "epoch": 31.1792808649736,
      "eval_loss": 2.62857723236084,
      "eval_runtime": 17.0812,
      "eval_samples_per_second": 784.135,
      "eval_steps_per_second": 12.294,
      "step": 124000
    },
    {
      "epoch": 31.204425446316318,
      "grad_norm": 2.5535521507263184,
      "learning_rate": 4.846415094339623e-05,
      "loss": 2.5385,
      "step": 124100
    },
    {
      "epoch": 31.22957002765904,
      "grad_norm": 2.5712766647338867,
      "learning_rate": 4.846289308176101e-05,
      "loss": 2.5444,
      "step": 124200
    },
    {
      "epoch": 31.25471460900176,
      "grad_norm": 2.6245858669281006,
      "learning_rate": 4.846163522012579e-05,
      "loss": 2.5485,
      "step": 124300
    },
    {
      "epoch": 31.27985919034448,
      "grad_norm": 2.6186575889587402,
      "learning_rate": 4.8460377358490566e-05,
      "loss": 2.5492,
      "step": 124400
    },
    {
      "epoch": 31.27985919034448,
      "eval_loss": 2.627464771270752,
      "eval_runtime": 17.1078,
      "eval_samples_per_second": 782.919,
      "eval_steps_per_second": 12.275,
      "step": 124400
    },
    {
      "epoch": 31.3050037716872,
      "grad_norm": 2.7338051795959473,
      "learning_rate": 4.845911949685535e-05,
      "loss": 2.5522,
      "step": 124500
    },
    {
      "epoch": 31.330148353029923,
      "grad_norm": 2.7304000854492188,
      "learning_rate": 4.845786163522012e-05,
      "loss": 2.5538,
      "step": 124600
    },
    {
      "epoch": 31.355292934372642,
      "grad_norm": 2.4590694904327393,
      "learning_rate": 4.8456603773584905e-05,
      "loss": 2.5438,
      "step": 124700
    },
    {
      "epoch": 31.380437515715364,
      "grad_norm": 2.7801315784454346,
      "learning_rate": 4.845534591194969e-05,
      "loss": 2.5576,
      "step": 124800
    },
    {
      "epoch": 31.380437515715364,
      "eval_loss": 2.6285674571990967,
      "eval_runtime": 16.9573,
      "eval_samples_per_second": 789.868,
      "eval_steps_per_second": 12.384,
      "step": 124800
    },
    {
      "epoch": 31.405582097058083,
      "grad_norm": 2.5920321941375732,
      "learning_rate": 4.845408805031447e-05,
      "loss": 2.5523,
      "step": 124900
    },
    {
      "epoch": 31.430726678400806,
      "grad_norm": 2.8243093490600586,
      "learning_rate": 4.845283018867925e-05,
      "loss": 2.5594,
      "step": 125000
    },
    {
      "epoch": 31.455871259743525,
      "grad_norm": 2.452707529067993,
      "learning_rate": 4.845157232704403e-05,
      "loss": 2.5575,
      "step": 125100
    },
    {
      "epoch": 31.481015841086247,
      "grad_norm": 2.6286613941192627,
      "learning_rate": 4.845031446540881e-05,
      "loss": 2.5496,
      "step": 125200
    },
    {
      "epoch": 31.481015841086247,
      "eval_loss": 2.6275763511657715,
      "eval_runtime": 17.0003,
      "eval_samples_per_second": 787.869,
      "eval_steps_per_second": 12.353,
      "step": 125200
    },
    {
      "epoch": 31.506160422428966,
      "grad_norm": 2.697308301925659,
      "learning_rate": 4.8449056603773585e-05,
      "loss": 2.5601,
      "step": 125300
    },
    {
      "epoch": 31.53130500377169,
      "grad_norm": 2.5676252841949463,
      "learning_rate": 4.844779874213837e-05,
      "loss": 2.5455,
      "step": 125400
    },
    {
      "epoch": 31.556449585114407,
      "grad_norm": 2.4912686347961426,
      "learning_rate": 4.844654088050315e-05,
      "loss": 2.5482,
      "step": 125500
    },
    {
      "epoch": 31.58159416645713,
      "grad_norm": 2.452840805053711,
      "learning_rate": 4.8445283018867925e-05,
      "loss": 2.5606,
      "step": 125600
    },
    {
      "epoch": 31.58159416645713,
      "eval_loss": 2.6269242763519287,
      "eval_runtime": 17.0305,
      "eval_samples_per_second": 786.471,
      "eval_steps_per_second": 12.331,
      "step": 125600
    },
    {
      "epoch": 31.60673874779985,
      "grad_norm": 2.6289217472076416,
      "learning_rate": 4.844402515723271e-05,
      "loss": 2.5468,
      "step": 125700
    },
    {
      "epoch": 31.63188332914257,
      "grad_norm": 2.5038869380950928,
      "learning_rate": 4.844276729559748e-05,
      "loss": 2.5596,
      "step": 125800
    },
    {
      "epoch": 31.65702791048529,
      "grad_norm": 2.624863862991333,
      "learning_rate": 4.8441509433962265e-05,
      "loss": 2.57,
      "step": 125900
    },
    {
      "epoch": 31.682172491828013,
      "grad_norm": 2.4644370079040527,
      "learning_rate": 4.844025157232705e-05,
      "loss": 2.5624,
      "step": 126000
    },
    {
      "epoch": 31.682172491828013,
      "eval_loss": 2.6288933753967285,
      "eval_runtime": 16.9834,
      "eval_samples_per_second": 788.653,
      "eval_steps_per_second": 12.365,
      "step": 126000
    },
    {
      "epoch": 31.70731707317073,
      "grad_norm": 2.4428930282592773,
      "learning_rate": 4.843899371069183e-05,
      "loss": 2.5667,
      "step": 126100
    },
    {
      "epoch": 31.732461654513454,
      "grad_norm": 2.7258496284484863,
      "learning_rate": 4.8437735849056605e-05,
      "loss": 2.5703,
      "step": 126200
    },
    {
      "epoch": 31.757606235856173,
      "grad_norm": 2.3755156993865967,
      "learning_rate": 4.843647798742139e-05,
      "loss": 2.5764,
      "step": 126300
    },
    {
      "epoch": 31.782750817198895,
      "grad_norm": 2.785560369491577,
      "learning_rate": 4.843522012578617e-05,
      "loss": 2.5597,
      "step": 126400
    },
    {
      "epoch": 31.782750817198895,
      "eval_loss": 2.626065969467163,
      "eval_runtime": 16.9112,
      "eval_samples_per_second": 792.019,
      "eval_steps_per_second": 12.418,
      "step": 126400
    },
    {
      "epoch": 31.807895398541614,
      "grad_norm": 2.472104072570801,
      "learning_rate": 4.8433962264150945e-05,
      "loss": 2.5699,
      "step": 126500
    },
    {
      "epoch": 31.833039979884337,
      "grad_norm": 2.616546392440796,
      "learning_rate": 4.843270440251573e-05,
      "loss": 2.5558,
      "step": 126600
    },
    {
      "epoch": 31.858184561227056,
      "grad_norm": 2.689241886138916,
      "learning_rate": 4.84314465408805e-05,
      "loss": 2.5527,
      "step": 126700
    },
    {
      "epoch": 31.883329142569778,
      "grad_norm": 2.7031495571136475,
      "learning_rate": 4.8430188679245285e-05,
      "loss": 2.5601,
      "step": 126800
    },
    {
      "epoch": 31.883329142569778,
      "eval_loss": 2.6249985694885254,
      "eval_runtime": 17.0891,
      "eval_samples_per_second": 783.775,
      "eval_steps_per_second": 12.289,
      "step": 126800
    },
    {
      "epoch": 31.908473723912497,
      "grad_norm": 2.576472043991089,
      "learning_rate": 4.842893081761006e-05,
      "loss": 2.5701,
      "step": 126900
    },
    {
      "epoch": 31.933618305255216,
      "grad_norm": 2.729309558868408,
      "learning_rate": 4.842767295597484e-05,
      "loss": 2.5723,
      "step": 127000
    },
    {
      "epoch": 31.95876288659794,
      "grad_norm": 2.3475968837738037,
      "learning_rate": 4.8426415094339625e-05,
      "loss": 2.5617,
      "step": 127100
    },
    {
      "epoch": 31.983907467940657,
      "grad_norm": 2.6317427158355713,
      "learning_rate": 4.842515723270441e-05,
      "loss": 2.5593,
      "step": 127200
    },
    {
      "epoch": 31.983907467940657,
      "eval_loss": 2.623591899871826,
      "eval_runtime": 17.067,
      "eval_samples_per_second": 784.79,
      "eval_steps_per_second": 12.304,
      "step": 127200
    },
    {
      "epoch": 32.009052049283376,
      "grad_norm": 2.609064817428589,
      "learning_rate": 4.842389937106919e-05,
      "loss": 2.5436,
      "step": 127300
    },
    {
      "epoch": 32.0341966306261,
      "grad_norm": 2.8274827003479004,
      "learning_rate": 4.8422641509433965e-05,
      "loss": 2.528,
      "step": 127400
    },
    {
      "epoch": 32.05934121196882,
      "grad_norm": 2.7640013694763184,
      "learning_rate": 4.842138364779875e-05,
      "loss": 2.5291,
      "step": 127500
    },
    {
      "epoch": 32.08448579331154,
      "grad_norm": 2.4896037578582764,
      "learning_rate": 4.842012578616352e-05,
      "loss": 2.5203,
      "step": 127600
    },
    {
      "epoch": 32.08448579331154,
      "eval_loss": 2.6258254051208496,
      "eval_runtime": 17.0332,
      "eval_samples_per_second": 786.345,
      "eval_steps_per_second": 12.329,
      "step": 127600
    },
    {
      "epoch": 32.10963037465426,
      "grad_norm": 2.5830111503601074,
      "learning_rate": 4.8418867924528305e-05,
      "loss": 2.5402,
      "step": 127700
    },
    {
      "epoch": 32.13477495599698,
      "grad_norm": 2.757408618927002,
      "learning_rate": 4.841761006289308e-05,
      "loss": 2.5345,
      "step": 127800
    },
    {
      "epoch": 32.159919537339704,
      "grad_norm": 2.7373507022857666,
      "learning_rate": 4.841635220125786e-05,
      "loss": 2.5276,
      "step": 127900
    },
    {
      "epoch": 32.185064118682426,
      "grad_norm": 3.070136070251465,
      "learning_rate": 4.8415094339622645e-05,
      "loss": 2.5458,
      "step": 128000
    },
    {
      "epoch": 32.185064118682426,
      "eval_loss": 2.62717604637146,
      "eval_runtime": 17.0783,
      "eval_samples_per_second": 784.269,
      "eval_steps_per_second": 12.296,
      "step": 128000
    },
    {
      "epoch": 32.21020870002514,
      "grad_norm": 2.7281534671783447,
      "learning_rate": 4.841383647798742e-05,
      "loss": 2.5422,
      "step": 128100
    },
    {
      "epoch": 32.235353281367864,
      "grad_norm": 2.4764187335968018,
      "learning_rate": 4.84125786163522e-05,
      "loss": 2.5446,
      "step": 128200
    },
    {
      "epoch": 32.26049786271059,
      "grad_norm": 2.623746156692505,
      "learning_rate": 4.8411320754716984e-05,
      "loss": 2.536,
      "step": 128300
    },
    {
      "epoch": 32.28564244405331,
      "grad_norm": 2.5681605339050293,
      "learning_rate": 4.841006289308177e-05,
      "loss": 2.5419,
      "step": 128400
    },
    {
      "epoch": 32.28564244405331,
      "eval_loss": 2.6262905597686768,
      "eval_runtime": 16.9364,
      "eval_samples_per_second": 790.842,
      "eval_steps_per_second": 12.399,
      "step": 128400
    },
    {
      "epoch": 32.310787025396024,
      "grad_norm": 2.4896039962768555,
      "learning_rate": 4.840880503144654e-05,
      "loss": 2.535,
      "step": 128500
    },
    {
      "epoch": 32.33593160673875,
      "grad_norm": 2.85237717628479,
      "learning_rate": 4.8407547169811324e-05,
      "loss": 2.5437,
      "step": 128600
    },
    {
      "epoch": 32.36107618808147,
      "grad_norm": 2.622896432876587,
      "learning_rate": 4.84062893081761e-05,
      "loss": 2.5368,
      "step": 128700
    },
    {
      "epoch": 32.38622076942419,
      "grad_norm": 2.617539644241333,
      "learning_rate": 4.840503144654088e-05,
      "loss": 2.5517,
      "step": 128800
    },
    {
      "epoch": 32.38622076942419,
      "eval_loss": 2.6265206336975098,
      "eval_runtime": 16.9262,
      "eval_samples_per_second": 791.316,
      "eval_steps_per_second": 12.407,
      "step": 128800
    },
    {
      "epoch": 32.41136535076691,
      "grad_norm": 2.5091326236724854,
      "learning_rate": 4.8403773584905664e-05,
      "loss": 2.5471,
      "step": 128900
    },
    {
      "epoch": 32.43650993210963,
      "grad_norm": 2.4842605590820312,
      "learning_rate": 4.840251572327044e-05,
      "loss": 2.5537,
      "step": 129000
    },
    {
      "epoch": 32.46165451345235,
      "grad_norm": 2.7685787677764893,
      "learning_rate": 4.840125786163522e-05,
      "loss": 2.5534,
      "step": 129100
    },
    {
      "epoch": 32.486799094795074,
      "grad_norm": 2.5535762310028076,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 2.5423,
      "step": 129200
    },
    {
      "epoch": 32.486799094795074,
      "eval_loss": 2.6249735355377197,
      "eval_runtime": 17.0742,
      "eval_samples_per_second": 784.459,
      "eval_steps_per_second": 12.299,
      "step": 129200
    },
    {
      "epoch": 32.51194367613779,
      "grad_norm": 2.696054458618164,
      "learning_rate": 4.8398742138364786e-05,
      "loss": 2.5541,
      "step": 129300
    },
    {
      "epoch": 32.53708825748051,
      "grad_norm": 2.510805606842041,
      "learning_rate": 4.839748427672956e-05,
      "loss": 2.5498,
      "step": 129400
    },
    {
      "epoch": 32.562232838823235,
      "grad_norm": 2.6190755367279053,
      "learning_rate": 4.8396226415094344e-05,
      "loss": 2.5439,
      "step": 129500
    },
    {
      "epoch": 32.58737742016596,
      "grad_norm": 2.689268112182617,
      "learning_rate": 4.839496855345912e-05,
      "loss": 2.5561,
      "step": 129600
    },
    {
      "epoch": 32.58737742016596,
      "eval_loss": 2.625108003616333,
      "eval_runtime": 17.082,
      "eval_samples_per_second": 784.101,
      "eval_steps_per_second": 12.294,
      "step": 129600
    },
    {
      "epoch": 32.61252200150867,
      "grad_norm": 2.5070619583129883,
      "learning_rate": 4.83937106918239e-05,
      "loss": 2.5474,
      "step": 129700
    },
    {
      "epoch": 32.637666582851395,
      "grad_norm": 2.645512819290161,
      "learning_rate": 4.8392452830188684e-05,
      "loss": 2.5392,
      "step": 129800
    },
    {
      "epoch": 32.66281116419412,
      "grad_norm": 2.537945032119751,
      "learning_rate": 4.839119496855346e-05,
      "loss": 2.5553,
      "step": 129900
    },
    {
      "epoch": 32.68795574553684,
      "grad_norm": 2.682896852493286,
      "learning_rate": 4.838993710691824e-05,
      "loss": 2.5613,
      "step": 130000
    },
    {
      "epoch": 32.68795574553684,
      "eval_loss": 2.625293731689453,
      "eval_runtime": 17.1394,
      "eval_samples_per_second": 781.475,
      "eval_steps_per_second": 12.252,
      "step": 130000
    },
    {
      "epoch": 32.713100326879555,
      "grad_norm": 2.491431951522827,
      "learning_rate": 4.838867924528302e-05,
      "loss": 2.5504,
      "step": 130100
    },
    {
      "epoch": 32.73824490822228,
      "grad_norm": 2.7099390029907227,
      "learning_rate": 4.83874213836478e-05,
      "loss": 2.5543,
      "step": 130200
    },
    {
      "epoch": 32.763389489565,
      "grad_norm": 2.5347135066986084,
      "learning_rate": 4.838616352201258e-05,
      "loss": 2.5564,
      "step": 130300
    },
    {
      "epoch": 32.78853407090772,
      "grad_norm": 2.455251455307007,
      "learning_rate": 4.8384905660377364e-05,
      "loss": 2.5658,
      "step": 130400
    },
    {
      "epoch": 32.78853407090772,
      "eval_loss": 2.6233272552490234,
      "eval_runtime": 17.2683,
      "eval_samples_per_second": 775.64,
      "eval_steps_per_second": 12.161,
      "step": 130400
    },
    {
      "epoch": 32.81367865225044,
      "grad_norm": 2.4312820434570312,
      "learning_rate": 4.8383647798742146e-05,
      "loss": 2.5442,
      "step": 130500
    },
    {
      "epoch": 32.83882323359316,
      "grad_norm": 2.4125847816467285,
      "learning_rate": 4.838238993710692e-05,
      "loss": 2.565,
      "step": 130600
    },
    {
      "epoch": 32.86396781493588,
      "grad_norm": 2.859001398086548,
      "learning_rate": 4.8381132075471704e-05,
      "loss": 2.5521,
      "step": 130700
    },
    {
      "epoch": 32.889112396278605,
      "grad_norm": 2.735071897506714,
      "learning_rate": 4.837987421383648e-05,
      "loss": 2.5553,
      "step": 130800
    },
    {
      "epoch": 32.889112396278605,
      "eval_loss": 2.623046636581421,
      "eval_runtime": 17.1256,
      "eval_samples_per_second": 782.103,
      "eval_steps_per_second": 12.262,
      "step": 130800
    },
    {
      "epoch": 32.91425697762132,
      "grad_norm": 2.5398032665252686,
      "learning_rate": 4.837861635220126e-05,
      "loss": 2.5637,
      "step": 130900
    },
    {
      "epoch": 32.93940155896404,
      "grad_norm": 2.463221311569214,
      "learning_rate": 4.837735849056604e-05,
      "loss": 2.5593,
      "step": 131000
    },
    {
      "epoch": 32.964546140306766,
      "grad_norm": 2.6946334838867188,
      "learning_rate": 4.837610062893082e-05,
      "loss": 2.552,
      "step": 131100
    },
    {
      "epoch": 32.98969072164948,
      "grad_norm": 2.6607320308685303,
      "learning_rate": 4.8374842767295595e-05,
      "loss": 2.5496,
      "step": 131200
    },
    {
      "epoch": 32.98969072164948,
      "eval_loss": 2.6220626831054688,
      "eval_runtime": 17.1141,
      "eval_samples_per_second": 782.63,
      "eval_steps_per_second": 12.271,
      "step": 131200
    },
    {
      "epoch": 33.0148353029922,
      "grad_norm": 2.5461926460266113,
      "learning_rate": 4.837358490566038e-05,
      "loss": 2.5397,
      "step": 131300
    },
    {
      "epoch": 33.039979884334926,
      "grad_norm": 2.2620739936828613,
      "learning_rate": 4.837232704402516e-05,
      "loss": 2.5093,
      "step": 131400
    },
    {
      "epoch": 33.06512446567765,
      "grad_norm": 2.808116912841797,
      "learning_rate": 4.837106918238994e-05,
      "loss": 2.5106,
      "step": 131500
    },
    {
      "epoch": 33.090269047020364,
      "grad_norm": 2.500051975250244,
      "learning_rate": 4.8369811320754723e-05,
      "loss": 2.5305,
      "step": 131600
    },
    {
      "epoch": 33.090269047020364,
      "eval_loss": 2.6257174015045166,
      "eval_runtime": 17.1207,
      "eval_samples_per_second": 782.326,
      "eval_steps_per_second": 12.266,
      "step": 131600
    },
    {
      "epoch": 33.115413628363086,
      "grad_norm": 2.5750720500946045,
      "learning_rate": 4.83685534591195e-05,
      "loss": 2.5249,
      "step": 131700
    },
    {
      "epoch": 33.14055820970581,
      "grad_norm": 2.6003220081329346,
      "learning_rate": 4.836729559748428e-05,
      "loss": 2.5178,
      "step": 131800
    },
    {
      "epoch": 33.16570279104853,
      "grad_norm": 2.619020938873291,
      "learning_rate": 4.836603773584906e-05,
      "loss": 2.5345,
      "step": 131900
    },
    {
      "epoch": 33.19084737239125,
      "grad_norm": 2.6205248832702637,
      "learning_rate": 4.836477987421384e-05,
      "loss": 2.5381,
      "step": 132000
    },
    {
      "epoch": 33.19084737239125,
      "eval_loss": 2.6255388259887695,
      "eval_runtime": 17.104,
      "eval_samples_per_second": 783.093,
      "eval_steps_per_second": 12.278,
      "step": 132000
    },
    {
      "epoch": 33.21599195373397,
      "grad_norm": 2.639838695526123,
      "learning_rate": 4.8363522012578614e-05,
      "loss": 2.5346,
      "step": 132100
    },
    {
      "epoch": 33.24113653507669,
      "grad_norm": 2.5903782844543457,
      "learning_rate": 4.8362264150943397e-05,
      "loss": 2.5274,
      "step": 132200
    },
    {
      "epoch": 33.266281116419414,
      "grad_norm": 2.6131880283355713,
      "learning_rate": 4.836100628930818e-05,
      "loss": 2.5374,
      "step": 132300
    },
    {
      "epoch": 33.29142569776213,
      "grad_norm": 2.736914873123169,
      "learning_rate": 4.8359748427672954e-05,
      "loss": 2.5469,
      "step": 132400
    },
    {
      "epoch": 33.29142569776213,
      "eval_loss": 2.626080274581909,
      "eval_runtime": 16.9539,
      "eval_samples_per_second": 790.026,
      "eval_steps_per_second": 12.387,
      "step": 132400
    },
    {
      "epoch": 33.31657027910485,
      "grad_norm": 2.625736951828003,
      "learning_rate": 4.8358490566037736e-05,
      "loss": 2.5369,
      "step": 132500
    },
    {
      "epoch": 33.341714860447574,
      "grad_norm": 2.5495851039886475,
      "learning_rate": 4.835723270440252e-05,
      "loss": 2.5404,
      "step": 132600
    },
    {
      "epoch": 33.3668594417903,
      "grad_norm": 2.825302839279175,
      "learning_rate": 4.83559748427673e-05,
      "loss": 2.5385,
      "step": 132700
    },
    {
      "epoch": 33.39200402313301,
      "grad_norm": 2.6225905418395996,
      "learning_rate": 4.8354716981132076e-05,
      "loss": 2.5493,
      "step": 132800
    },
    {
      "epoch": 33.39200402313301,
      "eval_loss": 2.625131130218506,
      "eval_runtime": 17.2367,
      "eval_samples_per_second": 777.062,
      "eval_steps_per_second": 12.183,
      "step": 132800
    },
    {
      "epoch": 33.417148604475734,
      "grad_norm": 2.5086252689361572,
      "learning_rate": 4.835345911949686e-05,
      "loss": 2.5382,
      "step": 132900
    },
    {
      "epoch": 33.44229318581846,
      "grad_norm": 2.75254487991333,
      "learning_rate": 4.835220125786164e-05,
      "loss": 2.5348,
      "step": 133000
    },
    {
      "epoch": 33.46743776716118,
      "grad_norm": 2.3608157634735107,
      "learning_rate": 4.8350943396226416e-05,
      "loss": 2.5472,
      "step": 133100
    },
    {
      "epoch": 33.492582348503895,
      "grad_norm": 2.715473175048828,
      "learning_rate": 4.83496855345912e-05,
      "loss": 2.548,
      "step": 133200
    },
    {
      "epoch": 33.492582348503895,
      "eval_loss": 2.6249125003814697,
      "eval_runtime": 17.3881,
      "eval_samples_per_second": 770.296,
      "eval_steps_per_second": 12.077,
      "step": 133200
    },
    {
      "epoch": 33.51772692984662,
      "grad_norm": 2.5543134212493896,
      "learning_rate": 4.8348427672955974e-05,
      "loss": 2.533,
      "step": 133300
    },
    {
      "epoch": 33.54287151118934,
      "grad_norm": 2.836143970489502,
      "learning_rate": 4.8347169811320756e-05,
      "loss": 2.5548,
      "step": 133400
    },
    {
      "epoch": 33.56801609253206,
      "grad_norm": 2.4720568656921387,
      "learning_rate": 4.834591194968554e-05,
      "loss": 2.5392,
      "step": 133500
    },
    {
      "epoch": 33.59316067387478,
      "grad_norm": 2.573352336883545,
      "learning_rate": 4.834465408805032e-05,
      "loss": 2.5462,
      "step": 133600
    },
    {
      "epoch": 33.59316067387478,
      "eval_loss": 2.6237313747406006,
      "eval_runtime": 17.1685,
      "eval_samples_per_second": 780.148,
      "eval_steps_per_second": 12.232,
      "step": 133600
    },
    {
      "epoch": 33.6183052552175,
      "grad_norm": 2.572887897491455,
      "learning_rate": 4.8343396226415096e-05,
      "loss": 2.5481,
      "step": 133700
    },
    {
      "epoch": 33.64344983656022,
      "grad_norm": 2.624511241912842,
      "learning_rate": 4.834213836477988e-05,
      "loss": 2.5312,
      "step": 133800
    },
    {
      "epoch": 33.668594417902945,
      "grad_norm": 2.5253865718841553,
      "learning_rate": 4.834088050314466e-05,
      "loss": 2.5475,
      "step": 133900
    },
    {
      "epoch": 33.69373899924566,
      "grad_norm": 2.6258535385131836,
      "learning_rate": 4.8339622641509436e-05,
      "loss": 2.5524,
      "step": 134000
    },
    {
      "epoch": 33.69373899924566,
      "eval_loss": 2.6226184368133545,
      "eval_runtime": 17.2232,
      "eval_samples_per_second": 777.67,
      "eval_steps_per_second": 12.193,
      "step": 134000
    },
    {
      "epoch": 33.71888358058838,
      "grad_norm": 2.544194459915161,
      "learning_rate": 4.833836477987422e-05,
      "loss": 2.5349,
      "step": 134100
    },
    {
      "epoch": 33.744028161931105,
      "grad_norm": 2.5088117122650146,
      "learning_rate": 4.8337106918238994e-05,
      "loss": 2.5533,
      "step": 134200
    },
    {
      "epoch": 33.76917274327383,
      "grad_norm": 2.7966699600219727,
      "learning_rate": 4.8335849056603776e-05,
      "loss": 2.5609,
      "step": 134300
    },
    {
      "epoch": 33.79431732461654,
      "grad_norm": 2.587941884994507,
      "learning_rate": 4.833459119496855e-05,
      "loss": 2.5359,
      "step": 134400
    },
    {
      "epoch": 33.79431732461654,
      "eval_loss": 2.624117851257324,
      "eval_runtime": 17.0539,
      "eval_samples_per_second": 785.393,
      "eval_steps_per_second": 12.314,
      "step": 134400
    },
    {
      "epoch": 33.819461905959265,
      "grad_norm": 2.5446410179138184,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 2.5626,
      "step": 134500
    },
    {
      "epoch": 33.84460648730199,
      "grad_norm": 2.522367238998413,
      "learning_rate": 4.8332075471698116e-05,
      "loss": 2.5445,
      "step": 134600
    },
    {
      "epoch": 33.86975106864471,
      "grad_norm": 2.3640408515930176,
      "learning_rate": 4.83308176100629e-05,
      "loss": 2.5562,
      "step": 134700
    },
    {
      "epoch": 33.894895649987426,
      "grad_norm": 2.6878480911254883,
      "learning_rate": 4.832955974842768e-05,
      "loss": 2.5566,
      "step": 134800
    },
    {
      "epoch": 33.894895649987426,
      "eval_loss": 2.6222052574157715,
      "eval_runtime": 17.1664,
      "eval_samples_per_second": 780.246,
      "eval_steps_per_second": 12.233,
      "step": 134800
    },
    {
      "epoch": 33.92004023133015,
      "grad_norm": 2.368751287460327,
      "learning_rate": 4.8328301886792456e-05,
      "loss": 2.546,
      "step": 134900
    },
    {
      "epoch": 33.94518481267287,
      "grad_norm": 2.5540273189544678,
      "learning_rate": 4.832704402515724e-05,
      "loss": 2.5574,
      "step": 135000
    },
    {
      "epoch": 33.97032939401559,
      "grad_norm": 2.5061898231506348,
      "learning_rate": 4.8325786163522013e-05,
      "loss": 2.5466,
      "step": 135100
    },
    {
      "epoch": 33.99547397535831,
      "grad_norm": 2.332521677017212,
      "learning_rate": 4.8324528301886796e-05,
      "loss": 2.5445,
      "step": 135200
    },
    {
      "epoch": 33.99547397535831,
      "eval_loss": 2.621899366378784,
      "eval_runtime": 17.0986,
      "eval_samples_per_second": 783.341,
      "eval_steps_per_second": 12.282,
      "step": 135200
    },
    {
      "epoch": 34.02061855670103,
      "grad_norm": 2.7240021228790283,
      "learning_rate": 4.832327044025157e-05,
      "loss": 2.5127,
      "step": 135300
    },
    {
      "epoch": 34.04576313804375,
      "grad_norm": 2.589811086654663,
      "learning_rate": 4.832201257861635e-05,
      "loss": 2.5247,
      "step": 135400
    },
    {
      "epoch": 34.07090771938647,
      "grad_norm": 2.6546437740325928,
      "learning_rate": 4.8320754716981136e-05,
      "loss": 2.5083,
      "step": 135500
    },
    {
      "epoch": 34.09605230072919,
      "grad_norm": 2.6622812747955322,
      "learning_rate": 4.831949685534591e-05,
      "loss": 2.5215,
      "step": 135600
    },
    {
      "epoch": 34.09605230072919,
      "eval_loss": 2.6245687007904053,
      "eval_runtime": 17.1022,
      "eval_samples_per_second": 783.173,
      "eval_steps_per_second": 12.279,
      "step": 135600
    },
    {
      "epoch": 34.121196882071914,
      "grad_norm": 2.521296501159668,
      "learning_rate": 4.831823899371069e-05,
      "loss": 2.5122,
      "step": 135700
    },
    {
      "epoch": 34.146341463414636,
      "grad_norm": 2.6160759925842285,
      "learning_rate": 4.8316981132075476e-05,
      "loss": 2.5124,
      "step": 135800
    },
    {
      "epoch": 34.17148604475735,
      "grad_norm": 2.5059592723846436,
      "learning_rate": 4.831572327044026e-05,
      "loss": 2.522,
      "step": 135900
    },
    {
      "epoch": 34.196630626100074,
      "grad_norm": 2.6792120933532715,
      "learning_rate": 4.831446540880503e-05,
      "loss": 2.5312,
      "step": 136000
    },
    {
      "epoch": 34.196630626100074,
      "eval_loss": 2.62322735786438,
      "eval_runtime": 16.911,
      "eval_samples_per_second": 792.028,
      "eval_steps_per_second": 12.418,
      "step": 136000
    },
    {
      "epoch": 34.221775207442796,
      "grad_norm": 2.6074159145355225,
      "learning_rate": 4.8313207547169815e-05,
      "loss": 2.5223,
      "step": 136100
    },
    {
      "epoch": 34.24691978878552,
      "grad_norm": 2.61790132522583,
      "learning_rate": 4.831194968553459e-05,
      "loss": 2.5262,
      "step": 136200
    },
    {
      "epoch": 34.272064370128234,
      "grad_norm": 2.697103500366211,
      "learning_rate": 4.831069182389937e-05,
      "loss": 2.5403,
      "step": 136300
    },
    {
      "epoch": 34.29720895147096,
      "grad_norm": 2.469167470932007,
      "learning_rate": 4.8309433962264155e-05,
      "loss": 2.5245,
      "step": 136400
    },
    {
      "epoch": 34.29720895147096,
      "eval_loss": 2.6261281967163086,
      "eval_runtime": 17.0778,
      "eval_samples_per_second": 784.294,
      "eval_steps_per_second": 12.297,
      "step": 136400
    },
    {
      "epoch": 34.32235353281368,
      "grad_norm": 2.6914660930633545,
      "learning_rate": 4.830817610062893e-05,
      "loss": 2.5296,
      "step": 136500
    },
    {
      "epoch": 34.3474981141564,
      "grad_norm": 2.446591854095459,
      "learning_rate": 4.830691823899371e-05,
      "loss": 2.5144,
      "step": 136600
    },
    {
      "epoch": 34.37264269549912,
      "grad_norm": 2.5247793197631836,
      "learning_rate": 4.830566037735849e-05,
      "loss": 2.5326,
      "step": 136700
    },
    {
      "epoch": 34.39778727684184,
      "grad_norm": 2.603367805480957,
      "learning_rate": 4.830440251572327e-05,
      "loss": 2.5321,
      "step": 136800
    },
    {
      "epoch": 34.39778727684184,
      "eval_loss": 2.6242449283599854,
      "eval_runtime": 16.9187,
      "eval_samples_per_second": 791.67,
      "eval_steps_per_second": 12.412,
      "step": 136800
    },
    {
      "epoch": 34.42293185818456,
      "grad_norm": 2.5560903549194336,
      "learning_rate": 4.830314465408805e-05,
      "loss": 2.5318,
      "step": 136900
    },
    {
      "epoch": 34.448076439527284,
      "grad_norm": 2.6351306438446045,
      "learning_rate": 4.8301886792452835e-05,
      "loss": 2.5469,
      "step": 137000
    },
    {
      "epoch": 34.47322102087,
      "grad_norm": 2.5849761962890625,
      "learning_rate": 4.830062893081761e-05,
      "loss": 2.5328,
      "step": 137100
    },
    {
      "epoch": 34.49836560221272,
      "grad_norm": 2.494619607925415,
      "learning_rate": 4.829937106918239e-05,
      "loss": 2.5401,
      "step": 137200
    },
    {
      "epoch": 34.49836560221272,
      "eval_loss": 2.6214399337768555,
      "eval_runtime": 17.0151,
      "eval_samples_per_second": 787.184,
      "eval_steps_per_second": 12.342,
      "step": 137200
    },
    {
      "epoch": 34.523510183555445,
      "grad_norm": 2.7913339138031006,
      "learning_rate": 4.8298113207547175e-05,
      "loss": 2.5488,
      "step": 137300
    },
    {
      "epoch": 34.54865476489817,
      "grad_norm": 2.800074577331543,
      "learning_rate": 4.829685534591195e-05,
      "loss": 2.5416,
      "step": 137400
    },
    {
      "epoch": 34.57379934624088,
      "grad_norm": 2.6693644523620605,
      "learning_rate": 4.829559748427673e-05,
      "loss": 2.5307,
      "step": 137500
    },
    {
      "epoch": 34.598943927583605,
      "grad_norm": 2.6144120693206787,
      "learning_rate": 4.829433962264151e-05,
      "loss": 2.5316,
      "step": 137600
    },
    {
      "epoch": 34.598943927583605,
      "eval_loss": 2.6234564781188965,
      "eval_runtime": 17.0625,
      "eval_samples_per_second": 784.997,
      "eval_steps_per_second": 12.308,
      "step": 137600
    },
    {
      "epoch": 34.62408850892633,
      "grad_norm": 2.6853973865509033,
      "learning_rate": 4.829308176100629e-05,
      "loss": 2.5482,
      "step": 137700
    },
    {
      "epoch": 34.64923309026905,
      "grad_norm": 2.441706418991089,
      "learning_rate": 4.8291823899371066e-05,
      "loss": 2.5332,
      "step": 137800
    },
    {
      "epoch": 34.674377671611765,
      "grad_norm": 2.6773581504821777,
      "learning_rate": 4.8290566037735855e-05,
      "loss": 2.5462,
      "step": 137900
    },
    {
      "epoch": 34.69952225295449,
      "grad_norm": 2.53275465965271,
      "learning_rate": 4.828930817610064e-05,
      "loss": 2.5431,
      "step": 138000
    },
    {
      "epoch": 34.69952225295449,
      "eval_loss": 2.623760223388672,
      "eval_runtime": 16.9564,
      "eval_samples_per_second": 789.907,
      "eval_steps_per_second": 12.385,
      "step": 138000
    },
    {
      "epoch": 34.72466683429721,
      "grad_norm": 2.520261764526367,
      "learning_rate": 4.828805031446541e-05,
      "loss": 2.5548,
      "step": 138100
    },
    {
      "epoch": 34.74981141563993,
      "grad_norm": 2.4346981048583984,
      "learning_rate": 4.8286792452830195e-05,
      "loss": 2.5453,
      "step": 138200
    },
    {
      "epoch": 34.77495599698265,
      "grad_norm": 2.4254519939422607,
      "learning_rate": 4.828553459119497e-05,
      "loss": 2.5423,
      "step": 138300
    },
    {
      "epoch": 34.80010057832537,
      "grad_norm": 2.5339367389678955,
      "learning_rate": 4.828427672955975e-05,
      "loss": 2.5474,
      "step": 138400
    },
    {
      "epoch": 34.80010057832537,
      "eval_loss": 2.621065378189087,
      "eval_runtime": 17.1185,
      "eval_samples_per_second": 782.43,
      "eval_steps_per_second": 12.267,
      "step": 138400
    },
    {
      "epoch": 34.82524515966809,
      "grad_norm": 2.638090133666992,
      "learning_rate": 4.828301886792453e-05,
      "loss": 2.5354,
      "step": 138500
    },
    {
      "epoch": 34.850389741010815,
      "grad_norm": 2.6023764610290527,
      "learning_rate": 4.828176100628931e-05,
      "loss": 2.5526,
      "step": 138600
    },
    {
      "epoch": 34.87553432235353,
      "grad_norm": 2.62764048576355,
      "learning_rate": 4.8280503144654086e-05,
      "loss": 2.5475,
      "step": 138700
    },
    {
      "epoch": 34.90067890369625,
      "grad_norm": 2.5841057300567627,
      "learning_rate": 4.827924528301887e-05,
      "loss": 2.5439,
      "step": 138800
    },
    {
      "epoch": 34.90067890369625,
      "eval_loss": 2.6199724674224854,
      "eval_runtime": 16.9631,
      "eval_samples_per_second": 789.595,
      "eval_steps_per_second": 12.38,
      "step": 138800
    },
    {
      "epoch": 34.925823485038975,
      "grad_norm": 2.583649158477783,
      "learning_rate": 4.827798742138365e-05,
      "loss": 2.5414,
      "step": 138900
    },
    {
      "epoch": 34.9509680663817,
      "grad_norm": 2.4430787563323975,
      "learning_rate": 4.827672955974843e-05,
      "loss": 2.5405,
      "step": 139000
    },
    {
      "epoch": 34.97611264772441,
      "grad_norm": 2.5950522422790527,
      "learning_rate": 4.8275471698113215e-05,
      "loss": 2.5502,
      "step": 139100
    },
    {
      "epoch": 35.001257229067136,
      "grad_norm": 2.4061214923858643,
      "learning_rate": 4.827421383647799e-05,
      "loss": 2.5491,
      "step": 139200
    },
    {
      "epoch": 35.001257229067136,
      "eval_loss": 2.619361400604248,
      "eval_runtime": 17.461,
      "eval_samples_per_second": 767.081,
      "eval_steps_per_second": 12.027,
      "step": 139200
    },
    {
      "epoch": 35.02640181040986,
      "grad_norm": 2.5650782585144043,
      "learning_rate": 4.827295597484277e-05,
      "loss": 2.5133,
      "step": 139300
    },
    {
      "epoch": 35.05154639175258,
      "grad_norm": 2.492077589035034,
      "learning_rate": 4.827169811320755e-05,
      "loss": 2.5156,
      "step": 139400
    },
    {
      "epoch": 35.076690973095296,
      "grad_norm": 2.769442558288574,
      "learning_rate": 4.827044025157233e-05,
      "loss": 2.5172,
      "step": 139500
    },
    {
      "epoch": 35.10183555443802,
      "grad_norm": 2.651124954223633,
      "learning_rate": 4.8269182389937105e-05,
      "loss": 2.5127,
      "step": 139600
    },
    {
      "epoch": 35.10183555443802,
      "eval_loss": 2.62353777885437,
      "eval_runtime": 17.2242,
      "eval_samples_per_second": 777.628,
      "eval_steps_per_second": 12.192,
      "step": 139600
    },
    {
      "epoch": 35.12698013578074,
      "grad_norm": 2.7364604473114014,
      "learning_rate": 4.826792452830189e-05,
      "loss": 2.5178,
      "step": 139700
    },
    {
      "epoch": 35.15212471712346,
      "grad_norm": 2.739039421081543,
      "learning_rate": 4.826666666666667e-05,
      "loss": 2.5146,
      "step": 139800
    },
    {
      "epoch": 35.17726929846618,
      "grad_norm": 2.526369333267212,
      "learning_rate": 4.8265408805031445e-05,
      "loss": 2.5093,
      "step": 139900
    },
    {
      "epoch": 35.2024138798089,
      "grad_norm": 2.450300455093384,
      "learning_rate": 4.826415094339623e-05,
      "loss": 2.521,
      "step": 140000
    },
    {
      "epoch": 35.2024138798089,
      "eval_loss": 2.623090982437134,
      "eval_runtime": 17.3386,
      "eval_samples_per_second": 772.498,
      "eval_steps_per_second": 12.112,
      "step": 140000
    },
    {
      "epoch": 35.227558461151624,
      "grad_norm": 2.6892454624176025,
      "learning_rate": 4.826289308176101e-05,
      "loss": 2.5249,
      "step": 140100
    },
    {
      "epoch": 35.25270304249434,
      "grad_norm": 2.5363380908966064,
      "learning_rate": 4.826163522012579e-05,
      "loss": 2.5176,
      "step": 140200
    },
    {
      "epoch": 35.27784762383706,
      "grad_norm": 2.6141183376312256,
      "learning_rate": 4.826037735849057e-05,
      "loss": 2.5235,
      "step": 140300
    },
    {
      "epoch": 35.302992205179784,
      "grad_norm": 2.8718838691711426,
      "learning_rate": 4.825911949685535e-05,
      "loss": 2.5196,
      "step": 140400
    },
    {
      "epoch": 35.302992205179784,
      "eval_loss": 2.624917984008789,
      "eval_runtime": 17.8355,
      "eval_samples_per_second": 750.976,
      "eval_steps_per_second": 11.774,
      "step": 140400
    },
    {
      "epoch": 35.328136786522506,
      "grad_norm": 2.456613302230835,
      "learning_rate": 4.825786163522013e-05,
      "loss": 2.5245,
      "step": 140500
    },
    {
      "epoch": 35.35328136786522,
      "grad_norm": 2.5492184162139893,
      "learning_rate": 4.825660377358491e-05,
      "loss": 2.5221,
      "step": 140600
    },
    {
      "epoch": 35.378425949207944,
      "grad_norm": 2.4584949016571045,
      "learning_rate": 4.825534591194969e-05,
      "loss": 2.5222,
      "step": 140700
    },
    {
      "epoch": 35.40357053055067,
      "grad_norm": 2.5908384323120117,
      "learning_rate": 4.8254088050314465e-05,
      "loss": 2.5277,
      "step": 140800
    },
    {
      "epoch": 35.40357053055067,
      "eval_loss": 2.6239707469940186,
      "eval_runtime": 17.1047,
      "eval_samples_per_second": 783.058,
      "eval_steps_per_second": 12.277,
      "step": 140800
    },
    {
      "epoch": 35.42871511189339,
      "grad_norm": 2.6928534507751465,
      "learning_rate": 4.825283018867925e-05,
      "loss": 2.5164,
      "step": 140900
    },
    {
      "epoch": 35.453859693236105,
      "grad_norm": 2.4467947483062744,
      "learning_rate": 4.825157232704402e-05,
      "loss": 2.5168,
      "step": 141000
    },
    {
      "epoch": 35.47900427457883,
      "grad_norm": 2.370609998703003,
      "learning_rate": 4.8250314465408805e-05,
      "loss": 2.537,
      "step": 141100
    },
    {
      "epoch": 35.50414885592155,
      "grad_norm": 2.5747265815734863,
      "learning_rate": 4.824905660377359e-05,
      "loss": 2.5388,
      "step": 141200
    },
    {
      "epoch": 35.50414885592155,
      "eval_loss": 2.621626615524292,
      "eval_runtime": 17.2746,
      "eval_samples_per_second": 775.358,
      "eval_steps_per_second": 12.157,
      "step": 141200
    },
    {
      "epoch": 35.52929343726427,
      "grad_norm": 2.865574598312378,
      "learning_rate": 4.824779874213837e-05,
      "loss": 2.5221,
      "step": 141300
    },
    {
      "epoch": 35.55443801860699,
      "grad_norm": 2.7031877040863037,
      "learning_rate": 4.824654088050315e-05,
      "loss": 2.533,
      "step": 141400
    },
    {
      "epoch": 35.57958259994971,
      "grad_norm": 2.5186190605163574,
      "learning_rate": 4.824528301886793e-05,
      "loss": 2.5306,
      "step": 141500
    },
    {
      "epoch": 35.60472718129243,
      "grad_norm": 2.4406931400299072,
      "learning_rate": 4.824402515723271e-05,
      "loss": 2.5481,
      "step": 141600
    },
    {
      "epoch": 35.60472718129243,
      "eval_loss": 2.620779037475586,
      "eval_runtime": 17.1234,
      "eval_samples_per_second": 782.204,
      "eval_steps_per_second": 12.264,
      "step": 141600
    },
    {
      "epoch": 35.629871762635155,
      "grad_norm": 2.7423648834228516,
      "learning_rate": 4.8242767295597485e-05,
      "loss": 2.5253,
      "step": 141700
    },
    {
      "epoch": 35.65501634397787,
      "grad_norm": 2.444586992263794,
      "learning_rate": 4.824150943396227e-05,
      "loss": 2.5468,
      "step": 141800
    },
    {
      "epoch": 35.68016092532059,
      "grad_norm": 2.4675261974334717,
      "learning_rate": 4.824025157232704e-05,
      "loss": 2.5337,
      "step": 141900
    },
    {
      "epoch": 35.705305506663315,
      "grad_norm": 2.59968638420105,
      "learning_rate": 4.8238993710691825e-05,
      "loss": 2.5306,
      "step": 142000
    },
    {
      "epoch": 35.705305506663315,
      "eval_loss": 2.620932102203369,
      "eval_runtime": 16.9825,
      "eval_samples_per_second": 788.694,
      "eval_steps_per_second": 12.366,
      "step": 142000
    },
    {
      "epoch": 35.73045008800604,
      "grad_norm": 2.288264274597168,
      "learning_rate": 4.82377358490566e-05,
      "loss": 2.5248,
      "step": 142100
    },
    {
      "epoch": 35.75559466934875,
      "grad_norm": 2.7246031761169434,
      "learning_rate": 4.823647798742138e-05,
      "loss": 2.5339,
      "step": 142200
    },
    {
      "epoch": 35.780739250691475,
      "grad_norm": 2.7055485248565674,
      "learning_rate": 4.8235220125786165e-05,
      "loss": 2.5423,
      "step": 142300
    },
    {
      "epoch": 35.8058838320342,
      "grad_norm": 2.6431331634521484,
      "learning_rate": 4.823396226415095e-05,
      "loss": 2.5426,
      "step": 142400
    },
    {
      "epoch": 35.8058838320342,
      "eval_loss": 2.6224844455718994,
      "eval_runtime": 17.0251,
      "eval_samples_per_second": 786.72,
      "eval_steps_per_second": 12.335,
      "step": 142400
    },
    {
      "epoch": 35.83102841337692,
      "grad_norm": 2.5168280601501465,
      "learning_rate": 4.823270440251573e-05,
      "loss": 2.5321,
      "step": 142500
    },
    {
      "epoch": 35.856172994719635,
      "grad_norm": 2.3010735511779785,
      "learning_rate": 4.8231446540880505e-05,
      "loss": 2.5412,
      "step": 142600
    },
    {
      "epoch": 35.88131757606236,
      "grad_norm": 2.622084617614746,
      "learning_rate": 4.823018867924529e-05,
      "loss": 2.5344,
      "step": 142700
    },
    {
      "epoch": 35.90646215740508,
      "grad_norm": 2.56330943107605,
      "learning_rate": 4.822893081761006e-05,
      "loss": 2.5294,
      "step": 142800
    },
    {
      "epoch": 35.90646215740508,
      "eval_loss": 2.6185669898986816,
      "eval_runtime": 17.0336,
      "eval_samples_per_second": 786.329,
      "eval_steps_per_second": 12.329,
      "step": 142800
    },
    {
      "epoch": 35.9316067387478,
      "grad_norm": 2.4208617210388184,
      "learning_rate": 4.8227672955974844e-05,
      "loss": 2.5448,
      "step": 142900
    },
    {
      "epoch": 35.95675132009052,
      "grad_norm": 2.6396872997283936,
      "learning_rate": 4.822641509433963e-05,
      "loss": 2.5371,
      "step": 143000
    },
    {
      "epoch": 35.98189590143324,
      "grad_norm": 2.529179573059082,
      "learning_rate": 4.82251572327044e-05,
      "loss": 2.5371,
      "step": 143100
    },
    {
      "epoch": 36.00704048277596,
      "grad_norm": 2.761653423309326,
      "learning_rate": 4.8223899371069184e-05,
      "loss": 2.5294,
      "step": 143200
    },
    {
      "epoch": 36.00704048277596,
      "eval_loss": 2.619807720184326,
      "eval_runtime": 17.0685,
      "eval_samples_per_second": 784.722,
      "eval_steps_per_second": 12.303,
      "step": 143200
    },
    {
      "epoch": 36.032185064118686,
      "grad_norm": 2.6211090087890625,
      "learning_rate": 4.8222641509433967e-05,
      "loss": 2.4915,
      "step": 143300
    },
    {
      "epoch": 36.0573296454614,
      "grad_norm": 2.6515862941741943,
      "learning_rate": 4.822138364779875e-05,
      "loss": 2.5046,
      "step": 143400
    },
    {
      "epoch": 36.08247422680412,
      "grad_norm": 2.58154034614563,
      "learning_rate": 4.8220125786163524e-05,
      "loss": 2.5128,
      "step": 143500
    },
    {
      "epoch": 36.107618808146846,
      "grad_norm": 2.8762295246124268,
      "learning_rate": 4.8218867924528307e-05,
      "loss": 2.5076,
      "step": 143600
    },
    {
      "epoch": 36.107618808146846,
      "eval_loss": 2.6219279766082764,
      "eval_runtime": 17.1139,
      "eval_samples_per_second": 782.639,
      "eval_steps_per_second": 12.271,
      "step": 143600
    },
    {
      "epoch": 36.13276338948957,
      "grad_norm": 2.5488619804382324,
      "learning_rate": 4.821761006289308e-05,
      "loss": 2.5015,
      "step": 143700
    },
    {
      "epoch": 36.157907970832284,
      "grad_norm": 2.649444580078125,
      "learning_rate": 4.8216352201257864e-05,
      "loss": 2.5034,
      "step": 143800
    },
    {
      "epoch": 36.183052552175006,
      "grad_norm": 2.623206615447998,
      "learning_rate": 4.8215094339622646e-05,
      "loss": 2.5238,
      "step": 143900
    },
    {
      "epoch": 36.20819713351773,
      "grad_norm": 2.7186665534973145,
      "learning_rate": 4.821383647798742e-05,
      "loss": 2.515,
      "step": 144000
    },
    {
      "epoch": 36.20819713351773,
      "eval_loss": 2.6246116161346436,
      "eval_runtime": 17.0458,
      "eval_samples_per_second": 785.766,
      "eval_steps_per_second": 12.32,
      "step": 144000
    },
    {
      "epoch": 36.23334171486045,
      "grad_norm": 2.6598527431488037,
      "learning_rate": 4.8212578616352204e-05,
      "loss": 2.5169,
      "step": 144100
    },
    {
      "epoch": 36.258486296203166,
      "grad_norm": 2.5585968494415283,
      "learning_rate": 4.821132075471698e-05,
      "loss": 2.52,
      "step": 144200
    },
    {
      "epoch": 36.28363087754589,
      "grad_norm": 2.649911642074585,
      "learning_rate": 4.821006289308176e-05,
      "loss": 2.51,
      "step": 144300
    },
    {
      "epoch": 36.30877545888861,
      "grad_norm": 2.5191221237182617,
      "learning_rate": 4.8208805031446544e-05,
      "loss": 2.5141,
      "step": 144400
    },
    {
      "epoch": 36.30877545888861,
      "eval_loss": 2.6238787174224854,
      "eval_runtime": 17.13,
      "eval_samples_per_second": 781.904,
      "eval_steps_per_second": 12.259,
      "step": 144400
    },
    {
      "epoch": 36.33392004023133,
      "grad_norm": 2.6773860454559326,
      "learning_rate": 4.8207547169811326e-05,
      "loss": 2.5086,
      "step": 144500
    },
    {
      "epoch": 36.35906462157405,
      "grad_norm": 2.420097589492798,
      "learning_rate": 4.82062893081761e-05,
      "loss": 2.5267,
      "step": 144600
    },
    {
      "epoch": 36.38420920291677,
      "grad_norm": 2.6726832389831543,
      "learning_rate": 4.8205031446540884e-05,
      "loss": 2.5214,
      "step": 144700
    },
    {
      "epoch": 36.409353784259494,
      "grad_norm": 2.6483099460601807,
      "learning_rate": 4.8203773584905666e-05,
      "loss": 2.5262,
      "step": 144800
    },
    {
      "epoch": 36.409353784259494,
      "eval_loss": 2.6237497329711914,
      "eval_runtime": 17.0678,
      "eval_samples_per_second": 784.751,
      "eval_steps_per_second": 12.304,
      "step": 144800
    },
    {
      "epoch": 36.43449836560221,
      "grad_norm": 2.7944040298461914,
      "learning_rate": 4.820251572327044e-05,
      "loss": 2.5128,
      "step": 144900
    },
    {
      "epoch": 36.45964294694493,
      "grad_norm": 2.5470824241638184,
      "learning_rate": 4.8201257861635224e-05,
      "loss": 2.5197,
      "step": 145000
    },
    {
      "epoch": 36.484787528287654,
      "grad_norm": 2.421841621398926,
      "learning_rate": 4.82e-05,
      "loss": 2.5325,
      "step": 145100
    },
    {
      "epoch": 36.50993210963038,
      "grad_norm": 2.634490728378296,
      "learning_rate": 4.819874213836478e-05,
      "loss": 2.523,
      "step": 145200
    },
    {
      "epoch": 36.50993210963038,
      "eval_loss": 2.6228725910186768,
      "eval_runtime": 17.174,
      "eval_samples_per_second": 779.902,
      "eval_steps_per_second": 12.228,
      "step": 145200
    },
    {
      "epoch": 36.53507669097309,
      "grad_norm": 2.419492483139038,
      "learning_rate": 4.819748427672956e-05,
      "loss": 2.5258,
      "step": 145300
    },
    {
      "epoch": 36.560221272315815,
      "grad_norm": 2.43269944190979,
      "learning_rate": 4.819622641509434e-05,
      "loss": 2.5244,
      "step": 145400
    },
    {
      "epoch": 36.58536585365854,
      "grad_norm": 2.3758883476257324,
      "learning_rate": 4.819496855345912e-05,
      "loss": 2.5246,
      "step": 145500
    },
    {
      "epoch": 36.61051043500126,
      "grad_norm": 2.4594898223876953,
      "learning_rate": 4.8193710691823904e-05,
      "loss": 2.5319,
      "step": 145600
    },
    {
      "epoch": 36.61051043500126,
      "eval_loss": 2.620870351791382,
      "eval_runtime": 17.016,
      "eval_samples_per_second": 787.139,
      "eval_steps_per_second": 12.341,
      "step": 145600
    },
    {
      "epoch": 36.635655016343975,
      "grad_norm": 2.6615781784057617,
      "learning_rate": 4.8192452830188686e-05,
      "loss": 2.5325,
      "step": 145700
    },
    {
      "epoch": 36.6607995976867,
      "grad_norm": 2.7175421714782715,
      "learning_rate": 4.819119496855346e-05,
      "loss": 2.5265,
      "step": 145800
    },
    {
      "epoch": 36.68594417902942,
      "grad_norm": 2.6735646724700928,
      "learning_rate": 4.8189937106918244e-05,
      "loss": 2.5354,
      "step": 145900
    },
    {
      "epoch": 36.71108876037214,
      "grad_norm": 2.8374876976013184,
      "learning_rate": 4.818867924528302e-05,
      "loss": 2.5313,
      "step": 146000
    },
    {
      "epoch": 36.71108876037214,
      "eval_loss": 2.6198952198028564,
      "eval_runtime": 16.8762,
      "eval_samples_per_second": 793.662,
      "eval_steps_per_second": 12.444,
      "step": 146000
    },
    {
      "epoch": 36.73623334171486,
      "grad_norm": 2.6033289432525635,
      "learning_rate": 4.81874213836478e-05,
      "loss": 2.524,
      "step": 146100
    },
    {
      "epoch": 36.76137792305758,
      "grad_norm": 2.6375820636749268,
      "learning_rate": 4.818616352201258e-05,
      "loss": 2.533,
      "step": 146200
    },
    {
      "epoch": 36.7865225044003,
      "grad_norm": 2.6434357166290283,
      "learning_rate": 4.818490566037736e-05,
      "loss": 2.5257,
      "step": 146300
    },
    {
      "epoch": 36.811667085743025,
      "grad_norm": 2.429751396179199,
      "learning_rate": 4.818364779874214e-05,
      "loss": 2.5354,
      "step": 146400
    },
    {
      "epoch": 36.811667085743025,
      "eval_loss": 2.61917781829834,
      "eval_runtime": 16.9991,
      "eval_samples_per_second": 787.926,
      "eval_steps_per_second": 12.354,
      "step": 146400
    },
    {
      "epoch": 36.83681166708574,
      "grad_norm": 2.4009344577789307,
      "learning_rate": 4.818238993710692e-05,
      "loss": 2.5281,
      "step": 146500
    },
    {
      "epoch": 36.86195624842846,
      "grad_norm": 2.425191879272461,
      "learning_rate": 4.81811320754717e-05,
      "loss": 2.5335,
      "step": 146600
    },
    {
      "epoch": 36.887100829771185,
      "grad_norm": 2.3766729831695557,
      "learning_rate": 4.817987421383648e-05,
      "loss": 2.5304,
      "step": 146700
    },
    {
      "epoch": 36.91224541111391,
      "grad_norm": 2.616053342819214,
      "learning_rate": 4.817861635220126e-05,
      "loss": 2.54,
      "step": 146800
    },
    {
      "epoch": 36.91224541111391,
      "eval_loss": 2.6197524070739746,
      "eval_runtime": 16.9847,
      "eval_samples_per_second": 788.592,
      "eval_steps_per_second": 12.364,
      "step": 146800
    },
    {
      "epoch": 36.93738999245662,
      "grad_norm": 2.514406204223633,
      "learning_rate": 4.817735849056604e-05,
      "loss": 2.5291,
      "step": 146900
    },
    {
      "epoch": 36.962534573799346,
      "grad_norm": 2.654329776763916,
      "learning_rate": 4.817610062893082e-05,
      "loss": 2.5354,
      "step": 147000
    },
    {
      "epoch": 36.98767915514207,
      "grad_norm": 2.51717209815979,
      "learning_rate": 4.8174842767295596e-05,
      "loss": 2.5323,
      "step": 147100
    },
    {
      "epoch": 37.01282373648479,
      "grad_norm": 2.6232638359069824,
      "learning_rate": 4.817358490566038e-05,
      "loss": 2.5043,
      "step": 147200
    },
    {
      "epoch": 37.01282373648479,
      "eval_loss": 2.6207315921783447,
      "eval_runtime": 16.951,
      "eval_samples_per_second": 790.161,
      "eval_steps_per_second": 12.389,
      "step": 147200
    },
    {
      "epoch": 37.037968317827506,
      "grad_norm": 2.6840479373931885,
      "learning_rate": 4.817232704402516e-05,
      "loss": 2.4971,
      "step": 147300
    },
    {
      "epoch": 37.06311289917023,
      "grad_norm": 2.5992889404296875,
      "learning_rate": 4.8171069182389936e-05,
      "loss": 2.4978,
      "step": 147400
    },
    {
      "epoch": 37.08825748051295,
      "grad_norm": 2.4603354930877686,
      "learning_rate": 4.816981132075472e-05,
      "loss": 2.4836,
      "step": 147500
    },
    {
      "epoch": 37.11340206185567,
      "grad_norm": 2.618720054626465,
      "learning_rate": 4.81685534591195e-05,
      "loss": 2.4989,
      "step": 147600
    },
    {
      "epoch": 37.11340206185567,
      "eval_loss": 2.623554229736328,
      "eval_runtime": 17.0011,
      "eval_samples_per_second": 787.832,
      "eval_steps_per_second": 12.352,
      "step": 147600
    },
    {
      "epoch": 37.13854664319839,
      "grad_norm": 2.423147678375244,
      "learning_rate": 4.816729559748428e-05,
      "loss": 2.5075,
      "step": 147700
    },
    {
      "epoch": 37.16369122454111,
      "grad_norm": 2.552314519882202,
      "learning_rate": 4.816603773584906e-05,
      "loss": 2.5076,
      "step": 147800
    },
    {
      "epoch": 37.18883580588383,
      "grad_norm": 2.8627545833587646,
      "learning_rate": 4.816477987421384e-05,
      "loss": 2.509,
      "step": 147900
    },
    {
      "epoch": 37.213980387226556,
      "grad_norm": 2.624513626098633,
      "learning_rate": 4.816352201257862e-05,
      "loss": 2.5139,
      "step": 148000
    },
    {
      "epoch": 37.213980387226556,
      "eval_loss": 2.6205806732177734,
      "eval_runtime": 16.8711,
      "eval_samples_per_second": 793.902,
      "eval_steps_per_second": 12.447,
      "step": 148000
    },
    {
      "epoch": 37.23912496856927,
      "grad_norm": 2.553778886795044,
      "learning_rate": 4.81622641509434e-05,
      "loss": 2.5051,
      "step": 148100
    },
    {
      "epoch": 37.264269549911994,
      "grad_norm": 2.771967887878418,
      "learning_rate": 4.816100628930818e-05,
      "loss": 2.508,
      "step": 148200
    },
    {
      "epoch": 37.289414131254716,
      "grad_norm": 2.5561816692352295,
      "learning_rate": 4.8159748427672956e-05,
      "loss": 2.5116,
      "step": 148300
    },
    {
      "epoch": 37.31455871259744,
      "grad_norm": 2.628323793411255,
      "learning_rate": 4.815849056603774e-05,
      "loss": 2.5122,
      "step": 148400
    },
    {
      "epoch": 37.31455871259744,
      "eval_loss": 2.6216659545898438,
      "eval_runtime": 17.0055,
      "eval_samples_per_second": 787.629,
      "eval_steps_per_second": 12.349,
      "step": 148400
    },
    {
      "epoch": 37.339703293940154,
      "grad_norm": 2.5555641651153564,
      "learning_rate": 4.8157232704402514e-05,
      "loss": 2.5137,
      "step": 148500
    },
    {
      "epoch": 37.36484787528288,
      "grad_norm": 2.6077659130096436,
      "learning_rate": 4.8155974842767296e-05,
      "loss": 2.513,
      "step": 148600
    },
    {
      "epoch": 37.3899924566256,
      "grad_norm": 2.6037561893463135,
      "learning_rate": 4.815471698113208e-05,
      "loss": 2.5148,
      "step": 148700
    },
    {
      "epoch": 37.415137037968314,
      "grad_norm": 2.5103859901428223,
      "learning_rate": 4.815345911949686e-05,
      "loss": 2.5173,
      "step": 148800
    },
    {
      "epoch": 37.415137037968314,
      "eval_loss": 2.6223061084747314,
      "eval_runtime": 17.2295,
      "eval_samples_per_second": 777.387,
      "eval_steps_per_second": 12.188,
      "step": 148800
    },
    {
      "epoch": 37.44028161931104,
      "grad_norm": 2.471041679382324,
      "learning_rate": 4.815220125786164e-05,
      "loss": 2.5144,
      "step": 148900
    },
    {
      "epoch": 37.46542620065376,
      "grad_norm": 2.5814707279205322,
      "learning_rate": 4.815094339622642e-05,
      "loss": 2.5117,
      "step": 149000
    },
    {
      "epoch": 37.49057078199648,
      "grad_norm": 2.7572319507598877,
      "learning_rate": 4.81496855345912e-05,
      "loss": 2.5191,
      "step": 149100
    },
    {
      "epoch": 37.5157153633392,
      "grad_norm": 2.733613967895508,
      "learning_rate": 4.8148427672955976e-05,
      "loss": 2.5179,
      "step": 149200
    },
    {
      "epoch": 37.5157153633392,
      "eval_loss": 2.62026309967041,
      "eval_runtime": 17.0449,
      "eval_samples_per_second": 785.808,
      "eval_steps_per_second": 12.32,
      "step": 149200
    },
    {
      "epoch": 37.54085994468192,
      "grad_norm": 2.70591402053833,
      "learning_rate": 4.814716981132076e-05,
      "loss": 2.5216,
      "step": 149300
    },
    {
      "epoch": 37.56600452602464,
      "grad_norm": 2.4742233753204346,
      "learning_rate": 4.8145911949685534e-05,
      "loss": 2.5265,
      "step": 149400
    },
    {
      "epoch": 37.591149107367364,
      "grad_norm": 2.4377472400665283,
      "learning_rate": 4.8144654088050316e-05,
      "loss": 2.5206,
      "step": 149500
    },
    {
      "epoch": 37.61629368871008,
      "grad_norm": 2.3948299884796143,
      "learning_rate": 4.814339622641509e-05,
      "loss": 2.5229,
      "step": 149600
    },
    {
      "epoch": 37.61629368871008,
      "eval_loss": 2.620730400085449,
      "eval_runtime": 16.9743,
      "eval_samples_per_second": 789.076,
      "eval_steps_per_second": 12.372,
      "step": 149600
    },
    {
      "epoch": 37.6414382700528,
      "grad_norm": 2.5671205520629883,
      "learning_rate": 4.8142138364779873e-05,
      "loss": 2.5125,
      "step": 149700
    },
    {
      "epoch": 37.666582851395525,
      "grad_norm": 2.6596198081970215,
      "learning_rate": 4.8140880503144656e-05,
      "loss": 2.5205,
      "step": 149800
    },
    {
      "epoch": 37.69172743273825,
      "grad_norm": 2.7093887329101562,
      "learning_rate": 4.813962264150944e-05,
      "loss": 2.5163,
      "step": 149900
    },
    {
      "epoch": 37.71687201408096,
      "grad_norm": 2.7090277671813965,
      "learning_rate": 4.813836477987422e-05,
      "loss": 2.5158,
      "step": 150000
    },
    {
      "epoch": 37.71687201408096,
      "eval_loss": 2.620112419128418,
      "eval_runtime": 17.1317,
      "eval_samples_per_second": 781.823,
      "eval_steps_per_second": 12.258,
      "step": 150000
    },
    {
      "epoch": 37.742016595423685,
      "grad_norm": 2.5733563899993896,
      "learning_rate": 4.8137106918238996e-05,
      "loss": 2.5262,
      "step": 150100
    },
    {
      "epoch": 37.76716117676641,
      "grad_norm": 2.655407428741455,
      "learning_rate": 4.813584905660378e-05,
      "loss": 2.5247,
      "step": 150200
    },
    {
      "epoch": 37.79230575810913,
      "grad_norm": 2.6121222972869873,
      "learning_rate": 4.813459119496855e-05,
      "loss": 2.5375,
      "step": 150300
    },
    {
      "epoch": 37.817450339451845,
      "grad_norm": 2.3771848678588867,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 2.5312,
      "step": 150400
    },
    {
      "epoch": 37.817450339451845,
      "eval_loss": 2.620004177093506,
      "eval_runtime": 17.1236,
      "eval_samples_per_second": 782.195,
      "eval_steps_per_second": 12.264,
      "step": 150400
    },
    {
      "epoch": 37.84259492079457,
      "grad_norm": 2.480534553527832,
      "learning_rate": 4.813207547169812e-05,
      "loss": 2.5316,
      "step": 150500
    }
  ],
  "logging_steps": 100,
  "max_steps": 3977000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1000,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 9.6432535165824e+16,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
